%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Nick Tustison at 2021-02-24 08:52:51 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{Fu:2020aa,
	author = {Yunguan Fu and Nina Monta{\~n}̃a Brown and Shaheer U. Saeed and Adr{\`a}à Casamitjana and Zachary M. C. Baum and{\'e}Rémi Delaunay and Qianye Yang and Alexander Grimwood and Zhe Min and Stefano B. Blumberg and Juan Eugenio Iglesias and Dean C. Barratt and Ester Bonmati and Daniel C. Alexander and Matthew J. Clarkson and Tom Vercauteren and Yipeng Hu},
	date-added = {2021-02-24 08:49:19 -0800},
	date-modified = {2021-02-24 08:52:46 -0800},
	journal = {Journal of Open Source Software},
	number = {55},
	pages = {2705},
	title = {DeepReg: a deep learning toolkit for medical image registration.},
	volume = {5},
	year = {2020}}

@article{Vos:2019wr,
	abstract = {Image registration, the process of aligning two or more images, is the core technique of many (semi-)automatic medical image analysis tasks. Recent studies have shown that deep learning methods, notably convolutional neural networks (ConvNets), can be used for image registration. Thus far training of ConvNets for registration was supervised using predefined example registrations. However, obtaining example registrations is not trivial. To circumvent the need for predefined examples, and thereby to increase convenience of training ConvNets for image registration, we propose the Deep Learning Image Registration (DLIR) framework for unsupervised affine and deformable image registration. In the DLIR framework ConvNets are trained for image registration by exploiting image similarity analogous to conventional intensity-based image registration. After a ConvNet has been trained with the DLIR framework, it can be used to register pairs of unseen images in one shot. We propose flexible ConvNets designs for affine image registration and for deformable image registration. By stacking multiple of these ConvNets into a larger architecture, we are able to perform coarse-to-fine image registration. We show for registration of cardiac cine MRI and registration of chest CT that performance of the DLIR framework is comparable to conventional image registration while being several orders of magnitude faster.},
	author = {de Vos, Bob D and Berendsen, Floris F and Viergever, Max A and Sokooti, Hessam and Staring, Marius and I{\v s}gum, Ivana},
	date-added = {2021-02-24 08:47:39 -0800},
	date-modified = {2021-02-24 08:47:39 -0800},
	doi = {10.1016/j.media.2018.11.010},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	keywords = {Affine image registration; Cardiac cine MRI; Chest CT; Deep learning; Deformable image registration; Unsupervised learning},
	mesh = {Deep Learning; Heart Diseases; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging, Cine; Neural Networks, Computer; Radiography, Thoracic; Tomography, X-Ray Computed; Unsupervised Machine Learning},
	month = {02},
	pages = {128-143},
	pmid = {30579222},
	pst = {ppublish},
	title = {A deep learning framework for unsupervised affine and deformable image registration},
	volume = {52},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.media.2018.11.010}}

@url{srpb,
	date-added = {2020-11-01 07:33:26 -0800},
	date-modified = {2020-11-01 07:38:30 -0800},
	lastchecked = {October 17, 2020},
	title = {{https://bicr-resource.atr.jp/srpbs1600/}},
	url = {https://bicr-resource.atr.jp/srpbs1600/},
	Bdsk-Url-1 = {https://bicr-resource.atr.jp/srpbs1600/}}

@url{oasis,
	date-added = {2020-11-01 07:31:48 -0800},
	date-modified = {2020-11-01 07:38:41 -0800},
	lastchecked = {October 17, 2020},
	title = {{https://www.oasis-brains.org}},
	url = {https://www.oasis-brains.org},
	Bdsk-Url-1 = {https://www.oasis-brains.org}}

@url{nki,
	date-added = {2020-11-01 07:31:21 -0800},
	date-modified = {2020-11-01 07:42:18 -0800},
	lastchecked = {October 17, 2020},
	title = {{http://fcon\_1000.projects.nitrc.org/indi/pro/nki.html}},
	url = {http://fcon_1000.projects.nitrc.org/indi/pro/nki.html},
	Bdsk-Url-1 = {http://fcon_1000.projects.nitrc.org/indi/pro/nki.html}}

@url{ixi,
	date-added = {2020-11-01 07:28:51 -0800},
	date-modified = {2020-11-01 07:38:58 -0800},
	lastchecked = {October 17, 2020},
	title = {{https://brain-development.org/ixi-dataset/}},
	url = {https://brain-development.org/ixi-dataset/},
	Bdsk-Url-1 = {https://brain-development.org/ixi-dataset/}}

@article{Breiman:2001aa,
	author = {L. Breiman},
	date-added = {2020-10-15 15:57:38 -0700},
	date-modified = {2020-10-15 15:58:30 -0700},
	journal = {Machine Learning},
	number = {1},
	pages = {5--32},
	title = {Random Forests},
	volume = {45},
	year = {2001}}

@incollection{verbeke1997linear,
	author = {Verbeke, Geert},
	booktitle = {Linear mixed models in practice},
	pages = {63--153},
	publisher = {Springer},
	title = {Linear mixed models for longitudinal data},
	year = {1997}}

@article{holbrook2020anterolateral,
	author = {Holbrook, Andrew J and Tustison, Nicholas J and Marquez, Freddie and Roberts, Jared and Yassa, Michael A and Gillen, Daniel L and Alzheimer's Disease Neuroimaging Initiative {\S}},
	journal = {Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring},
	number = {1},
	pages = {e12068},
	publisher = {Wiley Online Library},
	title = {Anterolateral entorhinal cortex thickness as a new biomarker for early detection of {A}lzheimer's disease},
	volume = {12},
	year = {2020}}

@inproceedings{Haris:2018aa,
	abstract = {The feed-forward architectures of recently proposed deep super-resolution networks learn representations of low-resolution inputs, and the non-linear mapping from those to high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and downsampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually-connected up- and down-sampling stages each of which represents different types of image degradation and high-resolution components. We show that extending this idea to allow concatenation of features across up- and downsampling stages (Dense DBPN) allows us to reconstruct further improve super-resolution, yielding superior results and in particular establishing new state of the art results for large scaling factors such as 8× across multiple data sets.},
	author = {M. {Haris} and G. {Shakhnarovich} and N. {Ukita}},
	booktitle = {2018 {IEEE/CVF} {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition},
	date-added = {2020-10-07 12:20:55 -0700},
	date-modified = {2020-10-07 12:38:47 -0700},
	doi = {10.1109/CVPR.2018.00179},
	issn = {2575-7075},
	keywords = {convolutional neural nets;image reconstruction;image resolution;iterative methods;learning (artificial intelligence);high-resolution images;nonlinear mapping;super-resolution networks;feed-forward architectures;back-projection networks;downsampling stages;high-resolution components;image degradation;down-sampling stages;projection errors;error feedback mechanism;Image reconstruction;Feature extraction;Task analysis;Training;Computer architecture;Spatial resolution},
	month = {June},
	pages = {1664-1673},
	title = {Deep Back-Projection Networks for Super-Resolution},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2018.00179}}

@article{Bashyam:2020aa,
	abstract = {Deep learning has emerged as a powerful approach to constructing imaging signatures of normal brain ageing as well as of various neuropathological processes associated with brain diseases. In particular, MRI-derived brain age has been used as a comprehensive biomarker of brain health that can identify both advanced and resilient ageing individuals via deviations from typical brain ageing. Imaging signatures of various brain diseases, including schizophrenia and Alzheimer's disease, have also been identified using machine learning. Prior efforts to derive these indices have been hampered by the need for sophisticated and not easily reproducible processing steps, by insufficiently powered or diversified samples from which typical brain ageing trajectories were derived, and by limited reproducibility across populations and MRI scanners. Herein, we develop and test a sophisticated deep brain network (DeepBrainNet) using a large (n = 11 729) set of MRI scans from a highly diversified cohort spanning different studies, scanners, ages and geographic locations around the world. Tests using both cross-validation and a separate replication cohort of 2739 individuals indicate that DeepBrainNet obtains robust brain-age estimates from these diverse datasets without the need for specialized image data preparation and processing. Furthermore, we show evidence that moderately fit brain ageing models may provide brain age estimates that are most discriminant of individuals with pathologies. This is not unexpected as tightly-fitting brain age models naturally produce brain-age estimates that offer little information beyond age, and loosely fitting models may contain a lot of noise. Our results offer some experimental evidence against commonly pursued tightly-fitting models. We show that the moderately fitting brain age models obtain significantly higher differentiation compared to tightly-fitting models in two of the four disease groups tested. Critically, we demonstrate that leveraging DeepBrainNet, along with transfer learning, allows us to construct more accurate classifiers of several brain diseases, compared to directly training classifiers on patient versus healthy control datasets or using common imaging databases such as ImageNet. We, therefore, derive a domain-specific deep network likely to reduce the need for application-specific adaptation and tuning of generic deep learning networks. We made the DeepBrainNet model freely available to the community for MRI-based evaluation of brain health in the general population and over the lifespan.},
	author = {Bashyam, Vishnu M and Erus, Guray and Doshi, Jimit and Habes, Mohamad and Nasralah, Ilya and Truelove-Hill, Monica and Srinivasan, Dhivya and Mamourian, Liz and Pomponio, Raymond and Fan, Yong and Launer, Lenore J and Masters, Colin L and Maruff, Paul and Zhuo, Chuanjun and V{\"o}lzke, Henry and Johnson, Sterling C and Fripp, Jurgen and Koutsouleris, Nikolaos and Satterthwaite, Theodore D and Wolf, Daniel and Gur, Raquel E and Gur, Ruben C and Morris, John and Albert, Marilyn S and Grabe, Hans J and Resnick, Susan and Bryan, R Nick and Wolk, David A and Shou, Haochang and Davatzikos, Christos},
	date-added = {2020-10-07 09:36:21 -0700},
	date-modified = {2020-10-07 09:39:51 -0700},
	doi = {10.1093/brain/awaa160},
	journal = {Brain},
	journal-full = {Brain : a journal of neurology},
	keywords = {brain age; deep learning; transfer learning},
	month = {Jul},
	number = {7},
	pages = {2312-2324},
	pmc = {PMC7364766},
	pmid = {32591831},
	pst = {ppublish},
	title = {MRI signatures of brain age and disease over the lifespan based on a deep brain network and 14,468 individuals worldwide},
	volume = {143},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1093/brain/awaa160}}

@article{Clarkson:2011aa,
	abstract = {Cortical thickness estimation performed in-vivo via magnetic resonance imaging is an important technique for the diagnosis and understanding of the progression of neurodegenerative diseases. Currently, two different computational paradigms exist, with methods generally classified as either surface or voxel-based. This paper provides a much needed comparison of the surface-based method FreeSurfer and two voxel-based methods using clinical data. We test the effects of computing regional statistics using two different atlases and demonstrate that this makes a significant difference to the cortical thickness results. We assess reproducibility, and show that FreeSurfer has a regional standard deviation of thickness difference on same day scans that is significantly lower than either a Laplacian or Registration based method and discuss the trade off between reproducibility and segmentation accuracy caused by bending energy constraints. We demonstrate that voxel-based methods can detect similar patterns of group-wise differences as well as FreeSurfer in typical applications such as producing group-wise maps of statistically significant thickness change, but that regional statistics can vary between methods. We use a Support Vector Machine to classify patients against controls and did not find statistically significantly different results with voxel based methods compared to FreeSurfer. Finally we assessed longitudinal performance and concluded that currently FreeSurfer provides the most plausible measure of change over time, with further work required for voxel based methods.},
	author = {Clarkson, Matthew J and Cardoso, M Jorge and Ridgway, Gerard R and Modat, Marc and Leung, Kelvin K and Rohrer, Jonathan D and Fox, Nick C and Ourselin, S{\'e}bastien},
	date-added = {2020-10-01 10:24:00 -0700},
	date-modified = {2020-10-01 10:24:00 -0700},
	doi = {10.1016/j.neuroimage.2011.05.053},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Aged; Alzheimer Disease; Brain; Brain Mapping; Female; Frontotemporal Dementia; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Reproducibility of Results},
	month = {Aug},
	number = {3},
	pages = {856-65},
	pmid = {21640841},
	pst = {ppublish},
	title = {A comparison of voxel and surface based cortical thickness estimation methods},
	volume = {57},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2011.05.053}}

@article{Schwarz:2016aa,
	abstract = {Alzheimer's disease (AD) researchers commonly use MRI as a quantitative measure of disease severity. Historically, hippocampal volume has been favored. Recently, "AD signature" measurements of gray matter (GM) volumes or cortical thicknesses have gained attention. Here, we systematically evaluate multiple thickness- and volume-based candidate-methods side-by-side, built using the popular FreeSurfer, SPM, and ANTs packages, according to the following criteria: (a) ability to separate clinically normal individuals from those with AD; (b) (extent of) correlation with head size, a nuisance covariatel (c) reliability on repeated scans; and (d) correlation with Braak neurofibrillary tangle stage in a group with autopsy. We show that volume- and thickness-based measures generally perform similarly for separating clinically normal from AD populations, and in correlation with Braak neurofibrillary tangle stage at autopsy. Volume-based measures are generally more reliable than thickness measures. As expected, volume measures are highly correlated with head size, while thickness measures are generally not. Because approaches to statistically correcting volumes for head size vary and may be inadequate to deal with this underlying confound, and because our goal is to determine a measure which can be used to examine age and sex effects in a cohort across a large age range, we thus recommend thickness-based measures. Ultimately, based on these criteria and additional practical considerations of run-time and failure rates, we recommend an AD signature measure formed from a composite of thickness measurements in the entorhinal, fusiform, parahippocampal, mid-temporal, inferior-temporal, and angular gyrus ROIs using ANTs with input segmentations from SPM12.},
	author = {Schwarz, Christopher G and Gunter, Jeffrey L and Wiste, Heather J and Przybelski, Scott A and Weigand, Stephen D and Ward, Chadwick P and Senjem, Matthew L and Vemuri, Prashanthi and Murray, Melissa E and Dickson, Dennis W and Parisi, Joseph E and Kantarci, Kejal and Weiner, Michael W and Petersen, Ronald C and Jack, Jr, Clifford R and {Alzheimer's Disease Neuroimaging Initiative}},
	date-added = {2020-10-01 10:21:34 -0700},
	date-modified = {2020-10-01 10:21:34 -0700},
	doi = {10.1016/j.nicl.2016.05.017},
	journal = {Neuroimage Clin},
	journal-full = {NeuroImage. Clinical},
	mesh = {Adult; Aged; Aged, 80 and over; Alzheimer Disease; Brain Mapping; Cerebral Cortex; Cognitive Dysfunction; Cohort Studies; Datasets as Topic; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Psychiatric Status Rating Scales},
	pages = {802-812},
	pmc = {PMC5187496},
	pmid = {28050342},
	pst = {epublish},
	title = {A large-scale comparison of cortical thickness and volume methods for measuring Alzheimer's disease severity},
	volume = {11},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.nicl.2016.05.017}}

@article{Nyul:1999aa,
	abstract = {The lack of a standard image intensity scale in MRI causes many difficulties in image display and analysis. A two-step postprocessing method is proposed for standardizing the intensity scale in such a way that for the same MR protocol and body region, similar intensities will have similar tissue meaning. In the first step, the parameters of the standardizing transformation are "learned" from a set of images. In the second step, for each MR study these parameters are used to map their histogram into the standardized histogram. The method was tested quantitatively on 90 whole-brain studies of multiple sclerosis patients for several protocols and qualitatively for several other protocols and body regions. Measurements using mean squared difference showed that the standardized image intensities have statistically significantly (P < 0.01) more consistent range and meaning than the originals. Fixed gray level windows can be established for the standardized images and used for display without the need of per case adjustment. Preliminary results also indicate that the method facilitates improving the degree of automation of image segmentation. Magn Reson Med 42:1072-1081, 1999.},
	author = {Ny{\'u}l, L G and Udupa, J K},
	date-added = {2020-10-01 08:52:44 -0700},
	date-modified = {2020-10-07 09:53:00 -0700},
	doi = {10.1002/(sici)1522-2594(199912)42:6<1072::aid-mrm11>3.0.co;2-m},
	journal = {Magn Reson Med},
	journal-full = {Magnetic resonance in medicine},
	mesh = {Algorithms; Brain; Data Display; Foot; Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Multiple Sclerosis},
	month = {Dec},
	number = {6},
	pages = {1072-81},
	pmid = {10571928},
	pst = {ppublish},
	title = {On standardizing the {MR} image intensity scale},
	volume = {42},
	year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1002/(sici)1522-2594(199912)42:6%3C1072::aid-mrm11%3E3.0.co;2-m}}

@article{deepscan,
	archiveprefix = {arXiv},
	author = {Richard McKinley and Michael Rebsamen and Raphael Meier and Mauricio Reyes and Christian Rummel and Roland Wiest},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1904-02436.bib},
	date-added = {2020-09-30 15:57:38 -0700},
	date-modified = {2020-09-30 15:57:38 -0700},
	eprint = {1904.02436},
	journal = {CoRR},
	timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
	title = {Few-shot brain segmentation from weakly labeled data with deep heteroscedastic multi-task networks},
	url = {http://arxiv.org/abs/1904.02436},
	volume = {abs/1904.02436},
	year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1904.02436}}

@article{gelman2006prior,
	abstract = {Various noninformative prior distributions have been suggested for
scale parameters in hierarchical models. We construct a new folded-noncentral-t
family of conditionally conjugate priors for hierarchical standard deviation parameters,
and then consider noninformative and weakly informative priors in this
family. We use an example to illustrate serious problems with the inverse-gamma
family of ?noninformative? prior distributions. We suggest instead to use a uniform
prior on the hierarchical standard deviation, using the half-t family when the
number of groups is small and in other settings where a weakly informative prior
is desired. We also illustrate the use of the half-t family for hierarchical modeling
of multiple variance parameters such as arise in the analysis of variance.},
	author = {Gelman, Andrew and others},
	date-added = {2020-09-29 14:39:56 -0700},
	date-modified = {2020-09-29 14:39:56 -0700},
	journal = {Bayesian analysis},
	number = {3},
	pages = {515--534},
	publisher = {International Society for Bayesian Analysis},
	title = {Prior distributions for variance parameters in hierarchical models (comment on article by {B}rowne and {D}raper)},
	volume = {1},
	year = {2006}}

@article{Reuter:2011aa,
	abstract = {Longitudinal image processing procedures frequently transfer or pool information across time within subject, with the dual goals of reducing the variability and increasing the accuracy of the derived measures. In this note, we discuss common difficulties in longitudinal image processing, focusing on the introduction of bias, and describe the approaches we have taken to avoid them in the FreeSurfer longitudinal processing stream.},
	author = {Reuter, Martin and Fischl, Bruce},
	date-added = {2020-09-29 14:02:12 -0700},
	date-modified = {2020-09-29 14:02:12 -0700},
	doi = {10.1016/j.neuroimage.2011.02.076},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Bias; Humans; Image Interpretation, Computer-Assisted; Longitudinal Studies},
	month = {Jul},
	number = {1},
	pages = {19-21},
	pmc = {PMC3260043},
	pmid = {21376812},
	pst = {ppublish},
	title = {Avoiding asymmetry-induced bias in longitudinal image processing},
	volume = {57},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2011.02.076}}

@article{Reuter:2012aa,
	abstract = {Longitudinal image analysis has become increasingly important in clinical studies of normal aging and neurodegenerative disorders. Furthermore, there is a growing appreciation of the potential utility of longitudinally acquired structural images and reliable image processing to evaluate disease modifying therapies. Challenges have been related to the variability that is inherent in the available cross-sectional processing tools, to the introduction of bias in longitudinal processing and to potential over-regularization. In this paper we introduce a novel longitudinal image processing framework, based on unbiased, robust, within-subject template creation, for automatic surface reconstruction and segmentation of brain MRI of arbitrarily many time points. We demonstrate that it is essential to treat all input images exactly the same as removing only interpolation asymmetries is not sufficient to remove processing bias. We successfully reduce variability and avoid over-regularization by initializing the processing in each time point with common information from the subject template. The presented results show a significant increase in precision and discrimination power while preserving the ability to detect large anatomical deviations; as such they hold great potential in clinical applications, e.g. allowing for smaller sample sizes or shorter trials to establish disease specific biomarkers or to quantify drug effects.},
	author = {Reuter, Martin and Schmansky, Nicholas J and Rosas, H Diana and Fischl, Bruce},
	date-added = {2020-09-29 14:01:24 -0700},
	date-modified = {2020-09-29 14:01:24 -0700},
	doi = {10.1016/j.neuroimage.2012.02.084},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Brain; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging},
	month = {Jul},
	number = {4},
	pages = {1402-18},
	pmc = {PMC3389460},
	pmid = {22430496},
	pst = {ppublish},
	title = {Within-subject template estimation for unbiased longitudinal image analysis},
	volume = {61},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2012.02.084}}

@article{Kriegeskorte:2009aa,
	abstract = {A neuroscientific experiment typically generates a large amount of data, of which only a small fraction is analyzed in detail and presented in a publication. However, selection among noisy measurements can render circular an otherwise appropriate analysis and invalidate results. Here we argue that systems neuroscience needs to adjust some widespread practices to avoid the circularity that can arise from selection. In particular, 'double dipping', the use of the same dataset for selection and selective analysis, will give distorted descriptive statistics and invalid statistical inference whenever the results statistics are not inherently independent of the selection criteria under the null hypothesis. To demonstrate the problem, we apply widely used analyses to noise data known to not contain the experimental effects in question. Spurious effects can appear in the context of both univariate activation analysis and multivariate pattern-information analysis. We suggest a policy for avoiding circularity.},
	author = {Kriegeskorte, Nikolaus and Simmons, W Kyle and Bellgowan, Patrick S F and Baker, Chris I},
	date-added = {2020-09-28 18:39:45 -0700},
	date-modified = {2020-09-28 18:39:45 -0700},
	doi = {10.1038/nn.2303},
	journal = {Nat Neurosci},
	journal-full = {Nature neuroscience},
	mesh = {Animals; Artifacts; Data Collection; Data Interpretation, Statistical; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neurosciences; Reproducibility of Results; Selection Bias; Systems Biology},
	month = {May},
	number = {5},
	pages = {535-40},
	pmc = {PMC2841687},
	pmid = {19396166},
	pst = {ppublish},
	title = {Circular analysis in systems neuroscience: the dangers of double dipping},
	volume = {12},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1038/nn.2303}}

@article{Lemaitre:2012aa,
	abstract = {Normal aging is accompanied by global as well as regional structural changes. While these age-related changes in gray matter volume have been extensively studied, less has been done using newer morphological indexes, such as cortical thickness and surface area. To this end, we analyzed structural images of 216 healthy volunteers, ranging from 18 to 87 years of age, using a surface-based automated parcellation approach. Linear regressions of age revealed a concomitant global age-related reduction in cortical thickness, surface area and volume. Cortical thickness and volume collectively confirmed the vulnerability of the prefrontal cortex, whereas in other cortical regions, such as in the parietal cortex, thickness was the only measure sensitive to the pronounced age-related atrophy. No cortical regions showed more surface area reduction than the global average. The distinction between these morphological measures may provide valuable information to dissect age-related structural changes of the brain, with each of these indexes probably reflecting specific histological changes occurring during aging.},
	author = {Lemaitre, Herve and Goldman, Aaron L and Sambataro, Fabio and Verchinski, Beth A and Meyer-Lindenberg, Andreas and Weinberger, Daniel R and Mattay, Venkata S},
	date-added = {2020-09-28 18:21:19 -0700},
	date-modified = {2020-09-28 18:21:19 -0700},
	doi = {10.1016/j.neurobiolaging.2010.07.013},
	journal = {Neurobiol Aging},
	journal-full = {Neurobiology of aging},
	mesh = {Adolescent; Adult; Aged; Aged, 80 and over; Aging; Atrophy; Cerebral Cortex; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Nerve Degeneration; Young Adult},
	month = {Mar},
	number = {3},
	pages = {617.e1-9},
	pmc = {PMC3026893},
	pmid = {20739099},
	pst = {ppublish},
	title = {Normal age-related brain morphometric changes: nonuniformity across cortical thickness, surface area and gray matter volume?},
	volume = {33},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neurobiolaging.2010.07.013}}

@article{Landman:2011aa,
	abstract = {Modern MRI image processing methods have yielded quantitative, morphometric, functional, and structural assessments of the human brain. These analyses typically exploit carefully optimized protocols for specific imaging targets. Algorithm investigators have several excellent public data resources to use to test, develop, and optimize their methods. Recently, there has been an increasing focus on combining MRI protocols in multi-parametric studies. Notably, these have included innovative approaches for fusing connectivity inferences with functional and/or anatomical characterizations. Yet, validation of the reproducibility of these interesting and novel methods has been severely hampered by the limited availability of appropriate multi-parametric data. We present an imaging protocol optimized to include state-of-the-art assessment of brain function, structure, micro-architecture, and quantitative parameters within a clinically feasible 60-min protocol on a 3-T MRI scanner. We present scan-rescan reproducibility of these imaging contrasts based on 21 healthy volunteers (11 M/10 F, 22-61 years old). The cortical gray matter, cortical white matter, ventricular cerebrospinal fluid, thalamus, putamen, caudate, cerebellar gray matter, cerebellar white matter, and brainstem were identified with mean volume-wise reproducibility of 3.5%. We tabulate the mean intensity, variability, and reproducibility of each contrast in a region of interest approach, which is essential for prospective study planning and retrospective power analysis considerations. Anatomy was highly consistent on structural acquisition (~1-5% variability), while variation on diffusion and several other quantitative scans was higher (~<10%). Some sequences are particularly variable in specific structures (ASL exhibited variation of 28% in the cerebral white matter) or in thin structures (quantitative T2 varied by up to 73% in the caudate) due, in large part, to variability in automated ROI placement. The richness of the joint distribution of intensities across imaging methods can be best assessed within the context of a particular analysis approach as opposed to a summary table. As such, all imaging data and analysis routines have been made publicly and freely available. This effort provides the neuroimaging community with a resource for optimization of algorithms that exploit the diversity of modern MRI modalities. Additionally, it establishes a baseline for continuing development and optimization of multi-parametric imaging protocols.},
	author = {Landman, Bennett A and Huang, Alan J and Gifford, Aliya and Vikram, Deepti S and Lim, Issel Anne L and Farrell, Jonathan A D and Bogovic, John A and Hua, Jun and Chen, Min and Jarso, Samson and Smith, Seth A and Joel, Suresh and Mori, Susumu and Pekar, James J and Barker, Peter B and Prince, Jerry L and van Zijl, Peter C M},
	date-added = {2020-09-28 16:25:58 -0700},
	date-modified = {2020-10-07 12:36:11 -0700},
	doi = {10.1016/j.neuroimage.2010.11.047},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Adult; Brain; Brain Mapping; Female; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Reproducibility of Results; Young Adult},
	month = {Feb},
	number = {4},
	pages = {2854-66},
	pmc = {PMC3020263},
	pmid = {21094686},
	pst = {ppublish},
	title = {Multi-parametric neuroimaging reproducibility: a {3-T} resource study},
	volume = {54},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2010.11.047}}

@article{Tustison:2019ac,
	abstract = {RATIONALE AND OBJECTIVES: We propose an automated segmentation pipeline based on deep learning for proton lung MRI segmentation and ventilation-based quantification which improves on our previously reported methodologies in terms of computational efficiency while demonstrating accuracy and robustness. The large data requirement for the proposed framework is made possible by a novel template-based data augmentation strategy. Supporting this work is the open-source ANTsRNet-a growing repository of well-known deep learning architectures first introduced here.
MATERIALS AND METHODS: Deep convolutional neural network (CNN) models were constructed and trained using a custom multilabel Dice metric loss function and a novel template-based data augmentation strategy. Training (including template generation and data augmentation) employed 205 proton MR images and 73 functional lung MRI. Evaluation was performed using data sets of size 63 and 40 images, respectively.
RESULTS: Accuracy for CNN-based proton lung MRI segmentation (in terms of Dice overlap) was left lung: 0.93 $\pm$ 0.03, right lung: 0.94 $\pm$ 0.02, and whole lung: 0.94 $\pm$ 0.02. Although slightly less accurate than our previously reported joint label fusion approach (left lung: 0.95 $\pm$ 0.02, right lung: 0.96 $\pm$ 0.01, and whole lung: 0.96 $\pm$ 0.01), processing time is <1 second per subject for the proposed approach versus ∼30 minutes per subject using joint label fusion. Accuracy for quantifying ventilation defects was determined based on a consensus labeling where average accuracy (Dice multilabel overlap of ventilation defect regions plus normal region) was 0.94 for the CNN method; 0.92 for our previously reported method; and 0.90, 0.92, and 0.94 for expert readers.
CONCLUSION: The proposed framework yields accurate automated quantification in near real time. CNNs drastically reduce processing time after offline model construction and demonstrate significant future potential for facilitating quantitative analysis of functional lung MRI.},
	author = {Tustison, Nicholas J and Avants, Brian B and Lin, Zixuan and Feng, Xue and Cullen, Nicholas and Mata, Jaime F and Flors, Lucia and Gee, James C and Altes, Talissa A and Mugler Iii, John P and Qing, Kun},
	date-added = {2020-09-28 16:09:25 -0700},
	date-modified = {2020-09-28 16:09:25 -0700},
	doi = {10.1016/j.acra.2018.08.003},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	keywords = {ANTsRNet; Advanced Normalization Tools; Hyperpolarized gas imaging; Neural networks; Proton lung MRI; U-net},
	mesh = {Computer Simulation; Datasets as Topic; Deep Learning; Humans; Lung; Magnetic Resonance Imaging; Protons; Pulmonary Ventilation},
	month = {03},
	number = {3},
	pages = {412-423},
	pmc = {PMC6397788},
	pmid = {30195415},
	pst = {ppublish},
	title = {Convolutional Neural Networks with Template-Based Data Augmentation for Functional Lung Image Quantification},
	volume = {26},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.acra.2018.08.003}}

@article{Schlemper:2019aa,
	abstract = {We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U-Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention mechanism can provide efficient object localisation while improving the overall prediction performance by reducing false positives. For segmentation, the proposed architecture is evaluated on two large 3D CT abdominal datasets with manual annotations for multiple organs. Experimental results show that AG models consistently improve the prediction performance of the base architectures across different datasets and training sizes while preserving computational efficiency. Moreover, AGs guide the model activations to be focused around salient regions, which provides better insights into how model predictions are made. The source code for the proposed AG models is publicly available.},
	author = {Schlemper, Jo and Oktay, Ozan and Schaap, Michiel and Heinrich, Mattias and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	date-added = {2020-09-28 16:03:07 -0700},
	date-modified = {2020-09-28 16:03:07 -0700},
	doi = {10.1016/j.media.2019.01.012},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	keywords = {Attention gates; Fully convolutional networks; Image classification; Localisation; Segmentation; Soft attention},
	mesh = {Algorithms; Datasets as Topic; Female; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Neural Networks, Computer; Pregnancy; Radiography, Abdominal; Tomography, X-Ray Computed; Ultrasonography, Prenatal},
	month = {04},
	pages = {197-207},
	pmid = {30802813},
	pst = {ppublish},
	title = {Attention gated networks: Learning to leverage salient regions in medical images},
	volume = {53},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.media.2019.01.012}}

@article{Fonov:2009aa,
	author = {V. S. Fonov and A. C. Evans and R. C. McKinstry and C. Almli and D. L. Collins},
	date-added = {2020-09-24 20:31:32 -0700},
	date-modified = {2020-09-24 20:32:51 -0700},
	journal = {NeuroImage},
	title = {Unbiased nonlinear average age-appropriate brain templates from birth to adulthood},
	volume = {S102},
	year = {2009}}

@article{Klein:2012aa,
	abstract = {We introduce the Mindboggle-101 dataset, the largest and most complete set of free, publicly accessible, manually labeled human brain images. To manually label the macroscopic anatomy in magnetic resonance images of 101 healthy participants, we created a new cortical labeling protocol that relies on robust anatomical landmarks and minimal manual edits after initialization with automated labels. The "Desikan-Killiany-Tourville" (DKT) protocol is intended to improve the ease, consistency, and accuracy of labeling human cortical areas. Given how difficult it is to label brains, the Mindboggle-101 dataset is intended to serve as brain atlases for use in labeling other brains, as a normative dataset to establish morphometric variation in a healthy population for comparison against clinical populations, and contribute to the development, training, testing, and evaluation of automated registration and labeling algorithms. To this end, we also introduce benchmarks for the evaluation of such algorithms by comparing our manual labels with labels automatically generated by probabilistic and multi-atlas registration-based approaches. All data and related software and updated information are available on the http://mindboggle.info/data website.},
	author = {Klein, Arno and Tourville, Jason},
	date-added = {2020-09-24 20:06:39 -0700},
	date-modified = {2020-09-24 20:06:39 -0700},
	doi = {10.3389/fnins.2012.00171},
	journal = {Front Neurosci},
	journal-full = {Frontiers in neuroscience},
	keywords = {MRI; anatomy; cerebral cortex; human brain; labeling; parcellation; segmentation},
	pages = {171},
	pmc = {PMC3514540},
	pmid = {23227001},
	pst = {epublish},
	title = {101 labeled brain images and a consistent human cortical labeling protocol},
	volume = {6},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.3389/fnins.2012.00171}}

@article{Ashburner:2000aa,
	abstract = {At its simplest, voxel-based morphometry (VBM) involves a voxel-wise comparison of the local concentration of gray matter between two groups of subjects. The procedure is relatively straightforward and involves spatially normalizing high-resolution images from all the subjects in the study into the same stereotactic space. This is followed by segmenting the gray matter from the spatially normalized images and smoothing the gray-matter segments. Voxel-wise parametric statistical tests which compare the smoothed gray-matter images from the two groups are performed. Corrections for multiple comparisons are made using the theory of Gaussian random fields. This paper describes the steps involved in VBM, with particular emphasis on segmenting gray matter from MR images with nonuniformity artifact. We provide evaluations of the assumptions that underpin the method, including the accuracy of the segmentation and the assumptions made about the statistical distribution of the data.},
	author = {Ashburner, J and Friston, K J},
	date-added = {2020-09-24 19:59:20 -0700},
	date-modified = {2020-09-24 19:59:20 -0700},
	doi = {10.1006/nimg.2000.0582},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Brain; False Positive Reactions; Humans; Magnetic Resonance Imaging; Models, Neurological; Periaqueductal Gray; Random Allocation},
	month = {Jun},
	number = {6 Pt 1},
	pages = {805-21},
	pmid = {10860804},
	pst = {ppublish},
	title = {Voxel-based morphometry--the methods},
	volume = {11},
	year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1006/nimg.2000.0582}}

@article{Avants:2012aa,
	abstract = {We contribute a novel and interpretable dimensionality reduction strategy, eigenanatomy, that is tuned for neuroimaging data. The method approximates the eigendecomposition of an image set with basis functions (the eigenanatomy vectors) that are sparse, unsigned and are anatomically clustered. We employ the eigenanatomy vectors as anatomical predictors to improve detection power in morphometry. Standard voxel-based morphometry (VBM) analyzes imaging data voxel-by-voxel--and follows this with cluster-based or voxel-wise multiple comparisons correction methods to determine significance. Eigenanatomy reverses the standard order of operations by first clustering the voxel data and then using standard linear regression in this reduced dimensionality space. As with traditional region-of-interest (ROI) analysis, this strategy can greatly improve detection power. Our results show that eigenanatomy provides a principled objective function that leads to localized, data-driven regions of interest. These regions improve our ability to quantify biologically plausible rates of cortical change in two distinct forms of neurodegeneration. We detail the algorithm and show experimental evidence of its efficacy.},
	author = {Avants, Brian and Dhillon, Paramveer and Kandel, Benjamin M and Cook, Philip A and McMillan, Corey T and Grossman, Murray and Gee, James C},
	date-added = {2020-09-24 19:56:52 -0700},
	date-modified = {2020-09-24 19:56:52 -0700},
	doi = {10.1007/978-3-642-33454-2_26},
	journal = {Med Image Comput Comput Assist Interv},
	journal-full = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	mesh = {Aging; Algorithms; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Longitudinal Studies; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity},
	number = {Pt 3},
	pages = {206-13},
	pmc = {PMC3653970},
	pmid = {23286132},
	pst = {ppublish},
	title = {Eigenanatomy improves detection power for longitudinal cortical change},
	volume = {15},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-33454-2_26}}

@article{Henschel:2020aa,
	abstract = {Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, replicating FreeSurfer's anatomical segmentation including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole-brain segmentation into 95 classes. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and subcortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (in under 1 ​min) and surface-based thickness analysis (within only around 1 ​h runtime). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and high sensitivity to group differences in dementia.},
	author = {Henschel, Leonie and Conjeti, Sailesh and Estrada, Santiago and Diers, Kersten and Fischl, Bruce and Reuter, Martin},
	date-added = {2020-09-23 21:13:41 -0700},
	date-modified = {2020-09-23 21:13:41 -0700},
	doi = {10.1016/j.neuroimage.2020.117012},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	keywords = {Artificial intelligence; Computational neuroimaging; Deep learning; Freesurfer; Structural MRI},
	month = {Oct},
	pages = {117012},
	pmid = {32526386},
	pst = {ppublish},
	title = {FastSurfer - A fast and accurate deep learning based neuroimaging pipeline},
	volume = {219},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2020.117012}}

@article{Rebsamen:2020aa,
	abstract = {Accurate and reliable measures of cortical thickness from magnetic resonance imaging are an important biomarker to study neurodegenerative and neurological disorders. Diffeomorphic registration-based cortical thickness (DiReCT) is a known technique to derive such measures from non-surface-based volumetric tissue maps. ANTs provides an open-source method for estimating cortical thickness, derived by applying DiReCT to an atlas-based segmentation. In this paper, we propose DL+DiReCT, a method using high-quality deep learning-based neuroanatomy segmentations followed by DiReCT, yielding accurate and reliable cortical thickness measures in a short time. We evaluate the methods on two independent datasets and compare the results against surface-based measures from FreeSurfer. Good correlation of DL+DiReCT with FreeSurfer was observed (r = .887) for global mean cortical thickness compared to ANTs versus FreeSurfer (r = .608). Experiments suggest that both DiReCT-based methods had higher sensitivity to changes in cortical thickness than Freesurfer. However, while ANTs showed low scan-rescan robustness, DL+DiReCT showed similar robustness to Freesurfer. Effect-sizes for group-wise differences of healthy controls compared to individuals with dementia were highest with the deep learning-based segmentation. DL+DiReCT is a promising combination of a deep learning-based method with a traditional registration technique to detect subtle changes in cortical thickness.},
	author = {Rebsamen, Michael and Rummel, Christian and Reyes, Mauricio and Wiest, Roland and McKinley, Richard},
	date-added = {2020-09-15 11:54:37 -0700},
	date-modified = {2020-09-15 11:54:37 -0700},
	doi = {10.1002/hbm.25159},
	journal = {Hum Brain Mapp},
	journal-full = {Human brain mapping},
	keywords = {MRI; brain morphometry; cortical thickness; deep learning; diffeomorphic registration; gray matter atrophy; neuroanatomy segmentation},
	month = {Aug},
	pmid = {32786059},
	pst = {aheadofprint},
	title = {Direct cortical thickness estimation using deep learning-based anatomy segmentation and cortex parcellation},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1002/hbm.25159}}

@article{Lei:2019aa,
	abstract = {PURPOSE: Automated synthetic computed tomography (sCT) generation based on magnetic resonance imaging (MRI) images would allow for MRI-only based treatment planning in radiation therapy, eliminating the need for CT simulation and simplifying the patient treatment workflow. In this work, the authors propose a novel method for generation of sCT based on dense cycle-consistent generative adversarial networks (cycle GAN), a deep-learning based model that trains two transformation mappings (MRI to CT and CT to MRI) simultaneously.
METHODS AND MATERIALS: The cycle GAN-based model was developed to generate sCT images in a patch-based framework. Cycle GAN was applied to this problem because it includes an inverse transformation from CT to MRI, which helps constrain the model to learn a one-to-one mapping. Dense block-based networks were used to construct generator of cycle GAN. The network weights and variables were optimized via a gradient difference (GD) loss and a novel distance loss metric between sCT and original CT.
RESULTS: Leave-one-out cross-validation was performed to validate the proposed model. The mean absolute error (MAE), peak signal-to-noise ratio (PSNR), and normalized cross correlation (NCC) indexes were used to quantify the differences between the sCT and original planning CT images. For the proposed method, the mean MAE between sCT and CT were 55.7 Hounsfield units (HU) for 24 brain cancer patients and 50.8 HU for 20 prostate cancer patients. The mean PSNR and NCC were 26.6 dB and 0.963 in the brain cases, and 24.5 dB and 0.929 in the pelvis.
CONCLUSION: We developed and validated a novel learning-based approach to generate CT images from routine MRIs based on dense cycle GAN model to effectively capture the relationship between the CT and MRIs. The proposed method can generate robust, high-quality sCT in minutes. The proposed method offers strong potential for supporting near real-time MRI-only treatment planning in the brain and pelvis.},
	author = {Lei, Yang and Harms, Joseph and Wang, Tonghe and Liu, Yingzi and Shu, Hui-Kuo and Jani, Ashesh B and Curran, Walter J and Mao, Hui and Liu, Tian and Yang, Xiaofeng},
	date-added = {2020-08-27 15:40:05 -0700},
	date-modified = {2020-08-27 15:40:05 -0700},
	doi = {10.1002/mp.13617},
	journal = {Med Phys},
	journal-full = {Medical physics},
	keywords = {MRI-only based radiotherapy; cycle consistent generative adversarial networks; deeply supervised network; dense convolutional networks; synthetic CT},
	mesh = {Image Processing, Computer-Assisted; Machine Learning; Magnetic Resonance Imaging; Tomography, X-Ray Computed},
	month = {Aug},
	number = {8},
	pages = {3565-3581},
	pmc = {PMC6692192},
	pmid = {31112304},
	pst = {ppublish},
	title = {MRI-only based synthetic CT generation using dense cycle consistent generative adversarial networks},
	volume = {46},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1002/mp.13617}}

@article{Robinson:2018aa,
	abstract = {Lewy bodies commonly occur in Alzheimer's disease, and Alzheimer's disease pathology is frequent in Lewy body diseases, but the burden of co-pathologies across neurodegenerative diseases is unknown. We assessed the extent of tau, amyloid-β, α-synuclein and TDP-43 proteinopathies in 766 autopsied individuals representing a broad spectrum of clinical neurodegenerative disease. We interrogated pathological Alzheimer's disease (n = 247); other tauopathies (n = 95) including Pick's disease, corticobasal disease and progressive supranuclear palsy; the synucleinopathies (n = 164) including multiple system atrophy and Lewy body disease; the TDP-43 proteinopathies (n = 188) including frontotemporal lobar degeneration with TDP-43 inclusions and amyotrophic lateral sclerosis; and a minimal pathology group (n = 72). Each group was divided into subgroups without or with co-pathologies. Age and sex matched logistic regression models compared co-pathology prevalence between groups. Co-pathology prevalence was similar between the minimal pathology group and most neurodegenerative diseases for each proteinopathy: tau was nearly universal (92-100%), amyloid-β common (20-57%); α-synuclein less common (4-16%); and TDP-43 the rarest (0-16%). In several neurodegenerative diseases, co-pathology increased: in Alzheimer's disease, α-synuclein (41-55%) and TDP-43 (33-40%) increased; in progressive supranuclear palsy, α-synuclein increased (22%); in corticobasal disease, TDP-43 increased (24%); and in neocortical Lewy body disease, amyloid-β (80%) and TDP-43 (22%) increased. Total co-pathology prevalence varied across groups (27-68%), and was increased in high Alzheimer's disease, progressive supranuclear palsy, and neocortical Lewy body disease (70-81%). Increased age at death was observed in the minimal pathology group, amyotrophic lateral sclerosis, and multiple system atrophy cases with co-pathologies. In amyotrophic lateral sclerosis and neocortical Lewy body disease, co-pathologies associated with APOE ɛ4. Lewy body disease cases with Alzheimer's disease co-pathology had substantially lower Mini-Mental State Examination scores than pure Lewy body disease. Our data imply that increased age and APOE ɛ4 status are risk factors for co-pathologies independent of neurodegenerative disease; that neurodegenerative disease severity influences co-pathology as evidenced by the prevalence of co-pathology in high Alzheimer's disease and neocortical Lewy body disease, but not intermediate Alzheimer's disease or limbic Lewy body disease; and that tau and α-synuclein strains may also modify co-pathologies since tauopathies and synucleinopathies had differing co-pathologies and burdens. These findings have implications for clinical trials that focus on monotherapies targeting tau, amyloid-β, α-synuclein and TDP-43.},
	author = {Robinson, John L and Lee, Edward B and Xie, Sharon X and Rennert, Lior and Suh, EunRan and Bredenberg, Colin and Caswell, Carrie and Van Deerlin, Vivianna M and Yan, Ning and Yousef, Ahmed and Hurtig, Howard I and Siderowf, Andrew and Grossman, Murray and McMillan, Corey T and Miller, Bruce and Duda, John E and Irwin, David J and Wolk, David and Elman, Lauren and McCluskey, Leo and Chen-Plotkin, Alice and Weintraub, Daniel and Arnold, Steven E and Brettschneider, Johannes and Lee, Virginia M-Y and Trojanowski, John Q},
	date-added = {2020-08-27 14:12:21 -0700},
	date-modified = {2020-08-27 14:12:21 -0700},
	doi = {10.1093/brain/awy146},
	journal = {Brain},
	journal-full = {Brain : a journal of neurology},
	mesh = {Aged; Alzheimer Disease; Amyotrophic Lateral Sclerosis; Apolipoprotein E4; DNA-Binding Proteins; Female; Humans; Inclusion Bodies; Lewy Bodies; Lewy Body Disease; Male; Middle Aged; Multiple System Atrophy; Neurodegenerative Diseases; Pick Disease of the Brain; Prevalence; Supranuclear Palsy, Progressive; TDP-43 Proteinopathies; Tauopathies; alpha-Synuclein; tau Proteins},
	month = {07},
	number = {7},
	pages = {2181-2193},
	pmc = {PMC6022546},
	pmid = {29878075},
	pst = {ppublish},
	title = {Neurodegenerative disease concomitant proteinopathies are prevalent, age-related and APOE4-associated},
	volume = {141},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1093/brain/awy146}}

@article{Lein:2007aa,
	abstract = {Molecular approaches to understanding the functional circuitry of the nervous system promise new insights into the relationship between genes, brain and behaviour. The cellular diversity of the brain necessitates a cellular resolution approach towards understanding the functional genomics of the nervous system. We describe here an anatomically comprehensive digital atlas containing the expression patterns of approximately 20,000 genes in the adult mouse brain. Data were generated using automated high-throughput procedures for in situ hybridization and data acquisition, and are publicly accessible online. Newly developed image-based informatics tools allow global genome-scale structural analysis and cross-correlation, as well as identification of regionally enriched genes. Unbiased fine-resolution analysis has identified highly specific cellular markers as well as extensive evidence of cellular heterogeneity not evident in classical neuroanatomical atlases. This highly standardized atlas provides an open, primary data resource for a wide variety of further studies concerning brain organization and function.},
	author = {Lein, Ed S and Hawrylycz, Michael J and Ao, Nancy and Ayres, Mikael and Bensinger, Amy and Bernard, Amy and Boe, Andrew F and Boguski, Mark S and Brockway, Kevin S and Byrnes, Emi J and Chen, Lin and Chen, Li and Chen, Tsuey-Ming and Chin, Mei Chi and Chong, Jimmy and Crook, Brian E and Czaplinska, Aneta and Dang, Chinh N and Datta, Suvro and Dee, Nick R and Desaki, Aimee L and Desta, Tsega and Diep, Ellen and Dolbeare, Tim A and Donelan, Matthew J and Dong, Hong-Wei and Dougherty, Jennifer G and Duncan, Ben J and Ebbert, Amanda J and Eichele, Gregor and Estin, Lili K and Faber, Casey and Facer, Benjamin A and Fields, Rick and Fischer, Shanna R and Fliss, Tim P and Frensley, Cliff and Gates, Sabrina N and Glattfelder, Katie J and Halverson, Kevin R and Hart, Matthew R and Hohmann, John G and Howell, Maureen P and Jeung, Darren P and Johnson, Rebecca A and Karr, Patrick T and Kawal, Reena and Kidney, Jolene M and Knapik, Rachel H and Kuan, Chihchau L and Lake, James H and Laramee, Annabel R and Larsen, Kirk D and Lau, Christopher and Lemon, Tracy A and Liang, Agnes J and Liu, Ying and Luong, Lon T and Michaels, Jesse and Morgan, Judith J and Morgan, Rebecca J and Mortrud, Marty T and Mosqueda, Nerick F and Ng, Lydia L and Ng, Randy and Orta, Geralyn J and Overly, Caroline C and Pak, Tu H and Parry, Sheana E and Pathak, Sayan D and Pearson, Owen C and Puchalski, Ralph B and Riley, Zackery L and Rockett, Hannah R and Rowland, Stephen A and Royall, Joshua J and Ruiz, Marcos J and Sarno, Nadia R and Schaffnit, Katherine and Shapovalova, Nadiya V and Sivisay, Taz and Slaughterbeck, Clifford R and Smith, Simon C and Smith, Kimberly A and Smith, Bryan I and Sodt, Andy J and Stewart, Nick N and Stumpf, Kenda-Ruth and Sunkin, Susan M and Sutram, Madhavi and Tam, Angelene and Teemer, Carey D and Thaller, Christina and Thompson, Carol L and Varnam, Lee R and Visel, Axel and Whitlock, Ray M and Wohnoutka, Paul E and Wolkey, Crissa K and Wong, Victoria Y and Wood, Matthew and Yaylaoglu, Murat B and Young, Rob C and Youngstrom, Brian L and Yuan, Xu Feng and Zhang, Bin and Zwingman, Theresa A and Jones, Allan R},
	date-added = {2020-07-26 18:38:04 -0700},
	date-modified = {2020-07-26 18:38:04 -0700},
	doi = {10.1038/nature05453},
	journal = {Nature},
	journal-full = {Nature},
	mesh = {Animals; Brain; Computational Biology; Gene Expression Profiling; Gene Expression Regulation; Genome; Genomics; Hippocampus; Male; Mice; Mice, Inbred C57BL; Organ Specificity; RNA, Messenger},
	month = {Jan},
	number = {7124},
	pages = {168-76},
	pmid = {17151600},
	pst = {ppublish},
	title = {Genome-wide atlas of gene expression in the adult mouse brain},
	volume = {445},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature05453}}

@article{Johnson:2010aa,
	abstract = {We describe an atlas of the C57BL/6 mouse brain based on MRI and conventional Nissl histology. Magnetic resonance microscopy was performed on a total of 14 specimens that were actively stained to enhance tissue contrast. Images were acquired with three different MR protocols yielding contrast dependent on spin lattice relaxation (T1), spin spin relaxation (T2), and magnetic susceptibility (T2*). Spatial resolution was 21.5 mum (isotropic). Conventional histology (Nissl) was performed on a limited set of these same specimens and the Nissl images were registered (3D-to-3D) to the MR data. Probabilistic atlases for 37 structures are provided, along with average atlases. The availability of three different MR protocols, the Nissl data, and the labels provides a rich set of options for registration of other atlases to the same coordinate system, thus facilitating data-sharing. All the data is available for download via the web.},
	author = {Johnson, G Allan and Badea, Alexandra and Brandenburg, Jeffrey and Cofer, Gary and Fubara, Boma and Liu, Song and Nissanov, Jonathan},
	date-added = {2020-07-26 18:30:20 -0700},
	date-modified = {2020-07-26 18:30:20 -0700},
	doi = {10.1016/j.neuroimage.2010.06.067},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Animals; Atlases as Topic; Brain; Databases, Factual; Histology; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Mice; Mice, Inbred C57BL; Reference Standards; Staining and Labeling; Thalamus},
	month = {Nov},
	number = {2},
	pages = {365-72},
	pmc = {PMC2930145},
	pmid = {20600960},
	pst = {ppublish},
	title = {Waxholm space: an image-based reference for coordinating mouse brain research},
	volume = {53},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2010.06.067}}

@article{Kennedy:2019aa,
	abstract = {There has been a recent major upsurge in the concerns about reproducibility in many areas of science. Within the neuroimaging domain, one approach is to promote reproducibility is to target the re-executability of the publication. The information supporting such re-executability can enable the detailed examination of how an initial finding generalizes across changes in the processing approach, and sampled population, in a controlled scientific fashion. ReproNim: A Center for Reproducible Neuroimaging Computation is a recently funded initiative that seeks to facilitate the "last mile" implementations of core re-executability tools in order to reduce the accessibility barrier and increase adoption of standards and best practices at the neuroimaging research laboratory level. In this report, we summarize the overall approach and tools we have developed in this domain.},
	author = {Kennedy, David N and Abraham, Sanu A and Bates, Julianna F and Crowley, Albert and Ghosh, Satrajit and Gillespie, Tom and Goncalves, Mathias and Grethe, Jeffrey S and Halchenko, Yaroslav O and Hanke, Michael and Haselgrove, Christian and Hodge, Steven M and Jarecka, Dorota and Kaczmarzyk, Jakub and Keator, David B and Meyer, Kyle and Martone, Maryann E and Padhy, Smruti and Poline, Jean-Baptiste and Preuss, Nina and Sincomb, Troy and Travers, Matt},
	date-added = {2020-07-26 18:16:40 -0700},
	date-modified = {2020-07-26 18:16:40 -0700},
	doi = {10.3389/fninf.2019.00001},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	keywords = {data model; neuroimaging; publication; re-executability; reproducibility},
	pages = {1},
	pmc = {PMC6374302},
	pmid = {30792636},
	pst = {epublish},
	title = {Everything Matters: The ReproNim Perspective on Reproducible Neuroimaging},
	volume = {13},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.3389/fninf.2019.00001}}

@article{Merkel:2014aa,
	address = {Houston, TX},
	articleno = {2},
	author = {Merkel, Dirk},
	date-added = {2020-07-26 18:14:57 -0700},
	date-modified = {2020-07-26 18:14:57 -0700},
	issn = {1075-3583},
	issue_date = {March 2014},
	journal = {Linux J.},
	month = mar,
	number = {239},
	numpages = {1},
	publisher = {Belltown Media},
	title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
	volume = {2014},
	year = {2014}}

@article{Glatard:2015aa,
	abstract = {Neuroimaging pipelines are known to generate different results depending on the computing platform where they are compiled and executed. We quantify these differences for brain tissue classification, fMRI analysis, and cortical thickness (CT) extraction, using three of the main neuroimaging packages (FSL, Freesurfer and CIVET) and different versions of GNU/Linux. We also identify some causes of these differences using library and system call interception. We find that these packages use mathematical functions based on single-precision floating-point arithmetic whose implementations in operating systems continue to evolve. While these differences have little or no impact on simple analysis pipelines such as brain extraction and cortical tissue classification, their accumulation creates important differences in longer pipelines such as subcortical tissue classification, fMRI analysis, and cortical thickness extraction. With FSL, most Dice coefficients between subcortical classifications obtained on different operating systems remain above 0.9, but values as low as 0.59 are observed. Independent component analyses (ICA) of fMRI data differ between operating systems in one third of the tested subjects, due to differences in motion correction. With Freesurfer and CIVET, in some brain regions we find an effect of build or operating system on cortical thickness. A first step to correct these reproducibility issues would be to use more precise representations of floating-point numbers in the critical sections of the pipelines. The numerical stability of pipelines should also be reviewed. },
	author = {Glatard, Tristan and Lewis, Lindsay B and Ferreira da Silva, Rafael and Adalat, Reza and Beck, Natacha and Lepage, Claude and Rioux, Pierre and Rousseau, Marc-Etienne and Sherif, Tarek and Deelman, Ewa and Khalili-Mahani, Najmeh and Evans, Alan C},
	date-added = {2020-07-26 18:11:41 -0700},
	date-modified = {2020-07-26 18:11:41 -0700},
	doi = {10.3389/fninf.2015.00012},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	keywords = {CIVET; FSL; Freesurfer; operating systems; reproducibility},
	pages = {12},
	pmc = {PMC4408913},
	pmid = {25964757},
	pst = {epublish},
	title = {Reproducibility of neuroimaging analyses across operating systems},
	volume = {9},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.3389/fninf.2015.00012}}

@article{Dickie:2017aa,
	abstract = {We examine the similarity of outputs from Freesurfer version 5.1, Freesurfer version 5.3 and ANTS for the ABIDEI dataset.},
	author = {Erin Dickie and Steven M Hodge and R. Cameron Craddock and Jean-Baptiste Poline and David N. Kennedy},
	date-added = {2020-07-26 18:10:45 -0700},
	date-modified = {2020-07-26 18:10:54 -0700},
	doi = {10.3897/rio.3.e13726},
	eprint = {https://doi.org/10.3897/rio.3.e13726},
	journal = {Research Ideas and Outcomes},
	pages = {e13726},
	publisher = {Pensoft Publishers},
	title = {Tools Matter: Comparison of Two Surface Analysis Tools Applied to the ABIDE Dataset},
	url = {https://doi.org/10.3897/rio.3.e13726},
	volume = {3},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.3897/rio.3.e13726}}

@article{Botvinik-Nezer:2020aa,
	abstract = {Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses1. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset2-5. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed.},
	author = {Botvinik-Nezer, Rotem and Holzmeister, Felix and Camerer, Colin F and Dreber, Anna and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Iwanir, Roni and Mumford, Jeanette A and Adcock, R Alison and Avesani, Paolo and Baczkowski, Blazej M and Bajracharya, Aahana and Bakst, Leah and Ball, Sheryl and Barilari, Marco and Bault, Nad{\`e}ge and Beaton, Derek and Beitner, Julia and Benoit, Roland G and Berkers, Ruud M W J and Bhanji, Jamil P and Biswal, Bharat B and Bobadilla-Suarez, Sebastian and Bortolini, Tiago and Bottenhorn, Katherine L and Bowring, Alexander and Braem, Senne and Brooks, Hayley R and Brudner, Emily G and Calderon, Cristian B and Camilleri, Julia A and Castrellon, Jaime J and Cecchetti, Luca and Cieslik, Edna C and Cole, Zachary J and Collignon, Olivier and Cox, Robert W and Cunningham, William A and Czoschke, Stefan and Dadi, Kamalaker and Davis, Charles P and Luca, Alberto De and Delgado, Mauricio R and Demetriou, Lysia and Dennison, Jeffrey B and Di, Xin and Dickie, Erin W and Dobryakova, Ekaterina and Donnat, Claire L and Dukart, Juergen and Duncan, Niall W and Durnez, Joke and Eed, Amr and Eickhoff, Simon B and Erhart, Andrew and Fontanesi, Laura and Fricke, G Matthew and Fu, Shiguang and Galv{\'a}n, Adriana and Gau, Remi and Genon, Sarah and Glatard, Tristan and Glerean, Enrico and Goeman, Jelle J and Golowin, Sergej A E and Gonz{\'a}lez-Garc{\'\i}a, Carlos and Gorgolewski, Krzysztof J and Grady, Cheryl L and Green, Mikella A and Guassi Moreira, Jo{\~a}o F and Guest, Olivia and Hakimi, Shabnam and Hamilton, J Paul and Hancock, Roeland and Handjaras, Giacomo and Harry, Bronson B and Hawco, Colin and Herholz, Peer and Herman, Gabrielle and Heunis, Stephan and Hoffstaedter, Felix and Hogeveen, Jeremy and Holmes, Susan and Hu, Chuan-Peng and Huettel, Scott A and Hughes, Matthew E and Iacovella, Vittorio and Iordan, Alexandru D and Isager, Peder M and Isik, Ayse I and Jahn, Andrew and Johnson, Matthew R and Johnstone, Tom and Joseph, Michael J E and Juliano, Anthony C and Kable, Joseph W and Kassinopoulos, Michalis and Koba, Cemal and Kong, Xiang-Zhen and Koscik, Timothy R and Kucukboyaci, Nuri Erkut and Kuhl, Brice A and Kupek, Sebastian and Laird, Angela R and Lamm, Claus and Langner, Robert and Lauharatanahirun, Nina and Lee, Hongmi and Lee, Sangil and Leemans, Alexander and Leo, Andrea and Lesage, Elise and Li, Flora and Li, Monica Y C and Lim, Phui Cheng and Lintz, Evan N and Liphardt, Schuyler W and Losecaat Vermeer, Annabel B and Love, Bradley C and Mack, Michael L and Malpica, Norberto and Marins, Theo and Maumet, Camille and McDonald, Kelsey and McGuire, Joseph T and Melero, Helena and M{\'e}ndez Leal, Adriana S and Meyer, Benjamin and Meyer, Kristin N and Mihai, Glad and Mitsis, Georgios D and Moll, Jorge and Nielson, Dylan M and Nilsonne, Gustav and Notter, Michael P and Olivetti, Emanuele and Onicas, Adrian I and Papale, Paolo and Patil, Kaustubh R and Peelle, Jonathan E and P{\'e}rez, Alexandre and Pischedda, Doris and Poline, Jean-Baptiste and Prystauka, Yanina and Ray, Shruti and Reuter-Lorenz, Patricia A and Reynolds, Richard C and Ricciardi, Emiliano and Rieck, Jenny R and Rodriguez-Thompson, Anais M and Romyn, Anthony and Salo, Taylor and Samanez-Larkin, Gregory R and Sanz-Morales, Emilio and Schlichting, Margaret L and Schultz, Douglas H and Shen, Qiang and Sheridan, Margaret A and Silvers, Jennifer A and Skagerlund, Kenny and Smith, Alec and Smith, David V and Sokol-Hessner, Peter and Steinkamp, Simon R and Tashjian, Sarah M and Thirion, Bertrand and Thorp, John N and Tingh{\"o}g, Gustav and Tisdall, Loreen and Tompson, Steven H and Toro-Serey, Claudio and Torre Tresols, Juan Jesus and Tozzi, Leonardo and Truong, Vuong and Turella, Luca and van 't Veer, Anna E and Verguts, Tom and Vettel, Jean M and Vijayarajah, Sagana and Vo, Khoi and Wall, Matthew B and Weeda, Wouter D and Weis, Susanne and White, David J and Wisniewski, David and Xifra-Porxas, Alba and Yearling, Emily A and Yoon, Sangsuk and Yuan, Rui and Yuen, Kenneth S L and Zhang, Lei and Zhang, Xu and Zosky, Joshua E and Nichols, Thomas E and Poldrack, Russell A and Schonberg, Tom},
	date-added = {2020-07-26 18:06:47 -0700},
	date-modified = {2020-07-26 18:06:47 -0700},
	doi = {10.1038/s41586-020-2314-9},
	journal = {Nature},
	journal-full = {Nature},
	mesh = {Brain; Data Analysis; Data Science; Datasets as Topic; Female; Functional Neuroimaging; Humans; Logistic Models; Magnetic Resonance Imaging; Male; Meta-Analysis as Topic; Models, Neurological; Reproducibility of Results; Research Personnel; Software},
	month = {06},
	number = {7810},
	pages = {84-88},
	pmid = {32483374},
	pst = {ppublish},
	title = {Variability in the analysis of a single neuroimaging dataset by many teams},
	volume = {582},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-020-2314-9}}

@article{Wilkinson:2016aa,
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community. },
	author = {Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau and Brookes, Anthony J and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J G and Groth, Paul and Goble, Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen, Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J and Martone, Maryann E and Mons, Albert and Packer, Abel L and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	date-added = {2020-07-26 18:05:30 -0700},
	date-modified = {2020-07-26 18:05:30 -0700},
	doi = {10.1038/sdata.2016.18},
	journal = {Sci Data},
	journal-full = {Scientific data},
	mesh = {Data Collection; Data Curation; Database Management Systems; Guidelines as Topic; Reproducibility of Results; Research Design},
	month = {Mar},
	pages = {160018},
	pmc = {PMC4792175},
	pmid = {26978244},
	pst = {epublish},
	title = {The FAIR Guiding Principles for scientific data management and stewardship},
	volume = {3},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1038/sdata.2016.18}}

@article{Beaulieu-Jones:2017aa,
	abstract = {Replication, validation and extension of experiments are crucial for scientific progress. Computational experiments are scriptable and should be easy to reproduce. However, computational analyses are designed and run in a specific computing environment, which may be difficult or impossible to match using written instructions. We report the development of continuous analysis, a workflow that enables reproducible computational analyses. Continuous analysis combines Docker, a container technology akin to virtual machines, with continuous integration, a software development technique, to automatically rerun a computational analysis whenever updates or improvements are made to source code or data. This enables researchers to reproduce results without contacting the study authors. Continuous analysis allows reviewers, editors or readers to verify reproducibility without manually downloading and rerunning code and can provide an audit trail for analyses of data that cannot be shared.},
	author = {Beaulieu-Jones, Brett K and Greene, Casey S},
	date-added = {2020-07-26 18:04:17 -0700},
	date-modified = {2020-07-26 18:04:17 -0700},
	doi = {10.1038/nbt.3780},
	journal = {Nat Biotechnol},
	journal-full = {Nature biotechnology},
	mesh = {Computational Biology; Gene Expression Profiling; Machine Learning; Reproducibility of Results; Sensitivity and Specificity; Software; User-Computer Interface; Workflow},
	month = {04},
	number = {4},
	pages = {342-346},
	pmc = {PMC6103790},
	pmid = {28288103},
	pst = {ppublish},
	title = {Reproducibility of computational workflows is automated using continuous analysis},
	volume = {35},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/nbt.3780}}

@article{Baker:2016aa,
	author = {Baker, Monya},
	date-added = {2020-07-26 18:03:13 -0700},
	date-modified = {2020-07-26 18:03:13 -0700},
	doi = {10.1038/533452a},
	journal = {Nature},
	journal-full = {Nature},
	mesh = {Attitude; Data Interpretation, Statistical; Mentors; Periodicals as Topic; Publishing; Reproducibility of Results; Research; Research Design; Research Personnel; Research Support as Topic; Surveys and Questionnaires},
	month = {05},
	number = {7604},
	pages = {452-4},
	pmid = {27225100},
	pst = {ppublish},
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1038/533452a}}

@inproceedings{Ravikumar:2020aa,
	author = {S. {Ravikumar} and L. E. M. {Wisse} and R. {Ittyerah} and S. {Lim} and M. {Lavery} and L. {Xie} and J. L. {Robinson} and T. {Schuck} and M. {Grossman} and E. B. {Lee} and M. D. {Tisdall} and K. {Prabhakaran} and J. A. {Detre} and S. R. {Das} and G. {Mizsei} and E. {Artacho-P{\'e}rula} and M. M. I. {de Onzono Martin} and M. {del Mar Arroyo Jim{\'e}nez} and M. {Mu{\~n}oz} and F. J. M. {Romero} and M. {del Pilar Marcos Rabal} and D. J. {Irwin} and J. Q. {Trojanowski} and D. A. {Wolk} and R. {Insausti} and P. A. {Yushkevich}},
	booktitle = {2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},
	date-added = {2020-07-26 18:01:14 -0700},
	date-modified = {2020-07-26 18:01:14 -0700},
	pages = {113-117},
	title = {Building an Ex Vivo Atlas of the Earliest Brain Regions Affected by Alzheimer's Disease Pathology},
	year = {2020}}

@inproceedings{Yushkevich:2020aa,
	author = {P. A. {Yushkevich} and M. M. I. {de Onzo{\~n}o Martin} and R. {Ittyerah} and S. {Lim} and M. {Lavery} and J. {Wang} and L. Y. {Hung} and N. {Vergnet} and S. {Ravikumar} and L. {Xie} and M. {Dong} and R. {DeFlores} and S. {Cui} and L. {McCollum} and D. T. {Ohm} and J. L. {Robinson} and T. {Schuck} and M. {Grossman} and M. D. {Tisdall} and K. {Prabhakaran} and G. {Mizsei} and S. R. {Das} and E. {Artacho-P{\'e}rula} and M. {del Mar Arroyo Jim{\'e}nez} and M. M. {L{\'o}pez} and M. P. M. {Rabal} and F. J. M. {Romero} and E. B. {Lee} and J. Q. {Trojanowski} and L. E. M. {Wisse} and D. A. {Wolk} and D. J. {Irwin} and R. {Insausti}},
	booktitle = {2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},
	date-added = {2020-07-26 18:00:29 -0700},
	date-modified = {2020-07-26 18:00:29 -0700},
	pages = {1312-1316},
	title = {3D Mapping of TAU Neurofibrillary Tangle Pathology in the Human Medial Temporal Lobe},
	year = {2020}}

@article{Adler:2018aa,
	abstract = {Although the hippocampus is one of the most studied structures in the human brain, limited quantitative data exist on its 3D organization, anatomical variability, and effects of disease on its subregions. Histological studies provide restricted reference information due to their 2D nature. In this paper, high-resolution (∼200 × 200 × 200 μm3) ex vivo MRI scans of 31 human hippocampal specimens are combined using a groupwise diffeomorphic registration approach into a 3D probabilistic atlas that captures average anatomy and anatomic variability of hippocampal subfields. Serial histological imaging in 9 of the 31 specimens was used to label hippocampal subfields in the atlas based on cytoarchitecture. Specimens were obtained from autopsies in patients with a clinical diagnosis of Alzheimer's disease (AD; 9 subjects, 13 hemispheres), of other dementia (nine subjects, nine hemispheres), and in subjects without dementia (seven subjects, nine hemispheres), and morphometric analysis was performed in atlas space to measure effects of age and AD on hippocampal subfields. Disproportional involvement of the cornu ammonis (CA) 1 subfield and stratum radiatum lacunosum moleculare was found in AD, with lesser involvement of the dentate gyrus and CA2/3 subfields. An association with age was found for the dentate gyrus and, to a lesser extent, for CA1. Three-dimensional patterns of variability and disease and aging effects discovered via the ex vivo hippocampus atlas provide information highly relevant to the active field of in vivo hippocampal subfield imaging.},
	author = {Adler, Daniel H and Wisse, Laura E M and Ittyerah, Ranjit and Pluta, John B and Ding, Song-Lin and Xie, Long and Wang, Jiancong and Kadivar, Salmon and Robinson, John L and Schuck, Theresa and Trojanowski, John Q and Grossman, Murray and Detre, John A and Elliott, Mark A and Toledo, Jon B and Liu, Weixia and Pickup, Stephen and Miller, Michael I and Das, Sandhitsu R and Wolk, David A and Yushkevich, Paul A},
	date-added = {2020-07-26 17:55:45 -0700},
	date-modified = {2020-07-26 17:55:45 -0700},
	doi = {10.1073/pnas.1801093115},
	journal = {Proc Natl Acad Sci U S A},
	journal-full = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {Alzheimer's disease; computational anatomy; ex vivo MRI; hippocampal subfields; histology},
	mesh = {Aged; Aging; Alzheimer Disease; Atlases as Topic; Atrophy; Dentate Gyrus; Hippocampus; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Neuroimaging; Organ Size},
	month = {04},
	number = {16},
	pages = {4252-4257},
	pmc = {PMC5910869},
	pmid = {29592955},
	pst = {ppublish},
	title = {Characterizing the human hippocampus in aging and Alzheimer's disease using a computational atlas derived from ex vivo MRI and histology},
	volume = {115},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1073/pnas.1801093115}}

@article{Adler:2014aa,
	abstract = {Recently, there has been a growing effort to analyze the morphometry of hippocampal subfields using both in vivo and postmortem magnetic resonance imaging (MRI). However, given that boundaries between subregions of the hippocampal formation (HF) are conventionally defined on the basis of microscopic features that often lack discernible signature in MRI, subfield delineation in MRI literature has largely relied on heuristic geometric rules, the validity of which with respect to the underlying anatomy is largely unknown. The development and evaluation of such rules are challenged by the limited availability of data linking MRI appearance to microscopic hippocampal anatomy, particularly in three dimensions (3D). The present paper, for the first time, demonstrates the feasibility of labeling hippocampal subfields in a high resolution volumetric MRI dataset based directly on microscopic features extracted from histology. It uses a combination of computational techniques and manual post-processing to map subfield boundaries from a stack of histology images (obtained with 200μm spacing and 5μm slice thickness; stained using the Kluver-Barrera method) onto a postmortem 9.4Tesla MRI scan of the intact, whole hippocampal formation acquired with 160μm isotropic resolution. The histology reconstruction procedure consists of sequential application of a graph-theoretic slice stacking algorithm that mitigates the effects of distorted slices, followed by iterative affine and diffeomorphic co-registration to postmortem MRI scans of approximately 1cm-thick tissue sub-blocks acquired with 200μm isotropic resolution. These 1cm blocks are subsequently co-registered to the MRI of the whole HF. Reconstruction accuracy is evaluated as the average displacement error between boundaries manually delineated in both the histology and MRI following the sequential stages of reconstruction. The methods presented and evaluated in this single-subject study can potentially be applied to multiple hippocampal tissue samples in order to construct a histologically informed MRI atlas of the hippocampal formation. },
	author = {Adler, Daniel H and Pluta, John and Kadivar, Salmon and Craige, Caryne and Gee, James C and Avants, Brian B and Yushkevich, Paul A},
	date-added = {2020-07-26 17:55:08 -0700},
	date-modified = {2020-07-26 17:55:08 -0700},
	doi = {10.1016/j.neuroimage.2013.08.067},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	keywords = {Hippocampus; Histology; MRI; Postmortem; Reconstruction; Subfields},
	mesh = {Aged, 80 and over; Algorithms; Autopsy; Female; Hippocampus; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Postmortem Changes; Reproducibility of Results; Sensitivity and Specificity},
	month = {Jan},
	pages = {505-23},
	pmc = {PMC3864597},
	pmid = {24036353},
	pst = {ppublish},
	title = {Histology-derived volumetric annotation of the human hippocampal subfields in postmortem MRI},
	volume = {84},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2013.08.067}}

@inproceedings{Durand:2017aa,
	author = {T. {Durand} and T. {Mordan} and N. {Thome} and M. {Cord}},
	booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2020-07-26 17:53:41 -0700},
	date-modified = {2020-07-26 17:54:06 -0700},
	pages = {5957-5966},
	title = {WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation},
	year = {2017}}

@article{Li:2018aa,
	abstract = {White matter hyperintensities (WMH) are commonly found in the brains of healthy elderly individuals and have been associated with various neurological and geriatric disorders. In this paper, we present a study using deep fully convolutional network and ensemble models to automatically detect such WMH using fluid attenuation inversion recovery (FLAIR) and T1 magnetic resonance (MR) scans. The algorithm was evaluated and ranked 1st in the WMH Segmentation Challenge at MICCAI 2017. In the evaluation stage, the implementation of the algorithm was submitted to the challenge organizers, who then independently tested it on a hidden set of 110 cases from 5 scanners. Averaged dice score, precision and robust Hausdorff distance obtained on held-out test datasets were 80%, 84% and 6.30 mm respectively. These were the highest achieved in the challenge, suggesting the proposed method is the state-of-the-art. Detailed descriptions and quantitative analysis on key components of the system were provided. Furthermore, a study of cross-scanner evaluation is presented to discuss how the combination of modalities affect the generalization capability of the system. The adaptability of the system to different scanners and protocols is also investigated. A quantitative study is further presented to show the effect of ensemble size and the effectiveness of the ensemble model. Additionally, software and models of our method are made publicly available. The effectiveness and generalization capability of the proposed system show its potential for real-world clinical practice.},
	author = {Li, Hongwei and Jiang, Gongfa and Zhang, Jianguo and Wang, Ruixuan and Wang, Zhaolei and Zheng, Wei-Shi and Menze, Bjoern},
	date-added = {2020-06-19 21:33:04 -0700},
	date-modified = {2020-06-19 21:33:04 -0700},
	doi = {10.1016/j.neuroimage.2018.07.005},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	keywords = {Brain lesion segmentation; Deep learning; Ensemble models; MICCAI WMH segmentation challenge; White matter hyperintensities},
	mesh = {Algorithms; Brain; Datasets as Topic; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Neuroimaging; White Matter},
	month = {12},
	pages = {650-665},
	pmid = {30125711},
	pst = {ppublish},
	title = {Fully convolutional network ensembles for white matter hyperintensities segmentation in MR images},
	volume = {183},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2018.07.005}}

@article{Kuijf:2019aa,
	abstract = {Quantification of cerebral white matter hyperintensities (WMH) of presumed vascular origin is of key importance in many neurological research studies. Currently, measurements are often still obtained from manual segmentations on brain MR images, which is a laborious procedure. The automatic WMH segmentation methods exist, but a standardized comparison of the performance of such methods is lacking. We organized a scientific challenge, in which developers could evaluate their methods on a standardized multi-center/-scanner image dataset, giving an objective comparison: the WMH Segmentation Challenge. Sixty T1 + FLAIR images from three MR scanners were released with the manual WMH segmentations for training. A test set of 110 images from five MR scanners was used for evaluation. The segmentation methods had to be containerized and submitted to the challenge organizers. Five evaluation metrics were used to rank the methods: 1) Dice similarity coefficient; 2) modified Hausdorff distance (95th percentile); 3) absolute log-transformed volume difference; 4) sensitivity for detecting individual lesions; and 5) F1-score for individual lesions. In addition, the methods were ranked on their inter-scanner robustness; 20 participants submitted their methods for evaluation. This paper provides a detailed analysis of the results. In brief, there is a cluster of four methods that rank significantly better than the other methods, with one clear winner. The inter-scanner robustness ranking shows that not all the methods generalize to unseen scanners. The challenge remains open for future submissions and provides a public platform for method evaluation.},
	author = {Kuijf, Hugo J and Biesbroek, J Matthijs and De Bresser, Jeroen and Heinen, Rutger and Andermatt, Simon and Bento, Mariana and Berseth, Matt and Belyaev, Mikhail and Cardoso, M Jorge and Casamitjana, Adria and Collins, D Louis and Dadar, Mahsa and Georgiou, Achilleas and Ghafoorian, Mohsen and Jin, Dakai and Khademi, April and Knight, Jesse and Li, Hongwei and Llado, Xavier and Luna, Miguel and Mahmood, Qaiser and McKinley, Richard and Mehrtash, Alireza and Ourselin, Sebastien and Park, Bo-Yong and Park, Hyunjin and Park, Sang Hyun and Pezold, Simon and Puybareau, Elodie and Rittner, Leticia and Sudre, Carole H and Valverde, Sergi and Vilaplana, Veronica and Wiest, Roland and Xu, Yongchao and Xu, Ziyue and Zeng, Guodong and Zhang, Jianguo and Zheng, Guoyan and Chen, Christopher and van der Flier, Wiesje and Barkhof, Frederik and Viergever, Max A and Biessels, Geert Jan},
	date-added = {2020-06-19 21:32:43 -0700},
	date-modified = {2020-06-19 21:32:43 -0700},
	doi = {10.1109/TMI.2019.2905770},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	month = {11},
	number = {11},
	pages = {2556-2568},
	pmid = {30908194},
	pst = {ppublish},
	title = {Standardized Assessment of Automatic Segmentation of White Matter Hyperintensities and Results of the WMH Segmentation Challenge},
	volume = {38},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/TMI.2019.2905770}}

@misc{Detone:2017aa,
	archiveprefix = {arXiv},
	author = {Daniel DeTone and Tomasz Malisiewicz and Andrew Rabinovich},
	date-added = {2020-06-19 20:45:24 -0700},
	date-modified = {2020-06-19 20:45:24 -0700},
	eprint = {1712.07629},
	primaryclass = {cs.CV},
	title = {SuperPoint: Self-Supervised Interest Point Detection and Description},
	year = {2017}}

@misc{Sarlin:2019aa,
	archiveprefix = {arXiv},
	author = {Paul-Edouard Sarlin and Daniel DeTone and Tomasz Malisiewicz and Andrew Rabinovich},
	date-added = {2020-06-19 20:43:52 -0700},
	date-modified = {2020-06-19 20:43:52 -0700},
	eprint = {1911.11763},
	primaryclass = {cs.CV},
	title = {SuperGlue: Learning Feature Matching with Graph Neural Networks},
	year = {2019}}

@article{Woods:1993aa,
	abstract = {OBJECTIVE: We have previously reported an automated method for within-modality (e.g., PET-to-PET) image alignment. We now describe modifications to this method that allow for cross-modality registration of MRI and PET brain images obtained from a single subject.
METHODS: This method does not require fiducial markers and the user is not required to identify common structures on the two image sets. To align the images, the algorithm seeks to minimize the standard deviation of the PET pixel values that correspond to each MRI pixel value. The MR images must be edited to exclude nonbrain regions prior to using the algorithm.
RESULTS AND CONCLUSION: The method has been validated quantitatively using data from patients with stereotaxic fiducial markers rigidly fixed in the skull. Maximal three-dimensional errors of < 3 mm and mean three-dimensional errors of < 2 mm were measured. Computation time on a SPARCstation IPX varies from 3 to 9 min to align MR image sets with [18F]fluorodeoxyglucose PET images. The MR alignment with noisy H2(15)O PET images typically requires 20-30 min.},
	author = {Woods, R P and Mazziotta, J C and Cherry, S R},
	date = {1993 Jul-Aug},
	date-added = {2020-06-18 13:42:01 -0700},
	date-modified = {2020-06-18 13:42:01 -0700},
	doi = {10.1097/00004728-199307000-00004},
	journal = {J Comput Assist Tomogr},
	journal-full = {Journal of computer assisted tomography},
	mesh = {Adult; Algorithms; Brain; Deoxyglucose; Epilepsy; Female; Fluorine Radioisotopes; Fluorodeoxyglucose F18; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Time Factors; Tomography, Emission-Computed},
	number = {4},
	pages = {536-46},
	pmid = {8331222},
	pst = {ppublish},
	title = {MRI-PET registration with automated algorithm},
	volume = {17},
	year = {1993},
	Bdsk-Url-1 = {https://doi.org/10.1097/00004728-199307000-00004}}

@article{Modat:2010aa,
	abstract = {A large number of algorithms have been developed to perform non-rigid registration and it is a tool commonly used in medical image analysis. The free-form deformation algorithm is a well-established technique, but is extremely time consuming. In this paper we present a parallel-friendly formulation of the algorithm suitable for graphics processing unit execution. Using our approach we perform registration of T1-weighted MR images in less than 1 min and show the same level of accuracy as a classical serial implementation when performing segmentation propagation. This technology could be of significant utility in time-critical applications such as image-guided interventions, or in the processing of large data sets.},
	author = {Modat, Marc and Ridgway, Gerard R and Taylor, Zeike A and Lehmann, Manja and Barnes, Josephine and Hawkes, David J and Fox, Nick C and Ourselin, S{\'e}bastien},
	date-added = {2020-06-18 13:41:05 -0700},
	date-modified = {2020-06-18 13:41:05 -0700},
	doi = {10.1016/j.cmpb.2009.09.002},
	journal = {Comput Methods Programs Biomed},
	journal-full = {Computer methods and programs in biomedicine},
	mesh = {Algorithms; Computer Graphics; Diagnostic Imaging; Image Processing, Computer-Assisted; Software},
	month = {Jun},
	number = {3},
	pages = {278-84},
	pmid = {19818524},
	pst = {ppublish},
	title = {Fast free-form deformation using graphics processing units},
	volume = {98},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.cmpb.2009.09.002}}

@article{Klein:2010aa,
	abstract = {Medical image registration is an important task in medical image processing. It refers to the process of aligning data sets, possibly from different modalities (e.g., magnetic resonance and computed tomography), different time points (e.g., follow-up scans), and/or different subjects (in case of population studies). A large number of methods for image registration are described in the literature. Unfortunately, there is not one method that works for all applications. We have therefore developed elastix, a publicly available computer program for intensity-based medical image registration. The software consists of a collection of algorithms that are commonly used to solve medical image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application. The command-line interface enables automated processing of large numbers of data sets, by means of scripting. The usage of elastix for comparing different registration methods is illustrated with three example experiments, in which individual components of the registration method are varied.},
	author = {Klein, Stefan and Staring, Marius and Murphy, Keelin and Viergever, Max A and Pluim, Josien P W},
	date-added = {2020-06-18 13:40:18 -0700},
	date-modified = {2020-06-18 13:40:18 -0700},
	doi = {10.1109/TMI.2009.2035616},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Diagnostic Imaging; Humans; Image Processing, Computer-Assisted; Models, Biological; Normal Distribution; Software},
	month = {Jan},
	number = {1},
	pages = {196-205},
	pmid = {19923044},
	pst = {ppublish},
	title = {elastix: a toolbox for intensity-based medical image registration},
	volume = {29},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/TMI.2009.2035616}}

@misc{nazib:2018aa,
	archiveprefix = {arXiv},
	author = {Abdullah Nazib and Clinton Fookes and Dimitri Perrin},
	date-added = {2020-06-18 13:38:40 -0700},
	date-modified = {2020-06-18 13:38:40 -0700},
	eprint = {1810.08315},
	primaryclass = {cs.CV},
	title = {A Comparative Analysis of Registration Tools: Traditional vs Deep Learning Approach on High Resolution Tissue Cleared Data},
	year = {2018}}

@article{Tustison:2019ab,
	abstract = {Recent methodological innovations in deep learning and associated advancements in computational hardware have significantly impacted the various core subfields of quantitative medical image analysis. The generalizability, computational efficiency and open-source availability of deep learning algorithms and related software, particularly those utilizing convolutional neural networks, have produced paradigm shifts within the field. This impact is evident from topical prevalence in the literature, conference and workshop themes and winning methodologies in relevant competitions. In this work, we review the various state-of-the-art approaches to learning and prediction and/or optimizing image transformations using convolutional neural networks. Although of primary importance within the quantitative imaging domain, image registration algorithmic development, in the context of these deep learning strategies, has received comparatively less attention than its counterparts (e.g., image segmentation). Nevertheless, significant progress has been made in this particular subfield which has been presented in various research venues. We contextualize these contributions within the broader scope of deep learning advancements and, in so doing, attempt to facilitate the leveraging and further development of such techniques within the medical imaging research community.},
	author = {Tustison, Nicholas J and Avants, Brian B and Gee, James C},
	date-added = {2020-06-17 20:30:35 -0700},
	date-modified = {2020-06-17 20:30:35 -0700},
	doi = {10.1016/j.mri.2019.05.037},
	journal = {Magn Reson Imaging},
	journal-full = {Magnetic resonance imaging},
	keywords = {ConvNets; Deep learning; Diffeomorphisms; Image registration; Spatial normalization},
	mesh = {Algorithms; Brain; Brain Neoplasms; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer},
	month = {12},
	pages = {142-153},
	pmid = {31200026},
	pst = {ppublish},
	title = {Learning image-based spatial transformations via convolutional neural networks: A review},
	volume = {64},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.mri.2019.05.037}}

@article{Dalca:2019aa,
	abstract = {Classical deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we build a connection between classical and learning-based methods. We present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that uses insights from classical registration methods and makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task for both images and anatomical surfaces, and provide extensive empirical analyses of the algorithm. Our principled approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees. Our implementation is available online at http://voxelmorph.csail.mit.edu.},
	author = {Dalca, Adrian V and Balakrishnan, Guha and Guttag, John and Sabuncu, Mert R},
	date-added = {2020-06-17 20:20:24 -0700},
	date-modified = {2020-06-17 20:20:24 -0700},
	doi = {10.1016/j.media.2019.07.006},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	keywords = {Convolutional neural networks; Diffeomorphic registration; Invertible registration; Machine learning; Medical image registration; Probabilistic modeling; Variational inference},
	month = {10},
	pages = {226-236},
	pmid = {31351389},
	pst = {ppublish},
	title = {Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces},
	volume = {57},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.media.2019.07.006}}

@article{Balakrishnan:2019aa,
	abstract = {We present VoxelMorph, a fast learning-based framework for deformable, pairwise medical image registration. Traditional registration methods optimize an objective function for each pair of images, which can be time-consuming for large datasets or rich deformation models. In contrast to this approach, and building on recent learning-based methods, we formulate registration as a function that maps an input image pair to a deformation field that aligns these images. We parameterize the function via a convolutional neural network (CNN), and optimize the parameters of the neural network on a set of images. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function. In this work, we explore two different training strategies. In the first (unsupervised) setting, we train the model to maximize standard image matching objective functions that are based on the image intensities. In the second setting, we leverage auxiliary segmentations available in the training data. We demonstrate that the unsupervised model's accuracy is comparable to state-of-the-art methods, while operating orders of magnitude faster. We also show that VoxelMorph trained with auxiliary data improves registration accuracy at test time, and evaluate the effect of training set size on registration. Our method promises to speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is freely available at https://github.com/voxelmorph/voxelmorph.},
	author = {Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R and Guttag, John and Dalca, Adrian V},
	date-added = {2020-06-17 20:20:09 -0700},
	date-modified = {2020-06-17 20:20:09 -0700},
	doi = {10.1109/TMI.2019.2897538},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	month = {Feb},
	pmid = {30716034},
	pst = {aheadofprint},
	title = {VoxelMorph: A Learning Framework for Deformable Medical Image Registration},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/TMI.2019.2897538}}

@inproceedings{Simonyan:2015aa,
	author = {Karen Simonyan and Andrew Zisserman},
	booktitle = {International Conference on Learning Representations},
	date-added = {2020-06-02 20:27:37 -0700},
	date-modified = {2020-06-02 20:27:37 -0700},
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	year = {2015}}

@article{Russakovsky:2015aa,
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	da = {2015/12/01},
	date-added = {2020-06-02 20:24:08 -0700},
	date-modified = {2020-06-02 20:24:08 -0700},
	doi = {10.1007/s11263-015-0816-y},
	id = {Russakovsky2015},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {3},
	pages = {211--252},
	title = {ImageNet Large Scale Visual Recognition Challenge},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	volume = {115},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11263-015-0816-y}}

@article{Rister:2017aa,
	abstract = {We present a method for image registration based on 3D scale- and rotation-invariant keypoints. The method extends the scale invariant feature transform (SIFT) to arbitrary dimensions by making key modifications to orientation assignment and gradient histograms. Rotation invariance is proven mathematically. Additional modifications are made to extrema detection and keypoint matching based on the demands of image registration. Our experiments suggest that the choice of neighborhood in discrete extrema detection has a strong impact on image registration accuracy. In head MR images, the brain is registered to a labeled atlas with an average Dice coefficient of 92%, outperforming registration from mutual information as well as an existing 3D SIFT implementation. In abdominal CT images, the spine is registered with an average error of 4.82 mm. Furthermore, keypoints are matched with high precision in simulated head MR images exhibiting lesions from multiple sclerosis. These results were achieved using only affine transforms, and with no change in parameters across a wide variety of medical images. This paper is freely available as a cross-platform software library.},
	author = {Rister, Blaine and Horowitz, Mark A and Rubin, Daniel L},
	date-added = {2020-06-02 20:20:16 -0700},
	date-modified = {2020-06-02 20:20:16 -0700},
	doi = {10.1109/TIP.2017.2722689},
	journal = {IEEE Trans Image Process},
	journal-full = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	month = {Oct},
	number = {10},
	pages = {4900-4910},
	pmc = {PMC5581541},
	pmid = {28682256},
	pst = {ppublish},
	title = {Volumetric Image Registration From Invariant Keypoints},
	volume = {26},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIP.2017.2722689}}

@inproceedings{Lowe:1999aa,
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.},
	author = {D. G. {Lowe}},
	booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
	date-added = {2020-06-02 20:18:32 -0700},
	date-modified = {2020-06-02 20:18:44 -0700},
	doi = {10.1109/ICCV.1999.790410},
	keywords = {object recognition;feature extraction;computational geometry;image matching;least squares approximations;local scale-invariant features;local image features;3D projection;inferior temporal cortex;primate vision;staged filtering approach;local geometric deformations;blurred image gradients;multiple orientation planes;nearest neighbor indexing method;candidate object matches;low residual least squares solution;unknown model parameters;robust object recognition;cluttered partially occluded images;computation time;Object recognition;Electrical capacitance tomography;Image recognition;Lighting;Neurons;Computer science;Reactive power;Filters;Programmable logic arrays;Layout},
	month = {Sep.},
	pages = {1150-1157 vol.2},
	title = {Object recognition from local scale-invariant features},
	volume = {2},
	year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCV.1999.790410}}

@article{Fedorov:2012aa,
	abstract = {Quantitative analysis has tremendous but mostly unrealized potential in healthcare to support objective and accurate interpretation of the clinical imaging. In 2008, the National Cancer Institute began building the Quantitative Imaging Network (QIN) initiative with the goal of advancing quantitative imaging in the context of personalized therapy and evaluation of treatment response. Computerized analysis is an important component contributing to reproducibility and efficiency of the quantitative imaging techniques. The success of quantitative imaging is contingent on robust analysis methods and software tools to bring these methods from bench to bedside. 3D Slicer is a free open-source software application for medical image computing. As a clinical research tool, 3D Slicer is similar to a radiology workstation that supports versatile visualizations but also provides advanced functionality such as automated segmentation and registration for a variety of application domains. Unlike a typical radiology workstation, 3D Slicer is free and is not tied to specific hardware. As a programming platform, 3D Slicer facilitates translation and evaluation of the new quantitative methods by allowing the biomedical researcher to focus on the implementation of the algorithm and providing abstractions for the common tasks of data communication, visualization and user interface development. Compared to other tools that provide aspects of this functionality, 3D Slicer is fully open source and can be readily extended and redistributed. In addition, 3D Slicer is designed to facilitate the development of new functionality in the form of 3D Slicer extensions. In this paper, we present an overview of 3D Slicer as a platform for prototyping, development and evaluation of image analysis tools for clinical research applications. To illustrate the utility of the platform in the scope of QIN, we discuss several use cases of 3D Slicer by the existing QIN teams, and we elaborate on the future directions that can further facilitate development and validation of imaging biomarkers using 3D Slicer.},
	author = {Fedorov, Andriy and Beichel, Reinhard and Kalpathy-Cramer, Jayashree and Finet, Julien and Fillion-Robin, Jean-Christophe and Pujol, Sonia and Bauer, Christian and Jennings, Dominique and Fennessy, Fiona and Sonka, Milan and Buatti, John and Aylward, Stephen and Miller, James V and Pieper, Steve and Kikinis, Ron},
	date-added = {2020-06-02 19:54:33 -0700},
	date-modified = {2020-06-02 19:54:33 -0700},
	doi = {10.1016/j.mri.2012.05.001},
	journal = {Magn Reson Imaging},
	journal-full = {Magnetic resonance imaging},
	mesh = {Automation; Biomarkers; Brain Neoplasms; Databases, Factual; Diagnostic Imaging; Glioblastoma; Head and Neck Neoplasms; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Medical Informatics; Positron-Emission Tomography; Prostatic Neoplasms; Software; Tomography, X-Ray Computed},
	month = {Nov},
	number = {9},
	pages = {1323-41},
	pmc = {PMC3466397},
	pmid = {22770690},
	pst = {ppublish},
	title = {3D Slicer as an image computing platform for the Quantitative Imaging Network},
	volume = {30},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.mri.2012.05.001}}

@article{Vercauteren:2009aa,
	abstract = {We propose an efficient non-parametric diffeomorphic image registration algorithm based on Thirion's demons algorithm. In the first part of this paper, we show that Thirion's demons algorithm can be seen as an optimization procedure on the entire space of displacement fields. We provide strong theoretical roots to the different variants of Thirion's demons algorithm. This analysis predicts a theoretical advantage for the symmetric forces variant of the demons algorithm. We show on controlled experiments that this advantage is confirmed in practice and yields a faster convergence. In the second part of this paper, we adapt the optimization procedure underlying the demons algorithm to a space of diffeomorphic transformations. In contrast to many diffeomorphic registration algorithms, our solution is computationally efficient since in practice it only replaces an addition of displacement fields by a few compositions. Our experiments show that in addition to being diffeomorphic, our algorithm provides results that are similar to the ones from the demons algorithm but with transformations that are much smoother and closer to the gold standard, available in controlled experiments, in terms of Jacobians.},
	author = {Vercauteren, Tom and Pennec, Xavier and Perchant, Aymeric and Ayache, Nicholas},
	date-added = {2020-06-02 19:51:22 -0700},
	date-modified = {2020-06-02 19:51:22 -0700},
	doi = {10.1016/j.neuroimage.2008.10.040},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Humans; Image Processing, Computer-Assisted},
	month = {Mar},
	number = {1 Suppl},
	pages = {S61-72},
	pmid = {19041946},
	pst = {ppublish},
	title = {Diffeomorphic demons: efficient non-parametric image registration},
	volume = {45},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2008.10.040}}

@url{nobrainer,
	author = {Jakub Kaczmarzyk and Satra Ghosh},
	date-added = {2020-06-02 18:26:42 -0700},
	date-modified = {2020-06-02 19:36:53 -0700},
	urldate = {https://github.com/neuronets/nobrainer}}

@article{Goubran:2020aa,
	abstract = {Hippocampal volumetry is a critical biomarker of aging and dementia, and it is widely used as a predictor of cognitive performance; however, automated hippocampal segmentation methods are limited because the algorithms are (a) not publicly available, (b) subject to error with significant brain atrophy, cerebrovascular disease and lesions, and/or (c) computationally expensive or require parameter tuning. In this study, we trained a 3D convolutional neural network using 259 bilateral manually delineated segmentations collected from three studies, acquired at multiple sites on different scanners with variable protocols. Our training dataset consisted of elderly cases difficult to segment due to extensive atrophy, vascular disease, and lesions. Our algorithm, (HippMapp3r), was validated against four other publicly available state-of-the-art techniques (HippoDeep, FreeSurfer, SBHV, volBrain, and FIRST). HippMapp3r outperformed the other techniques on all three metrics, generating an average Dice of 0.89 and a correlation coefficient of 0.95. It was two orders of magnitude faster than some of the tested techniques. Further validation was performed on 200 subjects from two other disease populations (frontotemporal dementia and vascular cognitive impairment), highlighting our method's low outlier rate. We finally tested the methods on real and simulated "clinical adversarial" cases to study their robustness to corrupt, low-quality scans. The pipeline and models are available at: https://hippmapp3r.readthedocs.ioto facilitate the study of the hippocampus in large multisite studies.},
	author = {Goubran, Maged and Ntiri, Emmanuel Edward and Akhavein, Hassan and Holmes, Melissa and Nestor, Sean and Ramirez, Joel and Adamo, Sabrina and Ozzoude, Miracle and Scott, Christopher and Gao, Fuqiang and Martel, Anne and Swardfager, Walter and Masellis, Mario and Swartz, Richard and MacIntosh, Bradley and Black, Sandra E},
	date-added = {2020-06-02 18:24:22 -0700},
	date-modified = {2020-06-02 18:24:22 -0700},
	doi = {10.1002/hbm.24811},
	journal = {Hum Brain Mapp},
	journal-full = {Human brain mapping},
	keywords = {brain atrophy; convolutional neural networks; deep learning; dementia; hippocampus; image segmentation},
	month = {Feb},
	number = {2},
	pages = {291-308},
	pmid = {31609046},
	pst = {ppublish},
	title = {Hippocampal segmentation for brains with extensive atrophy using three-dimensional convolutional neural networks},
	volume = {41},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1002/hbm.24811}}

@article{Falk:2019aa,
	abstract = {U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples.},
	author = {Falk, Thorsten and Mai, Dominic and Bensch, Robert and {\c C}i{\c c}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Marrakchi, Yassine and B{\"o}hm, Anton and Deubner, Jan and J{\"a}ckel, Zoe and Seiwald, Katharina and Dovzhenko, Alexander and Tietz, Olaf and Dal Bosco, Cristina and Walsh, Sean and Saltukoglu, Deniz and Tay, Tuan Leng and Prinz, Marco and Palme, Klaus and Simons, Matias and Diester, Ilka and Brox, Thomas and Ronneberger, Olaf},
	date-added = {2020-06-02 18:22:20 -0700},
	date-modified = {2020-06-02 18:22:20 -0700},
	doi = {10.1038/s41592-018-0261-2},
	journal = {Nat Methods},
	journal-full = {Nature methods},
	mesh = {Cell Count; Cloud Computing; Deep Learning; Neural Networks, Computer; Software Design},
	month = {01},
	number = {1},
	pages = {67-70},
	pmid = {30559429},
	pst = {ppublish},
	title = {U-Net: deep learning for cell counting, detection, and morphometry},
	volume = {16},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41592-018-0261-2}}

@article{Gorgolewski:2011aa,
	abstract = {Current neuroimaging software offer users an incredible opportunity to analyze their data in different ways, with different underlying assumptions. Several sophisticated software packages (e.g., AFNI, BrainVoyager, FSL, FreeSurfer, Nipy, R, SPM) are used to process and analyze large and often diverse (highly multi-dimensional) data. However, this heterogeneous collection of specialized applications creates several issues that hinder replicable, efficient, and optimal use of neuroimaging analysis approaches: (1) No uniform access to neuroimaging analysis software and usage information; (2) No framework for comparative algorithm development and dissemination; (3) Personnel turnover in laboratories often limits methodological continuity and training new personnel takes time; (4) Neuroimaging software packages do not address computational efficiency; and (5) Methods sections in journal articles are inadequate for reproducing results. To address these issues, we present Nipype (Neuroimaging in Python: Pipelines and Interfaces; http://nipy.org/nipype), an open-source, community-developed, software package, and scriptable library. Nipype solves the issues by providing Interfaces to existing neuroimaging software with uniform usage semantics and by facilitating interaction between these packages using Workflows. Nipype provides an environment that encourages interactive exploration of algorithms, eases the design of Workflows within and between packages, allows rapid comparative development of algorithms and reduces the learning curve necessary to use different packages. Nipype supports both local and remote execution on multi-core machines and clusters, without additional scripting. Nipype is Berkeley Software Distribution licensed, allowing anyone unrestricted usage. An open, community-driven development philosophy allows the software to quickly adapt and address the varied needs of the evolving neuroimaging community, especially in the context of increasing demand for reproducible research.},
	author = {Gorgolewski, Krzysztof and Burns, Christopher D and Madison, Cindee and Clark, Dav and Halchenko, Yaroslav O and Waskom, Michael L and Ghosh, Satrajit S},
	date-added = {2020-06-02 18:15:34 -0700},
	date-modified = {2020-06-02 18:15:34 -0700},
	doi = {10.3389/fninf.2011.00013},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	keywords = {Python; data processing; neuroimaging; pipeline; reproducible research; workflow},
	pages = {13},
	pmc = {PMC3159964},
	pmid = {21897815},
	pst = {epublish},
	title = {Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python},
	volume = {5},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.3389/fninf.2011.00013}}

@article{Muschelli:2019aa,
	abstract = {Neuroconductor (https://neuroconductor.org) is an open-source platform for rapid testing and dissemination of reproducible computational imaging software. The goals of the project are to: (i) provide a centralized repository of R software dedicated to image analysis, (ii) disseminate software updates quickly, (iii) train a large, diverse community of scientists using detailed tutorials and short courses, (iv) increase software quality via automatic and manual quality controls, and (v) promote reproducibility of image data analysis. Based on the programming language R (https://www.r-project.org/), Neuroconductor starts with 51 inter-operable packages that cover multiple areas of imaging including visualization, data processing and storage, and statistical inference. Neuroconductor accepts new R package submissions, which are subject to a formal review and continuous automated testing. We provide a description of the purpose of Neuroconductor and the user and developer experience.},
	author = {Muschelli, John and Gherman, Adrian and Fortin, Jean-Philippe and Avants, Brian and Whitcher, Brandon and Clayden, Jonathan D and Caffo, Brian S and Crainiceanu, Ciprian M},
	date-added = {2020-06-02 18:12:56 -0700},
	date-modified = {2020-10-07 12:40:05 -0700},
	doi = {10.1093/biostatistics/kxx068},
	journal = {Biostatistics},
	journal-full = {Biostatistics (Oxford, England)},
	keywords = {Bioinformatics; Image analysis; Statistical modelling},
	mesh = {Diagnostic Imaging; Female; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Male; Neuroimaging; Software},
	month = {04},
	number = {2},
	pages = {218-239},
	pmc = {PMC6409417},
	pmid = {29325029},
	pst = {ppublish},
	title = {Neuroconductor: an {R} platform for medical imaging analysis},
	volume = {20},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1093/biostatistics/kxx068}}

@article{Halchenko:2012aa,
	author = {Halchenko, Yaroslav O and Hanke, Michael},
	date-added = {2020-06-02 18:10:41 -0700},
	date-modified = {2020-06-02 18:10:41 -0700},
	doi = {10.3389/fninf.2012.00022},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	pages = {22},
	pmc = {PMC3458431},
	pmid = {23055966},
	pst = {epublish},
	title = {Open is Not Enough. Let's Take the Next Step: An Integrated, Community-Driven Computing Platform for Neuroscience},
	volume = {6},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.3389/fninf.2012.00022}}

@article{Gorgolewski:2016aa,
	abstract = {The development of magnetic resonance imaging (MRI) techniques has defined modern neuroimaging. Since its inception, tens of thousands of studies using techniques such as functional MRI and diffusion weighted imaging have allowed for the non-invasive study of the brain. Despite the fact that MRI is routinely used to obtain data for neuroscience research, there has been no widely adopted standard for organizing and describing the data collected in an imaging experiment. This renders sharing and reusing data (within or between labs) difficult if not impossible and unnecessarily complicates the application of automatic pipelines and quality assurance protocols. To solve this problem, we have developed the Brain Imaging Data Structure (BIDS), a standard for organizing and describing MRI datasets. The BIDS standard uses file formats compatible with existing software, unifies the majority of practices already common in the field, and captures the metadata necessary for most common data processing operations. },
	author = {Gorgolewski, Krzysztof J and Auer, Tibor and Calhoun, Vince D and Craddock, R Cameron and Das, Samir and Duff, Eugene P and Flandin, Guillaume and Ghosh, Satrajit S and Glatard, Tristan and Halchenko, Yaroslav O and Handwerker, Daniel A and Hanke, Michael and Keator, David and Li, Xiangrui and Michael, Zachary and Maumet, Camille and Nichols, B Nolan and Nichols, Thomas E and Pellman, John and Poline, Jean-Baptiste and Rokem, Ariel and Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and Turner, Jessica A and Varoquaux, Ga{\"e}l and Poldrack, Russell A},
	date-added = {2020-06-02 18:05:10 -0700},
	date-modified = {2020-06-02 18:05:10 -0700},
	doi = {10.1038/sdata.2016.44},
	journal = {Sci Data},
	journal-full = {Scientific data},
	mesh = {Data Collection; Datasets as Topic; Humans; Magnetic Resonance Imaging; Neuroimaging},
	month = {Jun},
	pages = {160044},
	pmc = {PMC4978148},
	pmid = {27326542},
	pst = {epublish},
	title = {The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments},
	volume = {3},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1038/sdata.2016.44}}

@article{De-Leener:2017aa,
	abstract = {For the past 25 years, the field of neuroimaging has witnessed the development of several software packages for processing multi-parametric magnetic resonance imaging (mpMRI) to study the brain. These software packages are now routinely used by researchers and clinicians, and have contributed to important breakthroughs for the understanding of brain anatomy and function. However, no software package exists to process mpMRI data of the spinal cord. Despite the numerous clinical needs for such advanced mpMRI protocols (multiple sclerosis, spinal cord injury, cervical spondylotic myelopathy, etc.), researchers have been developing specific tools that, while necessary, do not provide an integrative framework that is compatible with most usages and that is capable of reaching the community at large. This hinders cross-validation and the possibility to perform multi-center studies. In this study we introduce the Spinal Cord Toolbox (SCT), a comprehensive software dedicated to the processing of spinal cord MRI data. SCT builds on previously-validated methods and includes state-of-the-art MRI templates and atlases of the spinal cord, algorithms to segment and register new data to the templates, and motion correction methods for diffusion and functional time series. SCT is tailored towards standardization and automation of the processing pipeline, versatility, modularity, and it follows guidelines of software development and distribution. Preliminary applications of SCT cover a variety of studies, from cross-sectional area measures in large databases of patients, to the precise quantification of mpMRI metrics in specific spinal pathways. We anticipate that SCT will bring together the spinal cord neuroimaging community by establishing standard templates and analysis procedures.},
	author = {De Leener, Benjamin and L{\'e}vy, Simon and Dupont, Sara M and Fonov, Vladimir S and Stikov, Nikola and Louis Collins, D and Callot, Virginie and Cohen-Adad, Julien},
	date-added = {2020-06-02 17:57:01 -0700},
	date-modified = {2020-10-07 12:40:25 -0700},
	doi = {10.1016/j.neuroimage.2016.10.009},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	keywords = {Atlas; MRI; Open-source; Software; Spinal cord; Template},
	mesh = {Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Spinal Cord},
	month = {01},
	number = {Pt A},
	pages = {24-43},
	pmid = {27720818},
	pst = {ppublish},
	title = {SCT: Spinal Cord Toolbox, an open-source software for processing spinal cord {MRI} data},
	volume = {145},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neuroimage.2016.10.009}}

@article{Esteban:2019aa,
	abstract = {Preprocessing of functional magnetic resonance imaging (fMRI) involves numerous steps to clean and standardize the data before statistical analysis. Generally, researchers create ad hoc preprocessing workflows for each dataset, building upon a large inventory of available tools. The complexity of these workflows has snowballed with rapid advances in acquisition and processing. We introduce fMRIPrep, an analysis-agnostic tool that addresses the challenge of robust and reproducible preprocessing for fMRI data. fMRIPrep automatically adapts a best-in-breed workflow to the idiosyncrasies of virtually any dataset, ensuring high-quality preprocessing without manual intervention. By introducing visual assessment checkpoints into an iterative integration framework for software testing, we show that fMRIPrep robustly produces high-quality results on a diverse fMRI data collection. Additionally, fMRIPrep introduces less uncontrolled spatial smoothness than observed with commonly used preprocessing tools. fMRIPrep equips neuroscientists with an easy-to-use and transparent preprocessing workflow, which can help ensure the validity of inference and the interpretability of results.},
	author = {Esteban, Oscar and Markiewicz, Christopher J and Blair, Ross W and Moodie, Craig A and Isik, A Ilkay and Erramuzpe, Asier and Kent, James D and Goncalves, Mathias and DuPre, Elizabeth and Snyder, Madeleine and Oya, Hiroyuki and Ghosh, Satrajit S and Wright, Jessey and Durnez, Joke and Poldrack, Russell A and Gorgolewski, Krzysztof J},
	date-added = {2020-06-02 17:54:54 -0700},
	date-modified = {2020-10-23 07:59:19 -0700},
	doi = {10.1038/s41592-018-0235-4},
	journal = {Nat Methods},
	journal-full = {Nature methods},
	mesh = {Brain Mapping; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Reproducibility of Results; Workflow},
	month = {01},
	number = {1},
	pages = {111-116},
	pmc = {PMC6319393},
	pmid = {30532080},
	pst = {ppublish},
	title = {fMRIPrep: a robust preprocessing pipeline for functional {MRI}},
	volume = {16},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41592-018-0235-4}}

@article{Peyrat:2010aa,
	abstract = {We propose a framework for the nonlinear spatiotemporal registration of 4D time-series of images based on the Diffeomorphic Demons (DD) algorithm. In this framework, the 4D spatiotemporal registration is decoupled into a 4D temporal registration, defined as mapping physiological states, and a 4D spatial registration, defined as mapping trajectories of physical points. Our contribution focuses more specifically on the 4D spatial registration that should be consistent over time as opposed to 3D registration that solely aims at mapping homologous points at a given time-point. First, we estimate in each sequence the motion displacement field, which is a dense representation of the point trajectories we want to register. Then, we perform simultaneously 3D registrations of corresponding time-points with the constraints to map the same physical points over time called the trajectory constraints. Under these constraints, we show that the 4D spatial registration can be formulated as a multichannel registration of 3D images. To solve it, we propose a novel version of the Diffeomorphic Demons (DD) algorithm extended to vector-valued 3D images, the Multichannel Diffeomorphic Demons (MDD). For evaluation, this framework is applied to the registration of 4D cardiac computed tomography (CT) sequences and compared to other standard methods with real patient data and synthetic data simulated from a physiologically realistic electromechanical cardiac model. Results show that the trajectory constraints act as a temporal regularization consistent with motion whereas the multichannel registration acts as a spatial regularization. Finally, using these trajectory constraints with multichannel registration yields the best compromise between registration accuracy, temporal and spatial smoothness, and computation times. A prospective example of application is also presented with the spatiotemporal registration of 4D cardiac CT sequences of the same patient before and after radiofrequency ablation (RFA) in case of atrial fibrillation (AF). The intersequence spatial transformations over a cardiac cycle allow to analyze and quantify the regression of left ventricular hypertrophy and its impact on the cardiac function.},
	author = {Peyrat, Jean-Marc and Delingette, Herv{\'e} and Sermesant, Maxime and Xu, Chenyang and Ayache, Nicholas},
	date-added = {2020-06-02 17:41:45 -0700},
	date-modified = {2020-06-02 17:41:45 -0700},
	doi = {10.1109/TMI.2009.2038908},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Cardiac-Gated Imaging Techniques; Humans; Imaging, Three-Dimensional; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Tomography, X-Ray Computed},
	month = {Jul},
	number = {7},
	pages = {1351-68},
	pmid = {20304732},
	pst = {ppublish},
	title = {Registration of 4D cardiac CT sequences under trajectory constraints with multichannel diffeomorphic demons},
	volume = {29},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/TMI.2009.2038908}}

@article{Rueckert:1999aa,
	abstract = {In this paper we present a new approach for the nonrigid registration of contrast-enhanced breast MRI. A hierarchical transformation model of the motion of the breast has been developed. The global motion of the breast is modeled by an affine transformation while the local breast motion is described by a free-form deformation (FFD) based on B-splines. Normalized mutual information is used as a voxel-based similarity measure which is insensitive to intensity changes as a result of the contrast enhancement. Registration is achieved by minimizing a cost function, which represents a combination of the cost associated with the smoothness of the transformation and the cost associated with the image similarity. The algorithm has been applied to the fully automated registration of three-dimensional (3-D) breast MRI in volunteers and patients. In particular, we have compared the results of the proposed nonrigid registration algorithm to those obtained using rigid and affine registration techniques. The results clearly indicate that the nonrigid registration algorithm is much better able to recover the motion and deformation of the breast than rigid or affine registration algorithms.},
	author = {Rueckert, D and Sonoda, L I and Hayes, C and Hill, D L and Leach, M O and Hawkes, D J},
	date-added = {2020-06-02 17:39:53 -0700},
	date-modified = {2020-06-02 17:39:53 -0700},
	doi = {10.1109/42.796284},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Breast; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging},
	month = {Aug},
	number = {8},
	pages = {712-21},
	pmid = {10534053},
	pst = {ppublish},
	title = {Nonrigid registration using free-form deformations: application to breast MR images},
	volume = {18},
	year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1109/42.796284}}

@article{Beg:2005aa,
	abstract = {This paper examine the Euler-Lagrange equations for the solution of the large deformation diffeomorphic metric mapping problem studied in Dupuis et al. (1998) and Trouv{\'e}(1995) in which two images I        0, I        1 are given and connected via the diffeomorphic change of coordinates I        0○ϕ−1=I        1 where ϕ=Φ1 is the end point at t= 1 of curve Φt, t∈{$[$}0, 1{$]$} satisfying .Φt=v        t (Φt), t∈{$[$}0,1{$]$} with Φ0=id. The variational problem takes the form},
	author = {Beg, M. Faisal and Miller, Michael I. and Trouv{\'e}, Alain and Younes, Laurent},
	da = {2005/02/01},
	date-added = {2020-06-02 17:38:28 -0700},
	date-modified = {2020-06-02 17:38:28 -0700},
	doi = {10.1023/B:VISI.0000043755.93987.aa},
	id = {Beg2005},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {2},
	pages = {139--157},
	title = {Computing Large Deformation Metric Mappings via Geodesic Flows of Diffeomorphisms},
	ty = {JOUR},
	url = {https://doi.org/10.1023/B:VISI.0000043755.93987.aa},
	volume = {61},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1023/B:VISI.0000043755.93987.aa}}

@article{Jenkinson:2001aa,
	abstract = {Registration is an important component of medical image analysis and for analysing large amounts of data it is desirable to have fully automatic registration methods. Many different automatic registration methods have been proposed to date, and almost all share a common mathematical framework - one of optimising a cost function. To date little attention has been focused on the optimisation method itself, even though the success of most registration methods hinges on the quality of this optimisation. This paper examines the assumptions underlying the problem of registration for brain images using inter-modal voxel similarity measures. It is demonstrated that the use of local optimisation methods together with the standard multi-resolution approach is not sufficient to reliably find the global minimum. To address this problem, a global optimisation method is proposed that is specifically tailored to this form of registration. A full discussion of all the necessary implementation details is included as this is an important part of any practical method. Furthermore, results are presented for inter-modal, inter-subject registration experiments that show that the proposed method is more reliable at finding the global minimum than several of the currently available registration packages in common usage.},
	author = {Jenkinson, M and Smith, S},
	date-added = {2020-06-02 17:31:21 -0700},
	date-modified = {2020-06-02 17:31:21 -0700},
	doi = {10.1016/s1361-8415(01)00036-6},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Brain Mapping; Costs and Cost Analysis; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Mathematics},
	month = {Jun},
	number = {2},
	pages = {143-56},
	pmid = {11516708},
	pst = {ppublish},
	title = {A global optimisation method for robust affine registration of brain images},
	volume = {5},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1016/s1361-8415(01)00036-6}}

@article{Christensen:1996aa,
	abstract = {A general automatic approach is presented for accommodating local shape variation when mapping a two-dimensional (2-D) or three-dimensional (3-D) template image into alignment with a topologically similar target image. Local shape variability is accommodated by applying a vector-field transformation to the underlying material coordinate system of the template while constraining the transformation to be smooth (globally positive definite Jacobian). Smoothness is guaranteed without specifically penalizing large-magnitude deformations of small subvolumes by constraining the transformation on the basis of a Stokesian limit of the fluid-dynamical Navier-Stokes equations. This differs fundamentally from quadratic penalty methods, such as those based on linearized elasticity or thin-plate splines, in that stress restraining the motion relaxes over time allowing large-magnitude deformations. Kinematic nonlinearities are inherently necessary to maintain continuity of structures during large-magnitude deformations, and are included in all results. After initial global registration, final mappings are obtained by numerically solving a set of nonlinear partial differential equations associated with the constrained optimization problem. Automatic regridding is performed by propagating templates as the nonlinear transformations evaluated on a finite lattice become singular. Application of the method to intersubject registration of neuroanatomical structures illustrates the ability to account for local anatomical variability.},
	author = {Christensen, G E and Rabbitt, R D and Miller, M I},
	date-added = {2020-06-02 17:30:10 -0700},
	date-modified = {2020-06-02 17:30:10 -0700},
	doi = {10.1109/83.536892},
	journal = {IEEE Trans Image Process},
	journal-full = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	number = {10},
	pages = {1435-47},
	pmid = {18290061},
	pst = {ppublish},
	title = {Deformable templates using large deformation kinematics},
	volume = {5},
	year = {1996},
	Bdsk-Url-1 = {https://doi.org/10.1109/83.536892}}

@article{Wang:2020aa,
	abstract = {Recent large-scale collaborations are generating major surveys of cell types and connections in the mouse brain, collecting large amounts of data across modalities, spatial scales, and brain areas. Successful integration of these data requires a standard 3D reference atlas. Here, we present the Allen Mouse Brain Common Coordinate Framework (CCFv3) as such a resource. We constructed an average template brain at 10 μm voxel resolution by interpolating high resolution in-plane serial two-photon tomography images with 100 μm z-sampling from 1,675 young adult C57BL/6J mice. Then, using multimodal reference data, we parcellated the entire brain directly in 3D, labeling every voxel with a brain structure spanning 43 isocortical areas and their layers, 329 subcortical gray matter structures, 81 fiber tracts, and 8 ventricular structures. CCFv3 can be used to analyze, visualize, and integrate multimodal and multiscale datasets in 3D and is openly accessible (https://atlas.brain-map.org/).},
	author = {Wang, Quanxin and Ding, Song-Lin and Li, Yang and Royall, Josh and Feng, David and Lesnar, Phil and Graddis, Nile and Naeemi, Maitham and Facer, Benjamin and Ho, Anh and Dolbeare, Tim and Blanchard, Brandon and Dee, Nick and Wakeman, Wayne and Hirokawa, Karla E and Szafer, Aaron and Sunkin, Susan M and Oh, Seung Wook and Bernard, Amy and Phillips, John W and Hawrylycz, Michael and Koch, Christof and Zeng, Hongkui and Harris, Julie A and Ng, Lydia},
	date-added = {2020-06-02 09:57:43 -0700},
	date-modified = {2020-06-02 09:57:43 -0700},
	doi = {10.1016/j.cell.2020.04.007},
	journal = {Cell},
	journal-full = {Cell},
	keywords = {3D brain atlas; CCFv3; average mouse brain; brain anatomy; brain parcellation; common coordinate framework; fiber tracts; mouse cortex; reference atlas; transgenic mice},
	month = {May},
	number = {4},
	pages = {936-953.e20},
	pmid = {32386544},
	pst = {ppublish},
	title = {The Allen Mouse Brain Common Coordinate Framework: A 3D Reference Atlas},
	volume = {181},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.cell.2020.04.007}}

@article{Wang:2004aa,
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
	author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P},
	date-added = {2020-06-02 09:51:35 -0700},
	date-modified = {2020-06-02 09:51:35 -0700},
	doi = {10.1109/tip.2003.819861},
	journal = {IEEE Trans Image Process},
	journal-full = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	mesh = {Algorithms; Data Interpretation, Statistical; Hypermedia; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Models, Statistical; Pattern Recognition, Automated; Quality Control; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique},
	month = {Apr},
	number = {4},
	pages = {600-12},
	pmid = {15376593},
	pst = {ppublish},
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/tip.2003.819861}}

@article{Pontre:2017aa,
	abstract = {Cardiac magnetic resonance perfusion examinations enable noninvasive quantification of myocardial blood flow. However, motion between frames due to breathing must be corrected for quantitative analysis. Although several methods have been proposed, there is a lack of widely available benchmarks to compare different algorithms. We sought to compare many algorithms from several groups in an open benchmark challenge. Nine clinical studies from two different centers comprising normal and diseased myocardium at both rest and stress were made available for this study. The primary validation measure was regional myocardial blood flow based on the transfer coefficient (Ktrans), which was computed using a compartment model and the myocardial perfusion reserve (MPR) index. The ground truth was calculated using contours drawn manually on all frames by a single observer, and visually inspected by a second observer. Six groups participated and 19 different motion correction algorithms were compared. Each method used one of three different motion models: rigid, global affine, or local deformation. The similarity metric also varied with methods employing either sum-of-squared differences, mutual information, or cross correlation. There were no significant differences in Ktrans or MPR compared across different motion models or similarity metrics. Compared with the ground truth, only Ktrans for the sum-of-squared differences metric, and for local deformation motion models, had significant bias. In conclusion, the open benchmark enabled evaluation of clinical perfusion indices over a wide range of methods. In particular, there was no benefit of nonrigid registration techniques over the other methods evaluated in this study. The benchmark data and results are available from the Cardiac Atlas Project ( www.cardiacatlas.org).},
	author = {Pontre, Beau and Cowan, Brett R and DiBella, Edward and Kulaseharan, Sancgeetha and Likhite, Devavrat and Noorman, Nils and Tautz, Lennart and Tustison, Nicholas and Wollny, Gert and Young, Alistair A and Suinesiaputra, Avan},
	date-added = {2020-06-01 20:08:52 -0700},
	date-modified = {2020-06-01 20:08:52 -0700},
	doi = {10.1109/JBHI.2016.2597145},
	journal = {IEEE J Biomed Health Inform},
	journal-full = {IEEE journal of biomedical and health informatics},
	mesh = {Algorithms; Benchmarking; Cardiac Imaging Techniques; Heart; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Angiography; Movement},
	month = {09},
	number = {5},
	pages = {1315-1326},
	pmc = {PMC5658235},
	pmid = {28880152},
	pst = {ppublish},
	title = {An Open Benchmark Challenge for Motion Correction of Myocardial Perfusion MRI},
	volume = {21},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/JBHI.2016.2597145}}

@inproceedings{Zhang:2018aa,
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	author = {R. {Zhang} and P. {Isola} and A. A. {Efros} and E. {Shechtman} and O. {Wang}},
	booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	date-added = {2020-05-31 20:21:47 -0700},
	date-modified = {2020-05-31 20:22:01 -0700},
	doi = {10.1109/CVPR.2018.00068},
	issn = {2575-7075},
	keywords = {image classification;learning (artificial intelligence);deep features;perceptual metric;human perception;deep learning community;image synthesis;human perceptual similarity judgments;ImageNet-trained VGG features;deep visual representations;ImageNet classification;Distortion;Task analysis;Measurement;Visualization;Training;Network architecture;Computer architecture},
	month = {June},
	pages = {586-595},
	title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2018.00068}}

@inproceedings{Avants:2020aa,
	author = {Brian B. Avants and Elliot Greenblatt and Jacob Hesterman and Nicholas J. Tustison},
	booktitle = {Proceedings of the Workshop in Biomedical Image Registration},
	date-added = {2020-05-31 20:13:13 -0700},
	date-modified = {2020-05-31 20:14:27 -0700},
	title = {Deep volumetric feature encoding for biomedical images},
	year = {2020}}

@inproceedings{Zhou:2019aa,
	abstract = {Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via self-supervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis.},
	address = {Cham},
	author = {Zhou, Zongwei and Sodha, Vatsal and Rahman Siddiquee, Md Mahfuzur and Feng, Ruibin and Tajbakhsh, Nima and Gotway, Michael B. and Liang, Jianming},
	booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2019},
	date-added = {2020-05-31 20:10:32 -0700},
	date-modified = {2020-05-31 20:10:49 -0700},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	isbn = {978-3-030-32251-9},
	pages = {384--393},
	publisher = {Springer International Publishing},
	title = {Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis},
	year = {2019}}

@unpublished{Avants:2019aa,
	author = {Brian B Avants and Nicholas J. Tustison and James R. Stone},
	date-added = {2019-08-23 15:23:51 -0700},
	date-modified = {2019-08-23 15:25:12 -0700},
	note = {To be submitted},
	title = {Cross-modality effects of TBI and Blast Exposure in the CENC Cohort: Joint study of neuropsychological measurements, structural and functional MRI}}

@inproceedings{Stone:2019aa,
	author = {James R. Stone and Nicholas J. Tustison and Brian B. Avants and Eric M. Wassermann and Jessica Gill and Walter S. Carr and Carl Goforth and Natalie Domeisen and LT Claire M. Modica and Anna Tschiffely and F. Jay Haran and and Stephen T. Ahlers},
	booktitle = {Proceedings of the Military Health System Research Symposium},
	date-added = {2019-08-23 15:20:10 -0700},
	date-modified = {2019-08-23 15:23:46 -0700},
	title = {Structural and functional neuroimaging in career {B}reachers},
	year = {2019}}

@article{Tustison:2019aa,
	abstract = { Longitudinal studies of development and disease in the human brain have motivated the acquisition of large neuroimaging data sets and the concomitant development of robust methodological and statistical tools for quantifying neurostructural changes. Longitudinal-specific strategies for acquisition and processing have potentially significant benefits including more consistent estimates of intra-subject measurements while retaining predictive power. Using the first phase of the Alzheimer's Disease Neuroimaging Initiative (ADNI-1) data, comprising over 600 subjects with multiple time points from baseline to 36 months, we evaluate the utility of longitudinal FreeSurfer and Advanced Normalization Tools (ANTs) surrogate thickness values in the context of a linear mixed-effects (LME) modeling strategy. Specifically, we estimate the residual variability and between-subject variability associated with each processing stream as it is known from the statistical literature that minimizing the former while simultaneously maximizing the latter leads to greater scientific interpretability in terms of tighter confidence intervals in calculated mean trends, smaller prediction intervals, and narrower confidence intervals for determining cross-sectional effects. This strategy is evaluated over the entire cortex, as defined by the Desikan-Killiany-Tourville labeling protocol, where comparisons are made with the cross-sectional and longitudinal FreeSurfer processing streams. Subsequent linear mixed effects modeling for identifying diagnostic groupings within the ADNI cohort is provided as supporting evidence for the utility of the proposed ANTs longitudinal framework which provides unbiased structural neuroimage processing and competitive to superior power for longitudinal structural change detection.},
	author = {Tustison, Nicholas J and Holbrook, Andrew J and Avants, Brian B and Roberts, Jared M and Cook, Philip A and Reagh, Zachariah M and Duda, Jeffrey T and Stone, James R and Gillen, Daniel L and Yassa, Michael A and {Alzheimer's Disease Neuroimaging Initiative}},
	date-added = {2019-08-23 13:40:15 -0700},
	date-modified = {2020-10-07 12:37:25 -0700},
	doi = {10.3233/JAD-190283},
	journal = {J Alzheimers Dis},
	journal-full = {Journal of Alzheimer's disease : JAD},
	keywords = {Advanced normalization tools; FreeSurfer; linear mixed effects models; longitudinal processing},
	month = {Jul},
	pmid = {31356207},
	pst = {aheadofprint},
	title = {Longitudinal Mapping of Cortical Thickness Measurements: An {A}lzheimer's {D}isease {N}euroimaging {I}nitiative-Based Evaluation Study},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.3233/JAD-190283}}

@article{zhu2012:aa,
	author = {Y. Zhu and Y. Tan and Y. Hua and G. Zhang and J. Zhang},
	date-added = {2016-07-24 02:45:15 +0000},
	date-modified = {2016-07-24 02:48:30 +0000},
	journal = {J Digit Imaging},
	month = {Jun},
	number = {3},
	pages = {409--22},
	title = {Automatic segmentation of ground-glass opacities in lung {CT} images by using {M}arkov random field-based algorithms},
	volume = {25},
	year = {2012}}

@article{Lovinfosse:2016aa,
	abstract = {INTRODUCTION: With (18)F-FDG PET/CT, tumor uptake intensity and heterogeneity have been associated with outcome in several cancers. This study aimed at investigating whether (18)F-FDG uptake intensity, volume or heterogeneity could predict the outcome in patients with non-small cell lung cancers (NSCLC) treated by stereotactic body radiation therapy (SBRT).
METHODS: Sixty-three patients with NSCLC treated by SBRT underwent a (18)F-FDG PET/CT before treatment. Maximum and mean standard uptake value (SUVmax and SUVmean), metabolic tumoral volume (MTV), total lesion glycolysis (TLG), as well as 13 global, local and regional textural features were analysed. The predictive value of these parameters, along with clinical features, was assessed using univariate and multivariate analysis for overall survival (OS), disease-specific survival (DSS) and disease-free survival (DFS). Cutoff values were obtained using logistic regression analysis, and survivals were compared using Kaplan-Meier analysis.
RESULTS: The median follow-up period was 27.1 months for the entire cohort and 32.1 months for the surviving patients. At the end of the study, 25 patients had local and/or distant recurrence including 12 who died because of the cancer progression. None of the clinical variables was predictive of the outcome, except age, which was associated with DFS (HR 1.1, P = 0.002). None of the (18)F-FDG PET/CT or clinical parameters, except gender, were associated with OS. The univariate analysis showed that only dissimilarity (D) was associated with DSS (HR = 0.822, P = 0.037), and that several metabolic measurements were associated with DFS. In multivariate analysis, only dissimilarity was significantly associated with DSS (HR = 0.822, P = 0.037) and with DFS (HR = 0.834, P < 0.01).
CONCLUSION: The textural feature dissimilarity measured on the baseline (18)F-FDG PET/CT appears to be a strong independent predictor of the outcome in patients with NSCLC treated by SBRT. This may help selecting patients who may benefit from closer monitoring and therapeutic optimization.},
	author = {Lovinfosse, Pierre and Janvary, Zsolt Levente and Coucke, Philippe and Jodogne, S{\'e}bastien and Bernard, Claire and Hatt, Mathieu and Visvikis, Dimitris and Jansen, Nicolas and Duysinx, Bernard and Hustinx, Roland},
	date-added = {2016-07-22 05:50:14 +0000},
	date-modified = {2016-07-22 05:50:14 +0000},
	doi = {10.1007/s00259-016-3314-8},
	journal = {Eur J Nucl Med Mol Imaging},
	journal-full = {European journal of nuclear medicine and molecular imaging},
	keywords = {18F-FDG PET/CT; Heterogeneity; Non-small cell lung cancer; Prognostic factor; Stereotactic body radiation therapy; Textural analysis},
	month = {Jul},
	number = {8},
	pages = {1453-60},
	pmid = {26830299},
	pst = {ppublish},
	title = {FDG PET/CT texture analysis for predicting the outcome of lung cancer treated by stereotactic body radiation therapy},
	volume = {43},
	year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00259-016-3314-8}}

@article{Tustison:2016aa,
	abstract = {PURPOSE: To propose an accurate methodological framework for automatically segmenting pulmonary proton MRI based on an optimal consensus of a spatially normalized library of annotated lung atlases.
METHODS: A library of 62 manually annotated lung atlases comprising 48 mixed healthy, chronic obstructive pulmonary disease, and asthmatic subjects of a large age range with multiple ventilation levels is used to produce an optimal segmentation in proton MRI, based on a consensus of the spatially normalized library. An extension of this methodology is used to provide best-guess estimates of lobar subdivisions in proton MRI from annotated computed tomography data.
RESULTS: A leave-one-out evaluation strategy was used for evaluation. Jaccard overlap measures for the left and right lungs were used for performance comparisons relative to the current state-of-the-art (0.966 $\pm$ 0.018 and 0.970 $\pm$ 0.016, respectively). Best-guess estimates for the lobes exhibited comparable performance levels (left upper: 0.882 $\pm$ 0.059, left lower: 0.868 $\pm$ 0.06, right upper: 0.852 $\pm$ 0.067, right middle: 0.657 $\pm$ 0.130, right lower: 0.873 $\pm$ 0.063).
CONCLUSION: An annotated atlas library approach can be used to provide good lung and lobe estimation in proton MRI. The proposed framework is useful for subsequent anatomically based analysis of structural and/or functional pulmonary image data. Magn Reson Med 76:315-320, 2016. {\copyright} 2015 Wiley Periodicals, Inc.},
	author = {Tustison, Nicholas J and Qing, Kun and Wang, Chengbo and Altes, Talissa A and Mugler, 3rd, John P},
	date-added = {2016-07-22 05:08:18 +0000},
	date-modified = {2016-07-22 05:08:18 +0000},
	doi = {10.1002/mrm.25824},
	journal = {Magn Reson Med},
	journal-full = {Magnetic resonance in medicine},
	keywords = {advanced normalization tools; lobe segmentation; lung segmentation; multi-atlas label fusion; pulmonary image registration},
	month = {Jul},
	number = {1},
	pages = {315-20},
	pmid = {26222827},
	pst = {ppublish},
	title = {Atlas-based estimation of lung and lobar anatomy in proton MRI},
	volume = {76},
	year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/mrm.25824}}

@article{Henderson:2005aa,
	abstract = {Doctors' survival predictions for terminally ill patients have been shown to be inaccurate and there has been an argument for less guesswork and more use of carefully constructed statistical indices. As statisticians, the authors are less confident in the predictive value of statistical models and indices for individual survival times. This paper discusses and illustrates a variety of measures which can be used to summarise predictive information available from a statistical model. The authors argue that models and statistical indices can be useful at the group or population level, but that human survival is so uncertain that even the best statistical analysis cannot provide single-number predictions of real use for individual patients.},
	author = {Henderson, R and Keiding, N},
	date-added = {2016-07-21 17:39:58 +0000},
	date-modified = {2016-07-21 17:39:58 +0000},
	doi = {10.1136/jme.2005.012427},
	journal = {J Med Ethics},
	journal-full = {Journal of medical ethics},
	keywords = {Death and Euthanasia},
	mesh = {Carcinoma, Non-Small-Cell Lung; Female; Humans; Life Expectancy; Lung Neoplasms; Male; Prognosis; Proportional Hazards Models; Risk Factors; Terminally Ill; Time Factors},
	month = {Dec},
	number = {12},
	pages = {703-6},
	pmc = {PMC1734073},
	pmid = {16319233},
	pst = {ppublish},
	title = {Individual survival time prediction using statistical models},
	volume = {31},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1136/jme.2005.012427}}

@article{Depeursinge:2014aa,
	abstract = {Three-dimensional computerized characterization of biomedical solid textures is key to large-scale and high-throughput screening of imaging data. Such data increasingly become available in the clinical and research environments with an ever increasing spatial resolution. In this text we exhaustively analyze the state-of-the-art in 3-D biomedical texture analysis to identify the specific needs of the application domains and extract promising trends in image processing algorithms. The geometrical properties of biomedical textures are studied both in their natural space and on digitized lattices. It is found that most of the tissue types have strong multi-scale directional properties, that are well captured by imaging protocols with high resolutions and spherical spatial transfer functions. The information modeled by the various image processing techniques is analyzed and visualized by displaying their 3-D texture primitives. We demonstrate that non-convolutional approaches are expected to provide best results when the size of structures are inferior to five voxels. For larger structures, it is shown that only multi-scale directional convolutional approaches that are non-separable allow for an unbiased modeling of 3-D biomedical textures. With the increase of high-resolution isotropic imaging protocols in clinical routine and research, these models are expected to best leverage the wealth of 3-D biomedical texture analysis in the future. Future research directions and opportunities are proposed to efficiently model personalized image-based phenotypes of normal biomedical tissue and its alterations. The integration of the clinical and genomic context is expected to better explain the intra class variation of healthy biomedical textures. Using texture synthesis, this provides the exciting opportunity to simulate and visualize texture atlases of normal ageing process and disease progression for enhanced treatment planning and clinical care management.},
	author = {Depeursinge, Adrien and Foncubierta-Rodriguez, Antonio and Van De Ville, Dimitri and M{\"u}ller, Henning},
	date-added = {2016-07-21 17:36:44 +0000},
	date-modified = {2016-07-21 17:36:44 +0000},
	doi = {10.1016/j.media.2013.10.005},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	keywords = {3-D texture; Classification; Solid texture; Texture primitive; Volumetric texture},
	mesh = {Algorithms; Computer Simulation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Biological; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity},
	month = {Jan},
	number = {1},
	pages = {176-96},
	pmid = {24231667},
	pst = {ppublish},
	title = {Three-dimensional solid texture analysis in biomedical imaging: review and opportunities},
	volume = {18},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2013.10.005}}

@article{Chicklore:2013aa,
	abstract = {(18)F-Fluorodeoxyglucose positron emission tomography/computed tomography ((18)F-FDG PET/CT) is now routinely used in oncological imaging for diagnosis and staging and increasingly to determine early response to treatment, often employing semiquantitative measures of lesion activity such as the standardized uptake value (SUV). However, the ability to predict the behaviour of a tumour in terms of future therapy response or prognosis using SUVs from a baseline scan prior to treatment is limited. It is recognized that medical images contain more useful information than may be perceived with the naked eye, leading to the field of "radiomics" whereby additional features can be extracted by computational postprocessing techniques. In recent years, evidence has slowly accumulated showing that parameters obtained by texture analysis of radiological images, reflecting the underlying spatial variation and heterogeneity of voxel intensities within a tumour, may yield additional predictive and prognostic information. It is hoped that measurement of these textural features may allow better tissue characterization as well as better stratification of treatment in clinical trials, or individualization of future cancer treatment in the clinic, than is possible with current imaging biomarkers. In this review we focus on the literature describing the emerging methods of texture analysis in (18)FDG PET/CT, as well as other imaging modalities, and how the measurement of spatial variation of voxel grey-scale intensity within an image may provide additional predictive and prognostic information, and postulate the underlying biological mechanisms.},
	author = {Chicklore, Sugama and Goh, Vicky and Siddique, Musib and Roy, Arunabha and Marsden, Paul K and Cook, Gary J R},
	date-added = {2016-07-21 17:21:31 +0000},
	date-modified = {2016-07-21 17:21:31 +0000},
	doi = {10.1007/s00259-012-2247-0},
	journal = {Eur J Nucl Med Mol Imaging},
	journal-full = {European journal of nuclear medicine and molecular imaging},
	mesh = {Fluorodeoxyglucose F18; Humans; Mathematical Computing; Multimodal Imaging; Neoplasms; Positron-Emission Tomography; Probability; Prognosis; Radiopharmaceuticals; Tomography, X-Ray Computed},
	month = {Jan},
	number = {1},
	pages = {133-40},
	pmid = {23064544},
	pst = {ppublish},
	title = {Quantifying tumour heterogeneity in 18F-FDG PET/CT imaging by texture analysis},
	volume = {40},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00259-012-2247-0}}

@article{Ryu:2014aa,
	author = {Ryu, Jeong-Seon and Hyun, In Young},
	date-added = {2016-07-21 17:20:51 +0000},
	date-modified = {2016-07-21 17:20:51 +0000},
	doi = {10.1200/JCO.2013.53.6300},
	journal = {J Clin Oncol},
	journal-full = {Journal of clinical oncology : official journal of the American Society of Clinical Oncology},
	mesh = {Antineoplastic Combined Chemotherapy Protocols; Carcinoma, Non-Small-Cell Lung; Female; Fluorodeoxyglucose F18; Humans; Lung Neoplasms; Male; Positron-Emission Tomography; Radiopharmaceuticals},
	month = {May},
	number = {15},
	pages = {1630},
	pmid = {24752045},
	pst = {ppublish},
	title = {Prognostic impact of [18F]fluorodeoxyglucose positron emission tomography scanning in the era of molecular oncology},
	volume = {32},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1200/JCO.2013.53.6300}}

@article{Berman:2014aa,
	author = {Berman, Abigail T and Ellenberg, Susan S and Simone, 2nd, Charles B},
	date-added = {2016-07-21 17:20:05 +0000},
	date-modified = {2016-07-21 17:20:05 +0000},
	doi = {10.1200/JCO.2013.54.3074},
	journal = {J Clin Oncol},
	journal-full = {Journal of clinical oncology : official journal of the American Society of Clinical Oncology},
	mesh = {Antineoplastic Combined Chemotherapy Protocols; Carcinoma, Non-Small-Cell Lung; Female; Fluorodeoxyglucose F18; Humans; Lung Neoplasms; Male; Positron-Emission Tomography; Radiopharmaceuticals},
	month = {May},
	number = {15},
	pages = {1631-2},
	pmid = {24752052},
	pst = {ppublish},
	title = {Predicting survival in non-small-cell lung cancer using positron emission tomography: several conclusions from multiple comparisons},
	volume = {32},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1200/JCO.2013.54.3074}}

@article{Kim:2012aa,
	abstract = {OBJECTIVES: The aim of this study was to evaluate the usefulness of the tumor burden as characterized by the metabolic tumor volume (MTV) and total lesion glycolysis (TLG) measured by F-18 fluoro-2-deoxyglucose (F-18 FDG) PET-computed tomography (CT) in predicting recurrence-free survival (RFS) and overall survival (OS) in surgically resected non-small-cell lung cancer (NSCLC) patients.
METHODS: We retrospectively reviewed 91 patients with pathologically documented stages I-IIIA NSCLC. MTV and TLG were obtained according to various thresholds of the standard uptake value (SUV) of primary tumor using preoperative F-18 FDG PET-CT. We used comparison receiver-operating characteristic curve analysis to test the statistical significance of the differences among the multiple volumetric parameters calculated by various SUV cutoff values. RFS and OS were evaluated with the Kaplan-Meier method and Cox regression analysis.
RESULTS: On comparison receiver-operating characteristic curve analysis, no significant difference was found among the volumetric parameters calculated using various thresholds of SUV. Regardless of the thresholds, patients with smaller MTV and lower TLG showed longer RFS and OS. MTV and TLG measured by F-18 FDG PET-CT were found to have better predictive performance than SUVmax for recurrence and death. According to multivariate analyses, MTV2.5 was revealed as a significant prognostic factor for RFS. Tumor size over 3 cm was selected as a significant prognostic indicator of OS.
CONCLUSION: Volume-based parameters of F-18 FDG PET-CT may have a role in providing prognostic information in NSCLC patients who have received surgical treatment.},
	author = {Kim, Keunyoung and Kim, Seong-Jang and Kim, In-Joo and Kim, Yun Seong and Pak, Kyoungjune and Kim, Heeyoung},
	date-added = {2016-07-21 17:19:24 +0000},
	date-modified = {2016-07-21 17:19:24 +0000},
	doi = {10.1097/MNM.0b013e328351d4f5},
	journal = {Nucl Med Commun},
	journal-full = {Nuclear medicine communications},
	mesh = {Aged; Aged, 80 and over; Carcinoma, Non-Small-Cell Lung; Disease-Free Survival; Female; Fluorodeoxyglucose F18; Humans; Lung Neoplasms; Male; Middle Aged; Multimodal Imaging; Positron-Emission Tomography; Prognosis; Radiopharmaceuticals; Retrospective Studies; Survival Rate; Tomography, X-Ray Computed; Tumor Burden},
	month = {Jun},
	number = {6},
	pages = {613-20},
	pmid = {22407127},
	pst = {ppublish},
	title = {Prognostic value of volumetric parameters measured by F-18 FDG PET/CT in surgically resected non-small-cell lung cancer},
	volume = {33},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1097/MNM.0b013e328351d4f5}}

@article{Horne:2014aa,
	abstract = {BACKGROUND: This retrospective study aims to assess the usefulness of SUV(max) from FDG-PET imaging as a prognosticator for primary biopsy-proven stage I NSCLC treated with SBRT.
METHODS: This study includes 95 patients of median age 77 years, with primary, biopsy-confirmed peripheral stage IA/IB NSCLC. All patients were treated with 60 Gy in 3 fractions with a median treatment time of six days. Local, regional, and distant failures were evaluated independently according to the terms of RTOG1021. Local, regional, and distant control, overall- and progression-free survival were estimated by the Kaplan-Meier method. Cox proportional hazards regression was performed to determine whether SUV(max), age, KPS, gender, tumor size/T stage, or smoking history influenced outcomes. SUV(max) was evaluated as both a continuous and as a dichotomous variable using a cutoff of <5 and ≥ 5.
RESULTS: Median follow-up for the cohort was 16 months. Median OS and PFS were 25.3 and 40.3 months, respectively. SUV with a cutoff value of 5 predicted for OS and PFS (p = .024 for each) but did not achieve significance for LC (p = .256). On Cox univariate regression analysis, SUV as a dichotomous variable predicted for both OS and PFS (p = .027 and p = .030, respectively). Defined as a continuous variable, SUV(max) continued to predict for OS and PFS (p = .032 and p = .003), but also predicted LC (p = .045) and trended toward significance for DC (p = .059). SUV(max) did not predict for OS as a dichotomous or continuous variable. It did, however, predict for PFS as a continuous variable (p = .008), neared significance for local control (p = .057) and trended towards, significance for distant control (p = .092).
CONCLUSIONS: SUV(max) appears to be a statistically and clinically significant independent prognostic marker for progression-free survival in patients with stage I NSCLC treated with SBRT. Prospective studies to more accurately define the role of tumor FDG uptake in the prognosis of NSCLC are warranted.},
	author = {Horne, Zachary D and Clump, David A and Vargo, John A and Shah, Samir and Beriwal, Sushil and Burton, Steven A and Quinn, Annette E and Schuchert, Matthew J and Landreneau, Rodney J and Christie, Neil A and Luketich, James D and Heron, Dwight E},
	date-added = {2016-07-21 17:17:12 +0000},
	date-modified = {2016-07-21 17:17:12 +0000},
	doi = {10.1186/1748-717X-9-41},
	journal = {Radiat Oncol},
	journal-full = {Radiation oncology (London, England)},
	mesh = {Aged; Aged, 80 and over; Carcinoma, Non-Small-Cell Lung; Disease-Free Survival; Female; Fluorodeoxyglucose F18; Humans; Lung Neoplasms; Male; Middle Aged; Multimodal Imaging; Neoplasm Staging; Positron-Emission Tomography; Prognosis; Radiosurgery; Radiotherapy Planning, Computer-Assisted},
	pages = {41},
	pmc = {PMC3922961},
	pmid = {24479954},
	pst = {epublish},
	title = {Pretreatment SUVmax predicts progression-free survival in early-stage non-small cell lung cancer treated with stereotactic body radiation therapy},
	volume = {9},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1186/1748-717X-9-41}}

@article{Lee:2010aa,
	abstract = {INTRODUCTION: The purpose of this study was to assess the value of tumor response evaluation using combined interpretation of [18F] fluorodeoxyglucose positron emission tomography (PET) and computed tomography (CT) for the prediction of clinical outcome and pathologic response in patients with stage III non-small cell lung cancer who underwent neoadjuvant chemotherapy followed by surgery.
METHODS: This study was approved by the Institutional Review Board with a waiver of informed consent. Forty-four consecutive patients (M:F = 32:12; mean age, 60.7 years) with locally advanced non-small cell lung cancer received neoadjuvant chemotherapy followed by curative surgery. Time to recurrence (TTR) was stratified by radiologic, metabolic, and radiologic-metabolic response using the Kaplan-Meier method. The accuracy of radiologic, metabolic, and radiologic-metabolic response criteria for the prediction of pathologic response was evaluated.
RESULTS: Radiologic-metabolic responders had a longer TTR than nonresponders (mean TTR, 58.7 months versus 22.3 months, p = 0.001 with criteria of >or=30% reduction of size and >or=50% reduction of [maximum standardized uptake value] SUVmax and mean TTR, 49.4 months versus 23.5 months, p = 0.022 with criteria of >or=30% reduction of size and >or=25% reduction of SUVmax, respectively). The TTR of radiologic responders (criteria of >or=30% reduction of size) and metabolic responders (criteria of >or=25% reduction of SUVmax) was not different from the TTR of nonresponders (p > 0.05). The accuracy for the prediction of pathologic response was 70% in radiologic responders, 52 to 75% in metabolic responders, and 73 to 82% in radiologic-metabolic responders.
CONCLUSIONS: Tumor response evaluation using combined interpretation of [18F] fluorodeoxyglucose-PET and CT was more effective than single interpretation of CT response or PET response alone for the prediction of tumor recurrence and pathologic response.},
	author = {Lee, Ho Yun and Lee, Hyun Ju and Kim, Young Tae and Kang, Chang Hyun and Jang, Bo Gun and Chung, Doo Hyun and Goo, Jin Mo and Park, Chang Min and Lee, Chang Hyun and Kang, Keon Wook},
	date-added = {2016-07-21 17:16:39 +0000},
	date-modified = {2016-07-21 17:16:39 +0000},
	doi = {10.1097/JTO.0b013e3181d2efe7},
	journal = {J Thorac Oncol},
	journal-full = {Journal of thoracic oncology : official publication of the International Association for the Study of Lung Cancer},
	mesh = {Adenocarcinoma; Adult; Aged; Antineoplastic Combined Chemotherapy Protocols; Carcinoma, Large Cell; Carcinoma, Non-Small-Cell Lung; Carcinoma, Squamous Cell; Combined Modality Therapy; Disease Progression; Female; Fluorodeoxyglucose F18; Follow-Up Studies; Humans; Lung Neoplasms; Male; Middle Aged; Neoadjuvant Therapy; Neoplasm Recurrence, Local; Neoplasm Staging; Positron-Emission Tomography; Radiopharmaceuticals; Retrospective Studies; Survival Rate; Time Factors; Tomography, X-Ray Computed; Treatment Outcome},
	month = {Apr},
	number = {4},
	pages = {497-503},
	pmid = {20195167},
	pst = {ppublish},
	title = {Value of combined interpretation of computed tomography response and positron emission tomography response for prediction of prognosis after neoadjuvant chemotherapy in non-small cell lung cancer},
	volume = {5},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1097/JTO.0b013e3181d2efe7}}

@article{Lardinois:2003aa,
	abstract = {BACKGROUND: We compared the diagnostic accuracy of integrated positron-emission tomography (PET) and computed tomography (CT) with that of CT alone, that of PET alone, and that of conventional visual correlation of PET and CT in determining the stage of disease in non-small-cell lung cancer.
METHODS: In a prospective study, integrated PET-CT was performed in 50 patients with proven or suspected non-small-cell lung cancer. CT and PET alone, visually correlated PET and CT, and integrated PET-CT were evaluated separately, and a tumor-node-metastasis (TNM) stage was assigned on the basis of image analysis. Nodal stations were identified according to the mapping system of the American Thoracic Society. The standard of reference was histopathological assessment of tumor stage and node stage. Extrathoracic metastases were confirmed histopathologically or by at least one other imaging method. A paired sign test was used to compare integrated PET-CT with the other imaging methods.
RESULTS: Integrated PET-CT provided additional information in 20 of 49 patients (41 percent), beyond that provided by conventional visual correlation of PET and CT. Integrated PET-CT had better diagnostic accuracy than the other imaging methods. Tumor staging was significantly more accurate with integrated PET-CT than with CT alone (P=0.001), PET alone (P<0.001), or visual correlation of PET and CT (P=0.013); node staging was also significantly more accurate with integrated PET-CT than with PET alone (P=0.013). In metastasis staging, integrated PET-CT increased the diagnostic certainty in two of eight patients.
CONCLUSIONS: Integrated PET-CT improves the diagnostic accuracy of the staging of non-small-cell lung cancer.},
	author = {Lardinois, Didier and Weder, Walter and Hany, Thomas F and Kamel, Ehab M and Korom, Stephan and Seifert, Burkhardt and von Schulthess, Gustav K and Steinert, Hans C},
	date-added = {2016-07-21 17:16:06 +0000},
	date-modified = {2016-07-21 17:16:06 +0000},
	doi = {10.1056/NEJMoa022136},
	journal = {N Engl J Med},
	journal-full = {The New England journal of medicine},
	mesh = {Aged; Aged, 80 and over; Carcinoma, Non-Small-Cell Lung; Female; Fluorodeoxyglucose F18; Humans; Lung Neoplasms; Lymphatic Metastasis; Male; Middle Aged; Neoplasm Staging; Prospective Studies; Radiopharmaceuticals; Tomography, Emission-Computed; Tomography, X-Ray Computed},
	month = {Jun},
	number = {25},
	pages = {2500-7},
	pmid = {12815135},
	pst = {ppublish},
	title = {Staging of non-small-cell lung cancer with integrated positron-emission tomography and computed tomography},
	volume = {348},
	year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1056/NEJMoa022136}}

@article{Hansch:2012aa,
	abstract = {OBJECTIVE: To assess the feasibility of time-resolved parallel three-dimensional magnetic resonance imaging (MRI) for quantitative analysis of pulmonary perfusion using a blood pool contrast agent.
METHODS: Quantitative perfusion analysis was performed using novel software to assess pulmonary blood flow (PBF), pulmonary blood volume (PBV) and mean transit time (MTT) in a quantitative manner.
RESULTS: The evaluation of lung perfusion in the normal subjects showed an increase of PBF, PBV ventrally to dorsally (gravitational direction), and the highest values at the upper lobe, with a decrease to the middle and lower lobe (isogravitational direction). MTT showed no relevant changes in either the gravitational or isogravitational directions. In comparison with normally perfused lung areas (in diseased patients), the pulmonary embolism (PE) regions showed a significantly lower mean PBF (20 $\pm$ 0.6 ml/100 ml/min, normal region 94 $\pm$ 1 ml/100 ml/min; P < 0.001), mean PBV (2 $\pm$ 0.1 ml/100 ml, normal region 9.8 $\pm$ 0.1 ml/100 ml; P < 0.001) and mean MTT (3.8 $\pm$ 0.1 s; normal region 6.3 $\pm$ 0.1; P < 0.001).
CONCLUSION: Our results demonstrate the feasibility of using time-resolved dynamic contrast-enhanced MRI to determine normal range and regional variation of pulmonary perfusion and perfusion deficits in patients with PE.
KEY POINTS: * Recently introduced blood pool contrast agents improve MR evaluation of lung perfusion * Regional differences in lung perfusion indicating a gravitational and isogravitational dependency. * Focal areas of significantly decreased perfusion are detectable in pulmonary embolism.},
	author = {Hansch, Andreas and Kohlmann, Peter and Hinneburg, Uta and Boettcher, Joachim and Malich, Ansgar and Wolf, Gunter and Laue, Hendrik and Pfeil, Alexander},
	date-added = {2016-06-15 03:58:55 +0000},
	date-modified = {2016-06-15 03:58:55 +0000},
	doi = {10.1007/s00330-012-2428-z},
	journal = {Eur Radiol},
	journal-full = {European radiology},
	mesh = {Adult; Aged; Contrast Media; Feasibility Studies; Female; Humans; Lung; Magnetic Resonance Imaging; Male; Middle Aged; Perfusion; Pulmonary Circulation; Pulmonary Embolism; Software; Time Factors; Venous Thrombosis},
	month = {Aug},
	number = {8},
	pages = {1748-56},
	pmid = {22466513},
	pst = {ppublish},
	title = {Quantitative evaluation of MR perfusion imaging using blood pool contrast agent in subjects without pulmonary diseases and in patients with pulmonary embolism},
	volume = {22},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00330-012-2428-z}}

@article{Ohno:2004aa,
	abstract = {PURPOSE: To assess regional differences in quantitative pulmonary perfusion parameters, i.e., pulmonary blood flow (PBF), mean transit time (MTT), and pulmonary blood volume (PBV) in the entire lung on a pixel-by-pixel basis in normal volunteers and pulmonary hypertension patients.
MATERIALS AND METHODS: Three-dimensional ultrafast dynamic contrast-enhanced MR imaging was performed in 15 normal volunteers and 25 patients with pulmonary hypertension. From the signal intensity-time course curves, PBF, MTT and PBV maps were generated using deconvolution analysis, indicator dilution theories, and the central volume principle, on a pixel-by-pixel basis. From pulmonary perfusion parameter maps of normal volunteers and pulmonary hypertension patients, regional PBF, MTT, and PBV were statistically evaluated.
RESULTS: Regional PBF, MTT, and PBV showed significant differences in the gravitational and isogravitational directions (P < 0.05). The quantitative pulmonary perfusion parameter maps demonstrated significant differences between normal volunteers and pulmonary hypertension patients (P < 0.05).
CONCLUSION: Three-dimensional ultrafast dynamic contrast-enhanced MR imaging is feasible for the assessment of regional quantitative pulmonary perfusion parameters in the entire lung on a pixel-by-pixel basis in normal volunteers and pulmonary hypertension patients.},
	author = {Ohno, Yoshiharu and Hatabu, Hiroto and Murase, Kenya and Higashino, Takanori and Kawamitsu, Hideaki and Watanabe, Hirokazu and Takenaka, Daisuke and Fujii, Masahiko and Sugimura, Kazuro},
	date-added = {2016-06-15 03:57:36 +0000},
	date-modified = {2016-06-15 03:57:36 +0000},
	doi = {10.1002/jmri.20137},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Adult; Aged; Analysis of Variance; Chemotherapy, Cancer, Regional Perfusion; Contrast Media; Female; Gadolinium DTPA; Humans; Hypertension, Pulmonary; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Lung; Magnetic Resonance Imaging; Male; Middle Aged; Pulmonary Circulation},
	month = {Sep},
	number = {3},
	pages = {353-65},
	pmid = {15332240},
	pst = {ppublish},
	title = {Quantitative assessment of regional pulmonary perfusion in the entire lung using three-dimensional ultrafast dynamic contrast-enhanced magnetic resonance imaging: Preliminary experience in 40 subjects},
	volume = {20},
	year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.20137}}

@article{Ostergaard:1996aa,
	abstract = {The authors review the theoretical basis of determination of cerebral blood flow (CBF) using dynamic measurements of nondiffusible contrast agents, and demonstrate how parametric and nonparametric deconvolution techniques can be modified for the special requirements of CBF determination using dynamic MRI. Using Monte Carlo modeling, the use of simple, analytical residue models is shown to introduce large errors in flow estimates when actual, underlying vascular characteristics are not sufficiently described by the chosen function. The determination of the shape of the residue function on a regional basis is shown to be possible only at high signal-to-noise ratio. Comparison of several nonparametric deconvolution techniques showed that a nonparametric deconvolution technique (singular value decomposition) allows estimation of flow relatively independent of underlying vascular structure and volume even at low signal-to-noise ratio associated with pixel-by-pixel deconvolution.},
	author = {Ostergaard, L and Weisskoff, R M and Chesler, D A and Gyldensted, C and Rosen, B R},
	date-added = {2016-06-15 03:54:34 +0000},
	date-modified = {2016-06-15 03:54:34 +0000},
	journal = {Magn Reson Med},
	journal-full = {Magnetic resonance in medicine},
	mesh = {Cerebrovascular Circulation; Magnetic Resonance Imaging; Mathematics; Models, Theoretical; Monte Carlo Method; Statistics as Topic},
	month = {Nov},
	number = {5},
	pages = {715-25},
	pmid = {8916022},
	pst = {ppublish},
	title = {High resolution measurement of cerebral blood flow using intravascular tracer bolus passages. Part I: Mathematical approach and statistical analysis},
	volume = {36},
	year = {1996}}

@inproceedings{Staring:2012aa,
	author = {Marius Staring and Changyan Xiao and Denis P. Shamonin and Berend C. Stoel},
	booktitle = {{VES}sel {SE}gmentation in the {L}ung},
	date-added = {2016-06-14 23:12:41 +0000},
	date-modified = {2016-06-14 23:15:22 +0000},
	editor = {Rina Dewi Rudyanto and Eva van Rikxoort and Sjoerd Kerkstra and Bram van Ginneken},
	organization = {IEEE International Symposium on Biomedical Imaging (ISBI 2012)},
	title = {Pulmonary Vessel Segmentation using Vessel Enhancement Filters},
	year = {2012}}

@inproceedings{Lee:2009aa,
	author = {J. Lee and A. P. Reeves},
	booktitle = {Proc. of Second International Workshop on Pulmonary Image Analysis},
	date-added = {2015-09-28 14:58:06 +0000},
	date-modified = {2015-09-28 14:58:58 +0000},
	title = {Segmentation of the airway tree from chest {CT} using local volume of interest},
	year = {2009}}

@inproceedings{Feuerstein:2009aa,
	author = {Marco Feuerstein and Takayuki Kitasaka and Kensaku Mori},
	booktitle = {Proc. of Second International Workshop on Pulmonary Image Analysis},
	date-added = {2015-09-28 14:57:07 +0000},
	date-modified = {2015-09-28 14:57:59 +0000},
	title = {Adaptive branch tracing and image sharpening for airway tree extraction in 3-D chest CT},
	year = {2009}}

@article{Murphy:2009aa,
	abstract = {A scheme for the automatic detection of nodules in thoracic computed tomography scans is presented and extensively evaluated. The algorithm uses the local image features of shape index and curvedness in order to detect candidate structures in the lung volume and applies two successive k-nearest-neighbour classifiers in the reduction of false-positives. The nodule detection system is trained and tested on three databases extracted from a large-scale experimental screening study. The databases are constructed in order to evaluate the algorithm on both randomly chosen screening data as well as data containing higher proportions of nodules requiring follow-up. The system results are extensively evaluated including performance measurements on specific nodule types and sizes within the databases and on lesions which later proved to be malignant. In a random selection of 813 scans from the screening study a sensitivity of 80% with an average 4.2 false-positives per scan is achieved. The detection results presented are a realistic measure of a CAD system performance in a low-dose screening study which includes a diverse array of nodules of many varying sizes, types and textures.},
	author = {Murphy, K and van Ginneken, B and Schilham, A M R and de Hoop, B J and Gietema, H A and Prokop, M},
	date-added = {2015-09-28 14:40:48 +0000},
	date-modified = {2015-09-28 14:40:48 +0000},
	doi = {10.1016/j.media.2009.07.001},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Algorithms; Artificial Intelligence; Humans; Lung Neoplasms; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity; Solitary Pulmonary Nodule; Tomography, X-Ray Computed},
	month = {Oct},
	number = {5},
	pages = {757-70},
	pmid = {19646913},
	pst = {ppublish},
	title = {A large-scale evaluation of automatic pulmonary nodule detection in chest CT using local image features and k-nearest-neighbour classification},
	volume = {13},
	year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2009.07.001}}

@article{Kirby:2014aa,
	abstract = {PURPOSE: To evaluate the predictive value of imaging and clinical and physiological measurements of chronic obstructive pulmonary disease ( COPD chronic obstructive pulmonary disease ) in patients monitored for more than 5 years for pulmonary exacerbations that required hospitalization.
MATERIALS AND METHODS: Exacerbations requiring hospitalization were monitored over 5 years in 91 subjects who provided written informed consent. Study was local research ethics board and Health Canada approved and HIPAA compliant. Subjects with COPD chronic obstructive pulmonary disease underwent spirometry, plethysmography, diffusing capacity of carbon monoxide, St George's Respiratory Questionnaire, 6-minute walk test, and imaging. Computed tomographic (CT) wall area and relative area with attenuation values less than -950 HU ( RA950 relative area with attenuation values less than -950 HU ), helium 3 ((3)He) magnetic resonance (MR) imaging ventilation defect percentage ( VDP ventilation defect percentage ), and apparent diffusion coefficient were generated. Zero-inflated Poisson model was used to compare number of hospitalizations with lung function and imaging measurements.
RESULTS: Twenty-four subjects were hospitalized 58 times and had significantly worse forced expiratory volume in 1 second ( FEV1 forced expiratory volume in 1 second ) (P < .0001), CT RA950 relative area with attenuation values less than -950 HU (P = .02), and (3)He VDP ventilation defect percentage (P < .0001) than values in 67 subjects who were not hospitalized. In mild to moderate COPD chronic obstructive pulmonary disease , nine hospitalized subjects had significantly worse FEV1 forced expiratory volume in 1 second (P = .02) and (3)He VDP ventilation defect percentage (P = .02) than values in 52 subjects who were not hospitalized. (3)He VDP ventilation defect percentage was quantitatively related to CT airway morphology (r = 0.26, P = .01) and quantitatively (r = 0.61, P < .0001) and spatially related to emphysema; this spatial relationship was significantly greater for hospitalized patients with COPD chronic obstructive pulmonary disease than unhospitalized patients (P = .0006). For all subjects, number of prior hospitalizations (P < .0001), 6-minute walk test distance (P < .0001), CT RA950 relative area with attenuation values less than -950 HU (P = .03), and (3)He VDP ventilation defect percentage (P = .002) were significantly related to number of hospitalizations. For 61 subjects with mild to moderate COPD chronic obstructive pulmonary disease , only (3)He VDP ventilation defect percentage was significantly associated with COPD chronic obstructive pulmonary disease exacerbations (P = .01).
CONCLUSION: (3)He MR imaging VDP ventilation defect percentage represents a mixed airways-emphysema phenotype and helps identify subjects with mild to moderate COPD chronic obstructive pulmonary disease who are at risk for exacerbation that requires hospitalization.},
	author = {Kirby, Miranda and Pike, Damien and Coxson, Harvey O and McCormack, David G and Parraga, Grace},
	date-added = {2015-09-24 01:51:42 +0000},
	date-modified = {2015-09-24 01:51:42 +0000},
	doi = {10.1148/radiol.14140161},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Aged; Aged, 80 and over; Albuterol; Bronchodilator Agents; Diffusion Magnetic Resonance Imaging; Female; Helium; Humans; Isotopes; Male; Middle Aged; Plethysmography; Pulmonary Disease, Chronic Obstructive; Respiratory Function Tests; Spirometry; Tomography, X-Ray Computed},
	month = {Dec},
	number = {3},
	pages = {887-96},
	pmid = {24960283},
	pst = {ppublish},
	title = {Hyperpolarized (3)He ventilation defects used to predict pulmonary exacerbations in mild to moderate chronic obstructive pulmonary disease},
	volume = {273},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiol.14140161}}

@article{Altes:2012aa,
	author = {Talissa Altes and Mac Johnson and John Mugler III and G W. Miller and Lucia Flors and Jaime Mata and Carlos Salinas and Nick Tustison and Po-Shun Lee and Tao Song and Karl Yen Deborah Froh and Martyn Botfield},
	date-added = {2015-09-24 01:42:33 +0000},
	date-modified = {2015-09-24 01:44:36 +0000},
	journal = {PATHOGENESIS AND CLINICAL ISSUES IN CYSTIC FIBROSIS},
	month = {May},
	pages = {A2814-A2814},
	title = {The Effect Of {Ivacaftor}, An Investigational {CFTR} Potentiator, On Hyperpolarized Noble Gas Magnetic Resonance Imaging In Subjects With Cystic Fibrosis Who Have The {G551D-CFTR} Mutation},
	volume = {B35},
	year = {2012}}

@webpage{SnapWebsite,
	date-added = {2015-09-21 20:04:14 +0000},
	date-modified = {2015-09-21 20:05:20 +0000},
	url = {http://www.itksnap.org},
	Bdsk-Url-1 = {http://www.itksnap.org}}

@url{ANTsWebsite,
	date-added = {2015-09-21 20:02:59 +0000},
	date-modified = {2020-11-01 07:37:19 -0800},
	title = {http://picsl.upenn.edu/software/ants/},
	url = {http://picsl.upenn.edu/software/ants/},
	Bdsk-Url-1 = {http://picsl.upenn.edu/software/ants/}}

@article{Zhu:1996aa,
	author = {S.C. Zhu and A. Yuille},
	date-added = {2015-09-21 19:47:40 +0000},
	date-modified = {2015-09-21 19:48:50 +0000},
	journal = {IEEE Trans Pattern Anal Mach Intell},
	number = {9},
	pages = {884--900},
	title = {Region Competition: Unifying Snakes, Region Growing, and Bayes/MDL for Multiband Image Segmentation},
	volume = {18},
	year = {1996}}

@article{Caselles:1997aa,
	author = {V. Caselles and R. Kimmel and G. Sapiro},
	date-added = {2015-09-21 19:45:16 +0000},
	date-modified = {2015-09-21 19:47:05 +0000},
	journal = {Int J Comput Vision},
	pages = {61--79},
	title = {Geodesic active contours},
	volume = {22},
	year = {1997}}

@article{Zosso:2011aa,
	abstract = {In this paper we present a novel geometric framework called geodesic active fields for general image registration. In image registration, one looks for the underlying deformation field that best maps one image onto another. This is a classic ill-posed inverse problem, which is usually solved by adding a regularization term. Here, we propose a multiplicative coupling between the registration term and the regularization term, which turns out to be equivalent to embed the deformation field in a weighted minimal surface problem. Then, the deformation field is driven by a minimization flow toward a harmonic map corresponding to the solution of the registration problem. This proposed approach for registration shares close similarities with the well-known geodesic active contours model in image segmentation, where the segmentation term (the edge detector function) is coupled with the regularization term (the length functional) via multiplication as well. As a matter of fact, our proposed geometric model is actually the exact mathematical generalization to vector fields of the weighted length problem for curves and surfaces introduced by Caselles-Kimmel-Sapiro. The energy of the deformation field is measured with the Polyakov energy weighted by a suitable image distance, borrowed from standard registration models. We investigate three different weighting functions, the squared error and the approximated absolute error for monomodal images, and the local joint entropy for multimodal images. As compared to specialized state-of-the-art methods tailored for specific applications, our geometric framework involves important contributions. Firstly, our general formulation for registration works on any parametrizable, smooth and differentiable surface, including nonflat and multiscale images. In the latter case, multiscale images are registered at all scales simultaneously, and the relations between space and scale are intrinsically being accounted for. Second, this method is, to the best of our knowledge, the first reparametrization invariant registration method introduced in the literature. Thirdly, the multiplicative coupling between the registration term, i.e. local image discrepancy, and the regularization term naturally results in a data-dependent tuning of the regularization strength. Finally, by choosing the metric on the deformation field one can freely interpolate between classic Gaussian and more interesting anisotropic, TV-like regularization.},
	author = {Zosso, Dominique and Bresson, Xavier and Thiran, Jean-Philippe},
	date-added = {2015-09-21 19:42:28 +0000},
	date-modified = {2015-09-21 19:42:28 +0000},
	doi = {10.1109/TIP.2010.2093904},
	journal = {IEEE Trans Image Process},
	journal-full = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	mesh = {Algorithms; Image Enhancement; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Normal Distribution},
	month = {May},
	number = {5},
	pages = {1300-12},
	pmid = {21095870},
	pst = {ppublish},
	title = {Geodesic active fields--a geometric framework for image registration},
	volume = {20},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIP.2010.2093904}}

@article{Yushkevich:2006aa,
	abstract = {Active contour segmentation and its robust implementation using level set methods are well-established theoretical approaches that have been studied thoroughly in the image analysis literature. Despite the existence of these powerful segmentation methods, the needs of clinical research continue to be fulfilled, to a large extent, using slice-by-slice manual tracing. To bridge the gap between methodological advances and clinical routine, we developed an open source application called ITK-SNAP, which is intended to make level set segmentation easily accessible to a wide range of users, including those with little or no mathematical expertise. This paper describes the methods and software engineering philosophy behind this new tool and provides the results of validation experiments performed in the context of an ongoing child autism neuroimaging study. The validation establishes SNAP intrarater and interrater reliability and overlap error statistics for the caudate nucleus and finds that SNAP is a highly reliable and efficient alternative to manual tracing. Analogous results for lateral ventricle segmentation are provided.},
	author = {Yushkevich, Paul A and Piven, Joseph and Hazlett, Heather Cody and Smith, Rachel Gimpel and Ho, Sean and Gee, James C and Gerig, Guido},
	date-added = {2015-09-21 19:41:58 +0000},
	date-modified = {2015-09-21 19:41:58 +0000},
	doi = {10.1016/j.neuroimage.2006.01.015},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Brain; Caudate Nucleus; Dominance, Cerebral; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Mathematical Computing; Software; Software Validation; User-Computer Interface},
	month = {Jul},
	number = {3},
	pages = {1116-28},
	pmid = {16545965},
	pst = {ppublish},
	title = {User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability},
	volume = {31},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2006.01.015}}

@article{Boyes:2008aa,
	abstract = {Measures of structural brain change based on longitudinal MR imaging are increasingly important but can be degraded by intensity non-uniformity. This non-uniformity can be more pronounced at higher field strengths, or when using multichannel receiver coils. We assessed the ability of the non-parametric non-uniform intensity normalization (N3) technique to correct non-uniformity in 72 volumetric brain MR scans from the preparatory phase of the Alzheimer's Disease Neuroimaging Initiative (ADNI). Normal elderly subjects (n=18) were scanned on different 3-T scanners with a multichannel phased array receiver coil at baseline, using magnetization prepared rapid gradient echo (MP-RAGE) and spoiled gradient echo (SPGR) pulse sequences, and again 2 weeks later. When applying N3, we used five brain masks of varying accuracy and four spline smoothing distances (d=50, 100, 150 and 200 mm) to ascertain which combination of parameters optimally reduces the non-uniformity. We used the normalized white matter intensity variance (standard deviation/mean) to ascertain quantitatively the correction for a single scan; we used the variance of the normalized difference image to assess quantitatively the consistency of the correction over time from registered scan pairs. Our results showed statistically significant (p<0.01) improvement in uniformity for individual scans and reduction in the normalized difference image variance when using masks that identified distinct brain tissue classes, and when using smaller spline smoothing distances (e.g., 50-100 mm) for both MP-RAGE and SPGR pulse sequences. These optimized settings may assist future large-scale studies where 3-T scanners and phased array receiver coils are used, such as ADNI, so that intensity non-uniformity does not influence the power of MR imaging to detect disease progression and the factors that influence it.},
	author = {Boyes, Richard G and Gunter, Jeff L and Frost, Chris and Janke, Andrew L and Yeatman, Thomas and Hill, Derek L G and Bernstein, Matt A and Thompson, Paul M and Weiner, Michael W and Schuff, Norbert and Alexander, Gene E and Killiany, Ronald J and DeCarli, Charles and Jack, Clifford R and Fox, Nick C and {ADNI Study}},
	date-added = {2015-09-19 04:16:17 +0000},
	date-modified = {2015-09-19 04:16:17 +0000},
	doi = {10.1016/j.neuroimage.2007.10.026},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Aged; Algorithms; Alzheimer Disease; Brain; Calibration; Cognition Disorders; Data Interpretation, Statistical; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Reproducibility of Results},
	month = {Feb},
	number = {4},
	pages = {1752-62},
	pmc = {PMC2562663},
	pmid = {18063391},
	pst = {ppublish},
	title = {Intensity non-uniformity correction using N3 on 3-T scanners with multichannel phased array coils},
	volume = {39},
	year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2007.10.026}}

@article{Ashburner:2005aa,
	abstract = {A probabilistic framework is presented that enables image registration, tissue classification, and bias correction to be combined within the same generative model. A derivation of a log-likelihood objective function for the unified model is provided. The model is based on a mixture of Gaussians and is extended to incorporate a smooth intensity variation and nonlinear registration with tissue probability maps. A strategy for optimising the model parameters is described, along with the requisite partial derivatives of the objective function.},
	author = {Ashburner, John and Friston, Karl J},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1016/j.neuroimage.2005.02.018},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Brain Mapping; Data Interpretation, Statistical; Fuzzy Logic; Image Processing, Computer-Assisted; Likelihood Functions; Magnetic Resonance Imaging; Models, Neurological; Models, Statistical; Nonlinear Dynamics; Normal Distribution; Probability Theory},
	month = {Jul},
	number = {3},
	pages = {839-51},
	pmid = {15955494},
	pst = {ppublish},
	title = {Unified segmentation},
	volume = {26},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2005.02.018}}

@article{Awate:2006aa,
	abstract = {Image restoration is an important and widely studied problem in computer vision and image processing. Various image filtering strategies have been effective, but invariably make strong assumptions about the properties of the signal and/or degradation. Hence, these methods lack the generality to be easily applied to new applications or diverse image collections. This paper describes a novel unsupervised, information-theoretic, adaptive filter (UINTA) that improves the predictability of pixel intensities from their neighborhoods by decreasing their joint entropy. In this way, UINTA automatically discovers the statistical properties of the signal and can thereby restore a wide spectrum of images. The paper describes the formulation to minimize the joint entropy measure and presents several important practical considerations in estimating neighborhood statistics. It presents a series of results on both real and synthetic data along with comparisons with current state-of-the-art techniques, including novel applications to medical image processing.},
	author = {Awate, Suyash P and Whitaker, Ross T},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/TPAMI.2006.64},
	journal = {IEEE Trans Pattern Anal Mach Intell},
	journal-full = {IEEE transactions on pattern analysis and machine intelligence},
	mesh = {Algorithms; Artificial Intelligence; Computer Graphics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted},
	month = {Mar},
	number = {3},
	pages = {364-76},
	pmid = {16526423},
	pst = {ppublish},
	title = {Unsupervised, information-theoretic, adaptive image filtering for image restoration},
	volume = {28},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TPAMI.2006.64}}

@article{Buades2008,
	author = {Antoni Buades and Bartomeu Coll and Jean-Michel Morel},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {International Journal of Computer Vision},
	number = {2},
	pages = {123-139},
	title = {Nonlocal Image and Movie Denoising},
	volume = {76},
	year = {2008}}

@article{Cline:1990aa,
	abstract = {We describe a three-dimensional (3D) segmentation method that comprises (a) user interactive identification of tissue classes; (b) calculation of a probability distribution for each tissue; (c) creation of a feature map of the most probable tissues; (d) 3D segmentation of the magnetic resonance (MR) data; (e) smoothing of the segmented data; (f) extraction of surfaces of interest with connectivity; (g) generation of surfaces; and (h) rendering of multiple surfaces to plan surgery. Patients with normal head anatomy and with abnormalities such as multiple sclerosis lesions and brain tumors were scanned with a 1.5 T MR system using a two echo contiguous (interleaved), multislice pulse sequence that provides both proton density and T2-weighted contrast. After the user identified the tissues, the 3D data were automatically segmented into background, facial tissue, brain matter, CSF, and lesions. Surfaces of the face, brain, lateral ventricles, tumors, and multiple sclerosis lesions are displayed using color coding and gradient shading. Color improves the visualization of segmented tissues, while gradient shading enhances the perception of depth. Manipulation of the 3D model on a workstation aids surgical planning. Sulci and gyri stand out, thus aiding functional mapping of the brain surface.},
	author = {Cline, H E and Lorensen, W E and Kikinis, R and Jolesz, F},
	date = {1990 Nov-Dec},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {J Comput Assist Tomogr},
	journal-full = {Journal of computer assisted tomography},
	mesh = {Algorithms; Brain; Brain Neoplasms; Computer Graphics; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Multiple Sclerosis},
	number = {6},
	pages = {1037-45},
	pmid = {2229557},
	pst = {ppublish},
	title = {Three-dimensional segmentation of MR images of the head using probability and connectivity},
	volume = {14},
	year = {1990}}

@article{Dempster:1977aa,
	author = {A. Dempster and N. Laird and D. Rubin},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {Journal of the Royal Statistical Society},
	pages = {1--38},
	title = {Maximum likelihood estimation from incomplete data using the {EM} algorithms},
	volume = {39},
	year = {1977}}

@article{Geman:1984aa,
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	author = {Geman, S and Geman, D},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {IEEE Trans Pattern Anal Mach Intell},
	journal-full = {IEEE transactions on pattern analysis and machine intelligence},
	month = {Jun},
	number = {6},
	pages = {721-41},
	pmid = {22499653},
	pst = {ppublish},
	title = {Stochastic relaxation, gibbs distributions, and the bayesian restoration of images},
	volume = {6},
	year = {1984}}

@article{Gohagan:1987aa,
	abstract = {Preliminary investigations were conducted into the potential of magnetic resonance (MR) images for tissue classification of the breast on the basis of relative signal intensity. Multispectral techniques originally developed by the National Aeronautics and Space Administration for satellite image analysis were used in sequence selection, image data correction, image standardization, and image interpretation. Numerous sequence combinations with varying repetition times (TR) and echo times (TE) were considered, and a triplet was selected consisting of long TR/long TE, short TR/short TE, and an opposed phase sequence with intermediate TR and TE. Correction to remove system-imposed intensity inhomogeneities was required for all images. Image standardization based on fat and pectoral muscle signals was necessary for intercase comparisons. Multispectral images obtained based on this analysis suggest the feasibility of intensity-based image classification.},
	author = {Gohagan, J K and Spitznagel, E L and Murphy, W A and Vannier, M W and Dixon, W T and Gersell, D J and Rossnick, S L and Totty, W G and Destouet, J M and Rickman, D L},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1148/radiology.163.3.3575718},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Breast Neoplasms; Female; Humans; Magnetic Resonance Spectroscopy},
	month = {Jun},
	number = {3},
	pages = {703-7},
	pmid = {3575718},
	pst = {ppublish},
	title = {Multispectral analysis of MR images of the breast},
	volume = {163},
	year = {1987},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxArLi4vLi4vLi4vLi4vLi4vRG93bmxvYWRzL2Ficy0xOTA0LTAyNDM2LmJpYk8RAV4AAAAAAV4AAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xJhYnMtMTkwNC0wMjQzNi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAUAAgAACiBjdQAAAAAAAAAAAAAAAAAJRG93bmxvYWRzAAACAC4vOlVzZXJzOm50dXN0aXNvbjpEb3dubG9hZHM6YWJzLTE5MDQtMDI0MzYuYmliAA4AJgASAGEAYgBzAC0AMQA5ADAANAAtADAAMgA0ADMANgAuAGIAaQBiAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL250dXN0aXNvbi9Eb3dubG9hZHMvYWJzLTE5MDQtMDI0MzYuYmliABMAAS8AABUAAgAQ//8AAAAIAA0AGgAkAFIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABtA==},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiology.163.3.3575718}}

@article{Gubern-Merida:2015aa,
	abstract = {Breast density measurement is an important aspect in breast cancer diagnosis as dense tissue has been related to the risk of breast cancer development. The purpose of this study is to develop a method to automatically compute breast density in breast MRI. The framework is a combination of image processing techniques to segment breast and fibroglandular tissue. Intra- and interpatient signal intensity variability is initially corrected. The breast is segmented by automatically detecting body-breast and air-breast surfaces. Subsequently, fibroglandular tissue is segmented in the breast area using expectation-maximization. A dataset of 50 cases with manual segmentations was used for evaluation. Dice similarity coefficient (DSC), total overlap, false negative fraction (FNF), and false positive fraction (FPF) are used to report similarity between automatic and manual segmentations. For breast segmentation, the proposed approach obtained DSC, total overlap, FNF, and FPF values of 0.94, 0.96, 0.04, and 0.07, respectively. For fibroglandular tissue segmentation, we obtained DSC, total overlap, FNF, and FPF values of 0.80, 0.85, 0.15, and 0.22, respectively. The method is relevant for researchers investigating breast density as a risk factor for breast cancer and all the described steps can be also applied in computer aided diagnosis systems.},
	author = {Gubern-M{\'e}rida, Albert and Kallenberg, Michiel and Mann, Ritse M and Mart{\'\i}, Robert and Karssemeijer, Nico},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/JBHI.2014.2311163},
	journal = {IEEE J Biomed Health Inform},
	journal-full = {IEEE journal of biomedical and health informatics},
	month = {Jan},
	number = {1},
	pages = {349-57},
	pmid = {25561456},
	pst = {ppublish},
	title = {Breast segmentation and density estimation in breast MRI: a fully automatic framework},
	volume = {19},
	year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/JBHI.2014.2311163}}

@article{Held:1997aa,
	abstract = {We describe a fully-automatic three-dimensional (3-D)-segmentation technique for brain magnetic resonance (MR) images. By means of Markov random fields (MRF's) the segmentation algorithm captures three features that are of special importance for MR images, i.e., nonparametric distributions of tissue intensities, neighborhood correlations, and signal inhomogeneities. Detailed simulations and real MR images demonstrate the performance of the segmentation algorithm. In particular, the impact of noise, inhomogeneity, smoothing, and structure thickness are analyzed quantitatively. Even single-echo MR images are well classified into gray matter, white matter, cerebrospinal fluid, scalp-bone, and background. A simulated annealing and an iterated conditional modes implementation are presented.},
	author = {Held, K and Rota Kops, E and Krause, B J and Wells, 3rd, W M and Kikinis, R and M{\"u}ller-G{\"a}rtner, H W},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/42.650883},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging},
	month = {Dec},
	number = {6},
	pages = {878-86},
	pmid = {9533587},
	pst = {ppublish},
	title = {Markov random field segmentation of brain MR images},
	volume = {16},
	year = {1997},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.650883}}

@article{Kikinis:1992aa,
	abstract = {A computerized system for processing spin-echo magnetic resonance (MR) imaging data was implemented to estimate whole brain (gray and white matter) and cerebrospinal fluid volumes and to display three-dimensional surface reconstructions of specified tissue classes. The techniques were evaluated by assessing the radiometric variability of MR volume data and by comparing automated and manual procedures for measuring tissue volumes. Results showed (a) the homogeneity of the MR data and (b) that automated techniques were consistently superior to manual techniques. Both techniques, however, were affected by the complexity of the structure, with simpler structures (eg, the intracranial cavity) showing less variability and better spatial correlation of segmentation results between raters. Moreover, the automated techniques were completed for whole brain in a fraction of the time required to complete the equivalent segmentation manually. Additional evaluations included interrater reliability and an evaluation that included longitudinal measurement, in which one subject was imaged sequentially 24 times, with reliability computed from data collected by three raters over 1 year. Results showed good reliability for the automated segmentation procedures.},
	author = {Kikinis, R and Shenton, M E and Gerig, G and Martin, J and Anderson, M and Metcalf, D and Guttmann, C R and McCarley, R W and Lorensen, W and Cline, H},
	date = {1992 Nov-Dec},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Body Fluid Compartments; Brain; Cerebrospinal Fluid; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Reproducibility of Results},
	number = {6},
	pages = {619-29},
	pmid = {1446105},
	pst = {ppublish},
	title = {Routine quantitative analysis of brain and cerebrospinal fluid spaces with MR imaging},
	volume = {2},
	year = {1992}}

@article{Ribes:2014aa,
	abstract = {An algorithm dedicated to automatic segmentation of breast magnetic resonance images is presented in this paper. Our approach is based on a pipeline that includes a denoising step and statistical segmentation. The noise removal preprocessing relies on an anisotropic diffusion scheme, whereas the statistical segmentation is conducted through a Markov random field model. The continuous updating of all parameters governing the diffusion process enables automatic denoising, and the partial volume effect is also addressed during the labeling step. To assess the relevance, the Jaccard similarity coefficient was computed. Experiments were conducted on synthetic data and breast magnetic resonance images extracted from a high-risk population. The relevance of the approach for the dataset is highlighted, and we demonstrate accuracy superior to that of traditional clustering algorithms. The results emphasize the benefits of both denoising guided by input data and the inclusion of spatial dependency through a Markov random field. For example, the Jaccard coefficient for the clinical data was increased by 114%, 109%, and 140% with respect to a K-means algorithm and, respectively, for the adipose, glandular and muscle and skin components. Moreover, the agreement between the manual segmentations provided by an experienced radiologist and the automatic segmentations performed with this algorithm was good, with Jaccard coefficients equal to 0.769, 0.756, and 0.694 for the above-mentioned classes.},
	author = {Ribes, S and Didierlaurent, D and Decoster, N and Gonneau, E and Risser, L and Feillel, V and Caselles, O},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/TMI.2014.2329019},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Breast; Databases, Factual; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Markov Chains; Models, Statistical},
	month = {Oct},
	number = {10},
	pages = {1986-96},
	pmid = {24919158},
	pst = {ppublish},
	title = {Automatic segmentation of breast MR images through a Markov random field statistical model},
	volume = {33},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2014.2329019}}

@article{Shih:1987aa,
	abstract = {Missing data complicate the analysis of paired categorical data. This paper considers with the use of the EM algorithm, the maximum likelihood estimation and likelihood ratio test for incomplete square tables with missing data. An example involving the susceptibility of micro-organisms to antimicrobial drugs illustrates the procedure.},
	author = {Shih, W J},
	date = {1987 Jan-Feb},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {Stat Med},
	journal-full = {Statistics in medicine},
	mesh = {Algorithms; Biometry; Clinical Trials as Topic; Humans},
	number = {1},
	pages = {91-7},
	pmid = {3554443},
	pst = {ppublish},
	title = {Maximum likelihood estimation and likelihood ratio test for square tables with missing data},
	volume = {6},
	year = {1987}}

@article{Tustison2005,
	author = {N. J. Tustison and J. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2006.12.20},
	title = {${N}$-D ${C}^k$ {B}-Spline Scattered Data Approximation},
	url = {http://hdl.handle.net/1926/140},
	year = {2005},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/140}}

@article{Tustison2008b,
	author = {N. J. Tustison and J. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2009.06.16},
	title = {Run-Length Matrices For Texture Analysis},
	url = {http://hdl.handle.net/1926/1374},
	year = {2008},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1374}}

@article{Tustison2008c,
	author = {N. J. Tustison and H. Zhang and P. Yushkevich and J. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2008.10.19},
	title = {Meeting {A}ndy {W}arhol Somewhere Over the Rainbow: {RGB} Colormapping and {ITK}},
	url = {http://hdl.handle.net/1926/1452},
	year = {2008},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1452}}

@article{Tustison2008d,
	author = {N. J. Tustison and P. A. Yushkevich and J. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2009.06.16},
	title = {Live-Wire-ing the {I}nsight {T}oolkit with {I}ntelligent {S}cissors},
	url = {http://hdl.handle.net/1926/1372},
	year = {2008},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1372}}

@article{Tustison2009e,
	author = {N. J. Tustison and J. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2010.01.15},
	title = {{N4ITK}: Nick's {N3} {ITK} Implementation for {MRI} Bias Field Correction},
	url = {http://hdl.handle.net/10380/3053},
	year = {2009},
	Bdsk-Url-1 = {http://hdl.handle.net/10380/3053}}

@article{Tustison2009a,
	abstract = {Previous contributions to both the research and open source software
	communities detailed a generalization of a fast scalar field fitting
	technique for cubic B-splines based on the work originally proposed
	by Lee . One advantage of our proposed generalized B-spline fitting
	approach is its immediate application to a class of nonrigid registration
	techniques frequently employed in medical image analysis. Specifically,
	these registration techniques fall under the rubric of free-form
	deformation (FFD) approaches in which the object to be registered
	is embedded within a B-spline object. The deformation of the B-spline
	object describes the transformation of the image registration solution.
	Representative of this class of techniques, and often cited within
	the relevant community, is the formulation of Rueckert who employed
	cubic splines with normalized mutual information to study breast
	deformation. Similar techniques from various groups provided incremental
	novelty in the form of disparate explicit regularization terms, as
	well as the employment of various image metrics and tailored optimization
	methods. For several algorithms, the underlying gradient-based optimization
	retained the essential characteristics of Rueckert's original contribution.
	The contribution which we provide in this paper is two-fold: 1) the
	observation that the generic FFD framework is intrinsically susceptible
	to problematic energy topographies and 2) that the standard gradient
	used in FFD image registration can be modified to a well-understood
	preconditioned form which substantially improves performance. This
	is demonstrated with theoretical discussion and comparative evaluation
	experimentation.},
	author = {Nicholas J Tustison and Brian B Avants and James C Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/TIP.2008.2010072},
	journal = {IEEE Trans Image Process},
	month = {Mar},
	number = {3},
	pages = {624--635},
	pmid = {19171516},
	timestamp = {2009.03.03},
	title = {Directly manipulated free-form deformation image registration.},
	url = {http://dx.doi.org/10.1109/TIP.2008.2010072},
	volume = {18},
	year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIP.2008.2010072}}

@article{Tustison2010a,
	__markedentry = {[]},
	abstract = {PURPOSE: To propose and test the feasibility of a novel method for
	quantifying 3D regional pulmonary kinematics from hyperpolarized
	helium-3 tagged MRI in human subjects using a tailored image processing
	pipeline and a recently developed nonrigid registration framework.
	MATERIALS AND METHODS: Following image acquisition, inspiratory and
	expiratory tagged (3)He magnetic resonance (MR) images were preprocessed
	using various image filtering techniques to enhance the tag surfaces.
	Segmentation of the three orthogonal sets of tag planes in each lung
	produced distinct point-set representations of the tag surfaces.
	Using these labeled point-sets, deformation fields and corresponding
	strain maps were obtained via nonrigid point-set registration. Kinematic
	analysis was performed on three volunteers. RESULTS: Tag lines in
	inspiratory and expiratory images were coregistered producing a continuous
	3D correspondence mapping. Average displacement and directional strains
	were calculated in three subjects in the inferior, mid, and superior
	portions of the right and left lungs. As expected, the predominant
	direction of displacements with expiration is from inferior to superior.
	CONCLUSION: Kinematic quantitation of pulmonary motion using tagged
	(3)He MRI is feasible using the applied image preprocessing filtering
	techniques and nonrigid point-set registration. Potential benefits
	from regional pulmonary kinematic quantitation include the facilitation
	of diagnosis and local assessment of disease progression.},
	author = {Nicholas J Tustison and Suyash P Awate and Jing Cai and Talissa A Altes and G. Wilson Miller and Eduard E de Lange and John P Mugler and James C Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1002/jmri.22137},
	institution = {Penn Image Computing and Science Laboratory, University of Pennsylvania, Philadelphia, PA 19104, USA. tustison@picsl.upenn.edu},
	journal = {J Magn Reson Imaging},
	language = {eng},
	medline-pst = {ppublish},
	month = {May},
	number = {5},
	pages = {1236--1241},
	pmid = {20432362},
	timestamp = {2010.05.04},
	title = {Pulmonary kinematics from tagged hyperpolarized helium-3 MRI.},
	url = {http://dx.doi.org/10.1002/jmri.22137},
	volume = {31},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.22137}}

@article{Tustison2006,
	author = {Nicholas J. Tustison and Marcelo Siqueira and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	month = {February},
	timestamp = {2006.06.19},
	title = {N-{D} Linear Time Exact Signed {E}uclidean Distance Transform},
	url = {http://hdl.handle.net/1926/171},
	year = {2006},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/171}}

@article{Tustison2007,
	author = {Nicholas J. Tustison and Brian A. Avants and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2007.08.10},
	title = {Gridding Graphic Graticules},
	url = {http://hdl.handle.net/1926/475},
	year = {2007},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/475}}

@article{Tustison2007a,
	author = {Nicholas J. Tustison and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2007.12.20},
	title = {Go-Go {G}abor Gadgetry},
	url = {http://hdl.handle.net/1926/500},
	year = {2007},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/500}}

@article{Tustison2007b,
	author = {Nicholas J. Tustison and James. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2008.01.02},
	title = {Image Kernel Convolution},
	url = {http://hdl.handle.net/1926/1323},
	year = {2007},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1323}}

@article{Tustison2007c,
	author = {Nicholas J. Tustison and Marcelo Siqueira and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2009.06.16},
	title = {Well-Composedness and the Topological Repairing of {2-D} and {3-D} Digital Images},
	url = {http://hdl.handle.net/1926/470},
	year = {2007},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/470}}

@article{Tustison2008,
	author = {Nicholas J. Tustison},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2010.01.15},
	title = {Graph Cuts, Caveat Utilitor, and {E}uler's {B}ridges of {K}onigsberg},
	url = {http://hdl.handle.net/1926/1503},
	year = {2008},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1503}}

@article{Tustison2008a,
	author = {Nicholas J. Tustison and Suyash P. Awate and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2008.10.19},
	title = {A Novel Information-Theoretic Point-Set Measure Based on the {J}ensen-{H}avrda-{C}harvat-{T}sallis Divergence},
	url = {http://hdl.handle.net/1926/1497},
	year = {2008},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1497}}

@article{Tustison2009,
	author = {Nicholas J. Tustison},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Source: Kitware's Software Developer Quarterly},
	timestamp = {2010.01.16},
	title = {{RGB} Colormapping and {ITK}},
	year = {2009}}

@article{Tustison2009b,
	author = {Nicholas J. Tustison and Suyash P. Awate and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2009.07.23},
	title = {Information-Theoretic Directly Manipulated Free-Form Deformation Labeled Point-Set Registration},
	url = {http://hdl.handle.net/1926/1524},
	year = {2009},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1524}}

@article{Tustison2009c,
	author = {Nicholas J. Tustison and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2010.01.15},
	title = {Introducing {D}ice, {J}accard, and Other Label Overlap Measures To {ITK}},
	url = {http://hdl.handle.net/10380/3141},
	year = {2009},
	Bdsk-Url-1 = {http://hdl.handle.net/10380/3141}}

@article{Tustison2009d,
	author = {Nicholas J. Tustison and James. C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Insight Journal},
	timestamp = {2009.06.16},
	title = {Stochastic Fractal Dimension Image},
	url = {http://hdl.handle.net/1926/1323},
	year = {2009},
	Bdsk-Url-1 = {http://hdl.handle.net/1926/1323}}

@article{Tustison2010,
	author = {Nicholas J. Tustison},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {The Source: Kitware's Software Developer Quarterly},
	timestamp = {2010.01.16},
	title = {N3 {ITK} Implementation for MRI Bias Field Correction},
	year = {2010}}

@article{Tustison2010b,
	author = {Nicholas J. Tustison and Talissa A and Altes and Gang Song and Eduard E. de Lange and John P. Mugler III and James C. Gee},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	journal = {Magn Reson Med},
	timestamp = {2010.05.04},
	title = {Feature analysis of hyperpolarized helium-3 pulmonary MRI: A study of asthmatics versus nonasthmatics},
	volume = {To appear},
	year = {2010}}

@article{Van-Leemput:1999aa,
	abstract = {We describe a fully automated method for model-based tissue classification of magnetic resonance (MR) images of the brain. The method interleaves classification with estimation of the model parameters, improving the classification at each iteration. The algorithm is able to segment single- and multispectral MR images, corrects for MR signal inhomogeneities, and incorporates contextual information by means of Markov random Fields (MRF's). A digital brain atlas containing prior expectations about the spatial location of tissue classes is used to initialize the algorithm. This makes the method fully automated and therefore it provides objective and reproducible segmentations. We have validated the technique on simulated as well as on real MR images of the brain.},
	author = {Van Leemput, K and Maes, F and Vandermeulen, D and Suetens, P},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/42.811270},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Bias (Epidemiology); Brain; Computer Simulation; Humans; Likelihood Functions; Magnetic Resonance Imaging; Markov Chains; Models, Neurological; Reproducibility of Results},
	month = {Oct},
	number = {10},
	pages = {897-908},
	pmid = {10628949},
	pst = {ppublish},
	title = {Automated model-based tissue classification of MR images of the brain},
	volume = {18},
	year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.811270}}

@article{Vannier:1985aa,
	abstract = {Magnetic resonance (MR) imaging systems produce spatial distribution estimates of proton density, relaxation time, and flow, in a two dimensional matrix form that is analogous to that of the image data obtained from multispectral imaging satellites. Advanced NASA satellite image processing offers sophisticated multispectral analysis of MR images. Spin echo and inversion recovery pulse sequence images were entered in a digital format compatible with satellite images and accurately registered pixel by pixel. Signatures of each tissue class were automatically determined using both supervised and unsupervised classification. Overall tissue classification was obtained in the form of a theme map. In MR images of the brain, for example, the classes included CSF, gray matter, white matter, subcutaneous fat, muscle, and bone. These methods provide an efficient means of identifying subtle relationships in a multi-image MR study.},
	author = {Vannier, M W and Butterfield, R L and Jordan, D and Murphy, W A and Levitt, R G and Gado, M},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1148/radiology.154.1.3964938},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Basal Ganglia Diseases; Hematoma; Humans; Lymphatic Diseases; Magnetic Resonance Spectroscopy; Pleural Effusion},
	month = {Jan},
	number = {1},
	pages = {221-4},
	pmid = {3964938},
	pst = {ppublish},
	title = {Multispectral analysis of magnetic resonance images},
	volume = {154},
	year = {1985},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiology.154.1.3964938}}

@article{Wells:1996aa,
	abstract = {Intensity-based classification of MR images has proven problematic, even when advanced techniques are used. Intrascan and interscan intensity inhomogeneities are a common source of difficulty. While reported methods have had some success in correcting intrascan inhomogeneities, such methods require supervision for the individual scan. This paper describes a new method called adaptive segmentation that uses knowledge of tissue intensity properties and intensity inhomogeneities to correct and segment MR images. Use of the expectation-maximization (EM) algorithm leads to a method that allows for more accurate segmentation of tissue types as well as better visualization of magnetic resonance imaging (MRI) data, that has proven to be effective in a study that includes more than 1000 brain scans. Implementation and results are described for segmenting the brain in the following types of images: axial (dual-echo spin-echo), coronal [three dimensional Fourier transform (3-DFT) gradient-echo T1-weighted] all using a conventional head coil, and a sagittal section acquired using a surface coil. The accuracy of adaptive segmentation was found to be comparable with manual segmentation, and closer to manual segmentation than supervised multivariant classification while segmenting gray and white matter.},
	author = {Wells, W M and Grimson, W L and Kikinis, R and Jolesz, F A},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/42.511747},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	number = {4},
	pages = {429-42},
	pmid = {18215925},
	pst = {ppublish},
	title = {Adaptive segmentation of MRI data},
	volume = {15},
	year = {1996},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.511747}}

@article{Zhang:2001aa,
	abstract = {The finite mixture (FM) model is the most commonly used model for statistical segmentation of brain magnetic resonance (MR) images because of its simple mathematical form and the piecewise constant nature of ideal brain MR images. However, being a histogram-based model, the FM has an intrinsic limitation--no spatial information is taken into account. This causes the FM model to work only on well-defined images with low levels of noise; unfortunately, this is often not the the case due to artifacts such as partial volume effect and bias field distortion. Under these conditions, FM model-based methods produce unreliable results. In this paper, we propose a novel hidden Markov random field (HMRF) model, which is a stochastic process generated by a MRF whose state sequence cannot be observed directly but which can be indirectly estimated through observations. Mathematically, it can be shown that the FM model is a degenerate version of the HMRF model. The advantage of the HMRF model derives from the way in which the spatial information is encoded through the mutual influences of neighboring sites. Although MRF modeling has been employed in MR image segmentation by other researchers, most reported methods are limited to using MRF as a general prior in an FM model-based approach. To fit the HMRF model, an EM algorithm is used. We show that by incorporating both the HMRF model and the EM algorithm into a HMRF-EM framework, an accurate and robust segmentation can be achieved. More importantly, the HMRF-EM framework can easily be combined with other techniques. As an example, we show how the bias field correction algorithm of Guillemaud and Brady (1997) can be incorporated into this framework to achieve a three-dimensional fully automated approach for brain MR image segmentation.},
	author = {Zhang, Y and Brady, M and Smith, S},
	date-added = {2015-09-19 01:07:04 +0000},
	date-modified = {2015-09-19 01:07:04 +0000},
	doi = {10.1109/42.906424},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Brain; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Markov Chains},
	month = {Jan},
	number = {1},
	pages = {45-57},
	pmid = {11293691},
	pst = {ppublish},
	title = {Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm},
	volume = {20},
	year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.906424}}

@webpage{MALF,
	date-added = {2015-09-19 01:02:31 +0000},
	date-modified = {2015-09-19 01:02:43 +0000},
	url = {https://masi.vuse.vanderbilt.edu/workshop2013/index.php/Main_Page},
	Bdsk-Url-1 = {https://masi.vuse.vanderbilt.edu/workshop2013/index.php/Main_Page}}

@article{Yushkevich:2010aa,
	abstract = {We present and evaluate a new method for automatically labeling the subfields of the hippocampal formation in focal 0.4 × 0.5 × 2.0mm(3) resolution T2-weighted magnetic resonance images that can be acquired in the routine clinical setting with under 5 min scan time. The method combines multi-atlas segmentation, similarity-weighted voting, and a novel learning-based bias correction technique to achieve excellent agreement with manual segmentation. Initial partitioning of MRI slices into hippocampal 'head', 'body' and 'tail' slices is the only input required from the user, necessitated by the nature of the underlying segmentation protocol. Dice overlap between manual and automatic segmentation is above 0.87 for the larger subfields, CA1 and dentate gyrus, and is competitive with the best results for whole-hippocampus segmentation in the literature. Intraclass correlation of volume measurements in CA1 and dentate gyrus is above 0.89. Overlap in smaller hippocampal subfields is lower in magnitude (0.54 for CA2, 0.62 for CA3, 0.77 for subiculum and 0.79 for entorhinal cortex) but comparable to overlap between manual segmentations by trained human raters. These results support the feasibility of subfield-specific hippocampal morphometry in clinical studies of memory and neurodegenerative disease.},
	author = {Yushkevich, Paul A and Wang, Hongzhi and Pluta, John and Das, Sandhitsu R and Craige, Caryne and Avants, Brian B and Weiner, Michael W and Mueller, Susanne},
	date-added = {2015-09-19 01:01:00 +0000},
	date-modified = {2015-09-19 01:01:00 +0000},
	doi = {10.1016/j.neuroimage.2010.06.040},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Adult; Aged; Aged, 80 and over; Algorithms; Brain Mapping; Female; Hippocampus; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged},
	month = {Dec},
	number = {4},
	pages = {1208-24},
	pmc = {PMC2939190},
	pmid = {20600984},
	pst = {ppublish},
	title = {Nearly automatic segmentation of hippocampal subfields in in vivo focal T2-weighted MRI},
	volume = {53},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2010.06.040}}

@article{Wang:2013ab,
	abstract = {Multi-atlas segmentation is an effective approach for automatically labeling objects of interest in biomedical images. In this approach, multiple expert-segmented example images, called atlases, are registered to a target image, and deformed atlas segmentations are combined using label fusion. Among the proposed label fusion strategies, weighted voting with spatially varying weight distributions derived from atlas-target intensity similarity have been particularly successful. However, one limitation of these strategies is that the weights are computed independently for each atlas, without taking into account the fact that different atlases may produce similar label errors. To address this limitation, we propose a new solution for the label fusion problem in which weighted voting is formulated in terms of minimizing the total expectation of labeling error and in which pairwise dependency between atlases is explicitly modeled as the joint probability of two atlases making a segmentation error at a voxel. This probability is approximated using intensity similarity between a pair of atlases and the target image in the neighborhood of each voxel. We validate our method in two medical image segmentation problems: hippocampus segmentation and hippocampus subfield segmentation in magnetic resonance (MR) images. For both problems, we show consistent and significant improvement over label fusion strategies that assign atlas weights independently.},
	author = {Hongzhi Wang and Suh, J W and Das, S R and Pluta, J B and Craige, C and Yushkevich, P A},
	date-added = {2015-09-19 00:59:19 +0000},
	date-modified = {2015-09-19 00:59:19 +0000},
	doi = {10.1109/TPAMI.2012.143},
	journal = {IEEE Trans Pattern Anal Mach Intell},
	journal-full = {IEEE transactions on pattern analysis and machine intelligence},
	month = {Mar},
	number = {3},
	pages = {611-23},
	pmc = {PMC3864549},
	pmid = {22732662},
	pst = {ppublish},
	title = {Multi-Atlas Segmentation with Joint Label Fusion},
	volume = {35},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TPAMI.2012.143}}

@article{McMillan:2014aa,
	abstract = {Frontotemporal dementia (FTD) is a clinically and pathologically heterogeneous neurodegenerative disease that can result from either frontotemporal lobar degeneration (FTLD) or Alzheimer's disease (AD) pathology. It is critical to establish statistically powerful biomarkers that can achieve substantial cost-savings and increase the feasibility of clinical trials. We assessed three broad categories of neuroimaging methods to screen underlying FTLD and AD pathology in a clinical FTD series: global measures (e.g., ventricular volume), anatomical volumes of interest (VOIs) (e.g., hippocampus) using a standard atlas, and data-driven VOIs using Eigenanatomy. We evaluated clinical FTD patients (N = 93) with cerebrospinal fluid, gray matter (GM) magnetic resonance imaging (MRI), and diffusion tensor imaging (DTI) to assess whether they had underlying FTLD or AD pathology. Linear regression was performed to identify the optimal VOIs for each method in a training dataset and then we evaluated classification sensitivity and specificity in an independent test cohort. Power was evaluated by calculating minimum sample sizes required in the test classification analyses for each model. The data-driven VOI analysis using a multimodal combination of GM MRI and DTI achieved the greatest classification accuracy (89% sensitive and 89% specific) and required a lower minimum sample size (N = 26) relative to anatomical VOI and global measures. We conclude that a data-driven VOI approach using Eigenanatomy provides more accurate classification, benefits from increased statistical power in unseen datasets, and therefore provides a robust method for screening underlying pathology in FTD patients for entry into clinical trials.},
	author = {McMillan, Corey T and Avants, Brian B and Cook, Philip and Ungar, Lyle and Trojanowski, John Q and Grossman, Murray},
	date-added = {2015-09-19 00:24:07 +0000},
	date-modified = {2015-09-19 00:24:07 +0000},
	doi = {10.1002/hbm.22515},
	journal = {Hum Brain Mapp},
	journal-full = {Human brain mapping},
	keywords = {Alzheimer's disease; DTI; MRI; biomarkers; classification; frontotemporal degeneration; statistical power},
	mesh = {Aged; Alzheimer Disease; Amyloid beta-Peptides; Brain; Cohort Studies; Diagnosis, Differential; Diffusion Tensor Imaging; Female; Frontotemporal Dementia; Frontotemporal Lobar Degeneration; Gray Matter; Humans; Image Interpretation, Computer-Assisted; Linear Models; Magnetic Resonance Imaging; Male; Middle Aged; Organ Size; Peptide Fragments; Sensitivity and Specificity; tau Proteins},
	month = {Sep},
	number = {9},
	pages = {4827-40},
	pmc = {PMC4107021},
	pmid = {24687814},
	pst = {ppublish},
	title = {The power of neuroimaging biomarkers for screening frontotemporal dementia},
	volume = {35},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/hbm.22515}}

@article{Cook:2014aa,
	abstract = {Linking structural neuroimaging data from multiple modalities to cognitive performance is an important challenge for cognitive neuroscience. In this study we examined the relationship between verbal fluency performance and neuroanatomy in 54 patients with frontotemporal degeneration (FTD) and 15 age-matched controls, all of whom had T1- and diffusion-weighted imaging. Our goal was to incorporate measures of both gray matter (voxel-based cortical thickness) and white matter (fractional anisotropy) into a single statistical model that relates to behavioral performance. We first used eigenanatomy to define data-driven regions of interest (DD-ROIs) for both gray matter and white matter. Eigenanatomy is a multivariate dimensionality reduction approach that identifies spatially smooth, unsigned principal components that explain the maximal amount of variance across subjects. We then used a statistical model selection procedure to see which of these DD-ROIs best modeled performance on verbal fluency tasks hypothesized to rely on distinct components of a large-scale neural network that support language: category fluency requires a semantic-guided search and is hypothesized to rely primarily on temporal cortices that support lexical-semantic representations; letter-guided fluency requires a strategic mental search and is hypothesized to require executive resources to support a more demanding search process, which depends on prefrontal cortex in addition to temporal network components that support lexical representations. We observed that both types of verbal fluency performance are best described by a network that includes a combination of gray matter and white matter. For category fluency, the identified regions included bilateral temporal cortex and a white matter region including left inferior longitudinal fasciculus and frontal-occipital fasciculus. For letter fluency, a left temporal lobe region was also selected, and also regions of frontal cortex. These results are consistent with our hypothesized neuroanatomical models of language processing and its breakdown in FTD. We conclude that clustering the data with eigenanatomy before performing linear regression is a promising tool for multimodal data analysis.},
	author = {Cook, Philip A and McMillan, Corey T and Avants, Brian B and Peelle, Jonathan E and Gee, James C and Grossman, Murray},
	date-added = {2015-09-19 00:23:14 +0000},
	date-modified = {2015-09-19 00:23:14 +0000},
	doi = {10.1016/j.neuroimage.2014.05.008},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	keywords = {FTD; Language; Multimodal; Verbal fluency},
	mesh = {Aged; Brain; Cognition; Female; Frontotemporal Lobar Degeneration; Gray Matter; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Models, Neurological; Multivariate Analysis; Nerve Net; Neuropsychological Tests; Reproducibility of Results; Verbal Behavior; White Matter},
	month = {Oct},
	pages = {477-86},
	pmc = {PMC4151353},
	pmid = {24830834},
	pst = {ppublish},
	title = {Relating brain anatomy and cognitive ability using a multivariate multimodal framework},
	volume = {99},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2014.05.008}}

@article{Datta:2012aa,
	abstract = {There is a long history and a growing interest in the canine as a subject of study in neuroscience research and in translational neurology. In the last few years, anatomical and functional magnetic resonance imaging (MRI) studies of awake and anesthetized dogs have been reported. Such efforts can be enhanced by a population atlas of canine brain anatomy to implement group analyses. Here we present a canine brain atlas derived as the diffeomorphic average of a population of fifteen mesaticephalic dogs. The atlas includes: 1) A brain template derived from in-vivo, T1-weighted imaging at 1 mm isotropic resolution at 3 Tesla (with and without the soft tissues of the head); 2) A co-registered, high-resolution (0.33 mm isotropic) template created from imaging of ex-vivo brains at 7 Tesla; 3) A surface representation of the gray matter/white matter boundary of the high-resolution atlas (including labeling of gyral and sulcal features). The properties of the atlas are considered in relation to historical nomenclature and the evolutionary taxonomy of the Canini tribe. The atlas is available for download (https://cfn.upenn.edu/aguirre/wiki/public:data_plosone_2012_datta).},
	author = {Datta, Ritobrato and Lee, Jongho and Duda, Jeffrey and Avants, Brian B and Vite, Charles H and Tseng, Ben and Gee, James C and Aguirre, Gustavo D and Aguirre, Geoffrey K},
	date-added = {2015-09-19 00:22:05 +0000},
	date-modified = {2015-09-19 00:22:05 +0000},
	doi = {10.1371/journal.pone.0052140},
	journal = {PLoS One},
	journal-full = {PloS one},
	mesh = {Animals; Brain; Dogs; Magnetic Resonance Imaging},
	number = {12},
	pages = {e52140},
	pmc = {PMC3527386},
	pmid = {23284904},
	pst = {ppublish},
	title = {A digital atlas of the dog brain},
	volume = {7},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1371/journal.pone.0052140}}

@article{Avants:2015aa,
	abstract = {Magnetic resonance imaging (MRI) captures the dynamics of brain development with multiple modalities that quantify both structure and function. These measurements may yield valuable insights into the neural patterns that mark healthy maturation or that identify early risk for psychiatric disorder. The Pediatric Template of Brain Perfusion (PTBP) is a free and public neuroimaging resource that will help accelerate the understanding of childhood brain development as seen through the lens of multiple modality neuroimaging and in relation to cognitive and environmental factors. The PTBP uses cross-sectional and longitudinal MRI to quantify cortex, white matter, resting state functional connectivity and brain perfusion, as measured by Arterial Spin Labeling (ASL), in 120 children 7-18 years of age. We describe the PTBP and show, as a demonstration of validity, that global summary measurements capture the trajectories that demarcate critical turning points in brain maturation. This novel resource will allow a more detailed understanding of the network-level, structural and functional landmarks that are obtained during normal adolescent brain development.},
	author = {Avants, Brian B and Duda, Jeffrey T and Kilroy, Emily and Krasileva, Kate and Jann, Kay and Kandel, Benjamin T and Tustison, Nicholas J and Yan, Lirong and Jog, Mayank and Smith, Robert and Wang, Yi and Dapretto, Mirella and Wang, Danny J J},
	date-added = {2015-09-19 00:21:19 +0000},
	date-modified = {2015-09-19 00:21:19 +0000},
	doi = {10.1038/sdata.2015.3},
	journal = {Sci Data},
	journal-full = {Scientific data},
	pages = {150003},
	pmc = {PMC4413243},
	pmid = {25977810},
	pst = {epublish},
	title = {The pediatric template of brain perfusion},
	volume = {2},
	year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/sdata.2015.3}}

@article{Hankinson:1999aa,
	abstract = {Spirometric reference values for Caucasians, African-Americans, and Mexican-Americans 8 to 80 yr of age were developed from 7,429 asymptomatic, lifelong nonsmoking participants in the third National Health and Nutrition Examination Survey (NHANES III). Spirometry examinations followed the 1987 American Thoracic Society recommendations, and the quality of the data was continuously monitored and maintained. Caucasian subjects had higher mean FVC and FEV1 values than did Mexican-American and African-American subjects across the entire age range. However, Caucasian and Mexican-American subjects had similar FVC and FEV1 values with respect to height, and African-American subjects had lower values. These differences may be partially due to differences in body build: observed Mexican-Americans were shorter than Caucasian subjects of the same age, and African-Americans on average have a smaller trunk:leg ratio than do Caucasians. Reference values and lower limits of normal were derived using a piecewise polynomial model with age and height as predictors. These reference values encompass a wide age range for three race/ethnic groups and should prove useful for diagnostic and research purposes.},
	author = {Hankinson, J L and Odencrantz, J R and Fedan, K B},
	date-added = {2015-09-17 16:00:34 +0000},
	date-modified = {2015-09-17 16:00:34 +0000},
	doi = {10.1164/ajrccm.159.1.9712108},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Adolescent; Adult; African Continental Ancestry Group; Aged; Aged, 80 and over; Body Height; Child; European Continental Ancestry Group; Female; Forced Expiratory Volume; Humans; Male; Mexican Americans; Middle Aged; Nutrition Surveys; Reference Values; Respiration; Spirometry; United States; Vital Capacity},
	month = {Jan},
	number = {1},
	pages = {179-87},
	pmid = {9872837},
	pst = {ppublish},
	title = {Spirometric reference values from a sample of the general U.S. population},
	volume = {159},
	year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm.159.1.9712108}}

@article{Altes:2006aa,
	abstract = {PURPOSE: To determine whether hyperpolarized helium-3 (HHe) diffusion MR can detect the expected enlargement of alveoli that occurs with lung growth during childhood.
MATERIALS AND METHODS: A total of 29 normal subjects aged four to 30 years underwent HHe diffusion MR imaging with the b-value pair 0, 1.6 second/cm(2). A second acquisition during a separate breathhold was performed using the b-value pair 0, 4 second/cm(2) to evaluate the dependence on b-value. The mean apparent diffusion coefficient (ADC) and lung volume for each acquisition and each subject was determined.
RESULTS: Subjects as young as four years of age were able to cooperate with the imaging procedure. The mean ADC increased with increasing subject age (r = 0.8; P < 0.001), with a 55% increase in mean ADC from the youngest to oldest subject. Lung volumes measured on MR were highly repeatable for the two HHe MR acquisitions (r = 0.980, P < 0.001). The mean ADC values measured with the two different b-value pairs were highly correlated (r = 0.975; P < 0.001), but the higher b-value pair resulted in slightly lower mean ADCs (P < 0.001).
CONCLUSION: HHe diffusion MR appears to detect the expected increase in alveolar size during childhood, and thus HHe MR may be a noninvasive method to assess development of the lung microstructure.},
	author = {Altes, Talissa A and Mata, Jaime and de Lange, Eduard E and Brookeman, James R and Mugler, 3rd, John P},
	date-added = {2015-09-17 04:43:13 +0000},
	date-modified = {2015-09-17 04:43:13 +0000},
	doi = {10.1002/jmri.20723},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Adolescent; Adult; Algorithms; Child; Child, Preschool; Diffusion Magnetic Resonance Imaging; Female; Helium; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Isotopes; Lung; Male; Radiopharmaceuticals; Reproducibility of Results; Sensitivity and Specificity},
	month = {Dec},
	number = {6},
	pages = {1277-83},
	pmid = {17096396},
	pst = {ppublish},
	title = {Assessment of lung development using hyperpolarized helium-3 diffusion MR imaging},
	volume = {24},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.20723}}

@url{GOLD,
	date-added = {2015-09-17 00:00:48 +0000},
	date-modified = {2020-11-01 07:37:08 -0800},
	title = {http://www.goldcopd.org/Guidelines/ guidelines-resources.html},
	url = {http://www.goldcopd.org/Guidelines/ guidelines-resources.html},
	Bdsk-Url-1 = {http://www.goldcopd.org/Guidelines/%20guidelines-resources.html}}

@article{Miller:2005ab,
	author = {Miller, M R and Hankinson, J and Brusasco, V and Burgos, F and Casaburi, R and Coates, A and Crapo, R and Enright, P and van der Grinten, C P M and Gustafsson, P and Jensen, R and Johnson, D C and MacIntyre, N and McKay, R and Navajas, D and Pedersen, O F and Pellegrino, R and Viegi, G and Wanger, J and {ATS/ERS Task Force}},
	date-added = {2015-09-16 23:59:03 +0000},
	date-modified = {2015-09-16 23:59:03 +0000},
	doi = {10.1183/09031936.05.00034805},
	journal = {Eur Respir J},
	journal-full = {The European respiratory journal},
	mesh = {Humans; Maximal Voluntary Ventilation; Peak Expiratory Flow Rate; Spirometry; Vital Capacity},
	month = {Aug},
	number = {2},
	pages = {319-38},
	pmid = {16055882},
	pst = {ppublish},
	title = {Standardisation of spirometry},
	volume = {26},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1183/09031936.05.00034805}}

@article{Miller:2005aa,
	author = {Miller, M R and Crapo, R and Hankinson, J and Brusasco, V and Burgos, F and Casaburi, R and Coates, A and Enright, P and van der Grinten, C P M and Gustafsson, P and Jensen, R and Johnson, D C and MacIntyre, N and McKay, R and Navajas, D and Pedersen, O F and Pellegrino, R and Viegi, G and Wanger, J and {ATS/ERS Task Force}},
	date-added = {2015-09-16 23:58:21 +0000},
	date-modified = {2015-09-16 23:58:21 +0000},
	doi = {10.1183/09031936.05.00034505},
	journal = {Eur Respir J},
	journal-full = {The European respiratory journal},
	mesh = {Adult; Age Factors; Airway Obstruction; Airway Resistance; Female; Humans; Lung Diseases; Male; Middle Aged; Quality Control; Respiratory Function Tests; Risk Factors; Sensitivity and Specificity; Severity of Illness Index; Spirometry},
	month = {Jul},
	number = {1},
	pages = {153-61},
	pmid = {15994402},
	pst = {ppublish},
	title = {General considerations for lung function testing},
	volume = {26},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1183/09031936.05.00034505}}

@article{Qing:2014ab,
	abstract = {Magnetic-resonance spectroscopy and imaging using hyperpolarized xenon-129 show great potential for evaluation of the most important function of the human lung -- gas exchange. In particular, chemical shift saturation recovery (CSSR) xenon-129 spectroscopy provides important physiological information for the lung as a whole by characterizing the dynamic process of gas exchange, while dissolved-phase (DP) xenon-129 imaging captures the time-averaged regional distribution of gas uptake by lung tissue and blood. Herein, we present recent advances in assessing lung function using CSSR spectroscopy and DP imaging in a total of 45 subjects (23 healthy, 13 chronic obstructive pulmonary disease (COPD) and 9 asthma). From CSSR acquisitions, the COPD subjects showed red blood cell to tissue-plasma (RBC-to-TP) ratios below the average for the healthy subjects (p < 0.001), but significantly higher septal wall thicknesses as compared with the healthy subjects (p < 0.005); the RBC-to-TP ratios for the asthmatic subjects fell outside two standard deviations (either higher or lower) from the mean of the healthy subjects, although there was no statistically significant difference for the average ratio of the study group as a whole. Similarly, from the 3D DP imaging acquisitions, we found that all the ratios (TP to gas phase (GP), RBC to GP, RBC to TP) measured in the COPD subjects were lower than those from the healthy subjects (p < 0.05 for all ratios), while these ratios in the asthmatic subjects differed considerably between subjects. Despite having been performed at different lung inflation levels, the RBC-to-TP ratios measured by CSSR and 3D DP imaging were fairly consistent with each other, with a mean difference of 0.037 (ratios from 3D DP imaging larger). In ten subjects the RBC-to-GP ratios obtained from the 3D DP imaging acquisitions were also highly correlated with their diffusing capacity of the lung for carbon monoxide per unit alveolar volume ratios measured by pulmonary function testing (R = 0.91).},
	author = {Qing, Kun and Mugler, 3rd, John P and Altes, Talissa A and Jiang, Yun and Mata, Jaime F and Miller, G Wilson and Ruset, Iulian C and Hersman, F William and Ruppert, Kai},
	date-added = {2015-09-16 23:14:54 +0000},
	date-modified = {2015-09-16 23:14:54 +0000},
	doi = {10.1002/nbm.3179},
	journal = {NMR Biomed},
	journal-full = {NMR in biomedicine},
	keywords = {3D dissolved-phase imaging; COPD; CSSR; asthma; gas uptake; hyperpolarized xenon-129; pulmonary imaging},
	mesh = {Adolescent; Adult; Asthma; Carbon Monoxide; Computer Simulation; Erythrocytes; Female; Humans; Imaging, Three-Dimensional; Lung; Magnetic Resonance Imaging; Male; Middle Aged; Pulmonary Disease, Chronic Obstructive; Pulmonary Ventilation; Respiratory Function Tests; Spectrum Analysis; Xenon Isotopes; Young Adult},
	month = {Dec},
	number = {12},
	pages = {1490-501},
	pmc = {PMC4233004},
	pmid = {25146558},
	pst = {ppublish},
	title = {Assessment of lung function in asthma and COPD using hyperpolarized 129Xe chemical shift saturation recovery spectroscopy and dissolved-phase MRI},
	volume = {27},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/nbm.3179}}

@article{Mugler:2010aa,
	abstract = {Despite a myriad of technical advances in medical imaging, as well as the growing need to address the global impact of pulmonary diseases, such as asthma and chronic obstructive pulmonary disease, on health and quality of life, it remains challenging to obtain in vivo regional depiction and quantification of the most basic physiological functions of the lung-gas delivery to the airspaces and gas uptake by the lung parenchyma and blood-in a manner suitable for routine application in humans. We report a method based on MRI of hyperpolarized xenon-129 that permits simultaneous observation of the 3D distributions of ventilation (gas delivery) and gas uptake, as well as quantification of regional gas uptake based on the associated ventilation. Subjects with lung disease showed variations in gas uptake that differed from those in ventilation in many regions, suggesting that gas uptake as measured by this technique reflects such features as underlying pathological alterations of lung tissue or of local blood flow. Furthermore, the ratio of the signal associated with gas uptake to that associated with ventilation was substantially altered in subjects with lung disease compared with healthy subjects. This MRI-based method provides a way to quantify relationships among gas delivery, exchange, and transport, and appears to have significant potential to provide more insight into lung disease.},
	author = {Mugler, 3rd, John P and Altes, Talissa A and Ruset, Iulian C and Dregely, Isabel M and Mata, Jaime F and Miller, G Wilson and Ketel, Stephen and Ketel, Jeffrey and Hersman, F William and Ruppert, Kai},
	date-added = {2015-09-16 23:14:04 +0000},
	date-modified = {2015-09-16 23:14:04 +0000},
	doi = {10.1073/pnas.1011912107},
	journal = {Proc Natl Acad Sci U S A},
	journal-full = {Proceedings of the National Academy of Sciences of the United States of America},
	mesh = {Adult; Aged; Female; Gases; Humans; Lung; Lung Diseases; Magnetic Resonance Imaging; Male; Middle Aged; Pulmonary Ventilation; Respiration; Ventilation-Perfusion Ratio; Xenon Isotopes; Young Adult},
	month = {Dec},
	number = {50},
	pages = {21707-12},
	pmc = {PMC3003026},
	pmid = {21098267},
	pst = {ppublish},
	title = {Simultaneous magnetic resonance imaging of ventilation distribution and gas uptake in the human lung using hyperpolarized xenon-129},
	volume = {107},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1073/pnas.1011912107}}

@article{Dregely:2011aa,
	abstract = {PURPOSE: To develop and test a method to noninvasively assess the functional lung microstructure.
MATERIALS AND METHODS: The Multiple exchange time Xenon polarization Transfer Contrast technique (MXTC) encodes xenon gas-exchange contrast at multiple delay times permitting two lung-function parameters to be derived: (i) MXTC-F, the long exchange-time depolarization value, which is proportional to the tissue to alveolar-volume ratio and (ii) MXTC-S, the square root of the xenon exchange-time constant, which characterizes thickness and composition of alveolar septa. Three healthy volunteers, one asthmatic, and two chronic obstructive pulmonary disease (COPD) (GOLD stage I and II) subjects were imaged with MXTC MRI. In a subset of subjects, hyperpolarized xenon-129 ADC MRI and CT imaging were also performed.
RESULTS: The MXTC-S parameter was found to be elevated in subjects with lung disease (P-value = 0.018). In the MXTC-F parameter map it was feasible to identify regional loss of functional tissue in a COPD patient. MXTC-F maps showed excellent regional correlation with CT and ADC (P ≥ 0.90) in one COPD subject.
CONCLUSION: The functional tissue-density parameter MXTC-F showed regional agreement with other imaging techniques. The newly developed parameter MXTC-S, which characterizes the functional thickness of alveolar septa, has potential as a novel biomarker for regional parenchymal inflammation or thickening.},
	author = {Dregely, Isabel and Mugler, 3rd, John P and Ruset, Iulian C and Altes, Talissa A and Mata, Jaime F and Miller, G Wilson and Ketel, Jeffrey and Ketel, Steve and Distelbrink, Jan and Hersman, F W and Ruppert, Kai},
	date-added = {2015-09-16 23:13:20 +0000},
	date-modified = {2015-09-16 23:13:20 +0000},
	doi = {10.1002/jmri.22533},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Adult; Algorithms; Contrast Media; Female; Humans; Image Processing, Computer-Assisted; Lung; Lung Diseases, Obstructive; Magnetic Resonance Imaging; Male; Medical Informatics; Middle Aged; Models, Statistical; Software; Time Factors; Tomography, X-Ray Computed; Xenon; Xenon Isotopes},
	month = {May},
	number = {5},
	pages = {1052-62},
	pmc = {PMC3081140},
	pmid = {21509861},
	pst = {ppublish},
	title = {Hyperpolarized Xenon-129 gas-exchange imaging of lung microstructure: first case studies in subjects with obstructive lung disease},
	volume = {33},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.22533}}

@article{Gee:1993aa,
	abstract = {To evaluate our system for elastically deforming a three-dimensional atlas to match anatomical brain images, six deformed versions of an atlas were generated. The deformed atlases were created by elastically mapping an anatomical brain atlas onto different MR brain image volumes. The mapping matches the edges of the ventricles and the surface of the brain; the resultant deformations are propagated through the atlas volume, deforming the remainder of the structures in the process. The atlas was then elastically matched to its deformed versions. The accuracy of the resultant matches was evaluated by determining the correspondence of 32 cortical and subcortical structures. The system on average matched the centroid of a structure to within 1 mm of its true position and fit a structure to within 11% of its true volume. The overlap between the matched and true structures, defined by the ratio between the volume of their intersection and the volume of their union, averaged 66%. When the gray-white interface was included for matching, the mean overlap improved to 78%; each structure was matched to within 0.6 mm of its true position and fit to within 6% of its true volume. Preliminary studies were also made to determine the effect of the compliance of the atlas on the resultant match.},
	author = {Gee, J C and Reivich, M and Bajcsy, R},
	date = {1993 Mar-Apr},
	date-added = {2015-09-14 21:51:21 +0000},
	date-modified = {2015-09-14 21:51:21 +0000},
	journal = {J Comput Assist Tomogr},
	journal-full = {Journal of computer assisted tomography},
	mesh = {Anatomy, Artistic; Brain; Caudate Nucleus; Cerebral Ventricles; Female; Frontal Lobe; Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Medical Illustration; Medulla Oblongata; Mesencephalon; Pons; Sensitivity and Specificity; Temporal Lobe; Thalamus},
	number = {2},
	pages = {225-36},
	pmid = {8454749},
	pst = {ppublish},
	title = {Elastically deforming 3D atlas to match anatomical brain images},
	volume = {17},
	year = {1993}}

@article{Bajcsy:1989aa,
	author = {Ruzena Bajcsy and Stanislav Kovacic},
	bibsource = {dblp computer science bibliography, http://dblp.org},
	biburl = {http://dblp.uni-trier.de/rec/bib/journals/cvgip/BajcsyK89},
	date-added = {2015-09-14 21:50:35 +0000},
	date-modified = {2015-09-14 21:50:54 +0000},
	doi = {10.1016/S0734-189X(89)80014-3},
	journal = {Computer Vision, Graphics, and Image Processing},
	number = {1},
	pages = {1--21},
	timestamp = {Mon, 04 Sep 2006 12:38:04 +0200},
	title = {Multiresolution elastic matching},
	url = {http://dx.doi.org/10.1016/S0734-189X(89)80014-3},
	volume = {46},
	year = {1989},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/S0734-189X(89)80014-3}}

@inproceedings{Bajcsy:1982aa,
	author = {R. Bajcsy and C. Broit},
	booktitle = {{S}ixth {I}nternational {C}onference on {P}attern {R}ecognition ({ICPR}'82)},
	date-added = {2015-09-14 21:48:25 +0000},
	date-modified = {2020-10-12 07:59:46 -0700},
	pages = {351--353},
	title = {Matching of deformed images},
	year = {1982}}

@article{Messay:2010aa,
	abstract = {Early detection of lung nodules is extremely important for the diagnosis and clinical management of lung cancer. In this paper, a novel computer aided detection (CAD) system for the detection of pulmonary nodules in thoracic computed tomography (CT) imagery is presented. The paper describes the architecture of the CAD system and assesses its performance on a publicly available database to serve as a benchmark for future research efforts. Training and tuning of all modules in our CAD system is done using a separate and independent dataset provided courtesy of the University of Texas Medical Branch (UTMB). The publicly available testing dataset is that created by the Lung Image Database Consortium (LIDC). The LIDC data used here is comprised of 84 CT scans containing 143 nodules ranging from 3 to 30mm in effective size that are manually segmented at least by one of the four radiologists. The CAD system uses a fully automated lung segmentation algorithm to define the boundaries of the lung regions. It combines intensity thresholding with morphological processing to detect and segment nodule candidates simultaneously. A set of 245 features is computed for each segmented nodule candidate. A sequential forward selection process is used to determine the optimum subset of features for two distinct classifiers, a Fisher Linear Discriminant (FLD) classifier and a quadratic classifier. A performance comparison between the two classifiers is presented, and based on this, the FLD classifier is selected for the CAD system. With an average of 517.5 nodule candidates per case/scan (517.5+/-72.9), the proposed front-end detector/segmentor is able to detect 92.8% of all the nodules in the LIDC/testing dataset (based on merged ground truth). The mean overlap between the nodule regions delineated by three or more radiologists and the ones segmented by the proposed segmentation algorithm is approximately 63%. Overall, with a specificity of 3 false positives (FPs) per case/patient on average, the CAD system is able to correctly identify 80.4% of the nodules (115/143) using 40 selected features. A 7-fold cross-validation performance analysis using the LIDC database only shows CAD sensitivity of 82.66% with an average of 3 FPs per CT scan/case.},
	author = {Messay, Temesguen and Hardie, Russell C and Rogers, Steven K},
	date-added = {2015-09-14 20:49:08 +0000},
	date-modified = {2015-09-14 20:49:08 +0000},
	doi = {10.1016/j.media.2010.02.004},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Algorithms; Artificial Intelligence; Humans; Lung Neoplasms; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Solitary Pulmonary Nodule; Tomography, X-Ray Computed},
	month = {Jun},
	number = {3},
	pages = {390-406},
	pmid = {20346728},
	pst = {ppublish},
	title = {A new computationally efficient CAD system for pulmonary nodule detection in CT imagery},
	volume = {14},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2010.02.004}}

@article{Ginneken:2010aa,
	abstract = {Numerous publications and commercial systems are available that deal with automatic detection of pulmonary nodules in thoracic computed tomography scans, but a comparative study where many systems are applied to the same data set has not yet been performed. This paper introduces ANODE09 ( http://anode09.isi.uu.nl), a database of 55 scans from a lung cancer screening program and a web-based framework for objective evaluation of nodule detection algorithms. Any team can upload results to facilitate benchmarking. The performance of six algorithms for which results are available are compared; five from academic groups and one commercially available system. A method to combine the output of multiple systems is proposed. Results show a substantial performance difference between algorithms, and demonstrate that combining the output of algorithms leads to marked performance improvements.},
	author = {van Ginneken, Bram and Armato, 3rd, Samuel G and de Hoop, Bartjan and van Amelsvoort-van de Vorst, Saskia and Duindam, Thomas and Niemeijer, Meindert and Murphy, Keelin and Schilham, Arnold and Retico, Alessandra and Fantacci, Maria Evelina and Camarlinghi, Niccol{\`o} and Bagagli, Francesco and Gori, Ilaria and Hara, Takeshi and Fujita, Hiroshi and Gargano, Gianfranco and Bellotti, Roberto and Tangaro, Sabina and Bola{\~n}os, Lourdes and De Carlo, Francesco and Cerello, Piergiorgio and Cristian Cheran, Sorin and Lopez Torres, Ernesto and Prokop, Mathias},
	date-added = {2015-09-14 20:46:25 +0000},
	date-modified = {2015-09-14 20:46:25 +0000},
	doi = {10.1016/j.media.2010.05.005},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Aged; Algorithms; Female; Humans; Lung Neoplasms; Male; Middle Aged; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Software; Software Validation; Solitary Pulmonary Nodule; Tomography, X-Ray Computed},
	month = {Dec},
	number = {6},
	pages = {707-22},
	pmid = {20573538},
	pst = {ppublish},
	title = {Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: The ANODE09 study},
	volume = {14},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2010.05.005}}

@proceedings{Landman2012,
	date-added = {2015-09-07 03:06:25 +0000},
	date-modified = {2015-09-07 03:07:30 +0000},
	editor = {B. Landman and S. Warfield},
	publisher = {CreateSpace},
	title = {{MICCAI} 2012 Workshop on Multi- Atlas Labeling},
	year = {2012}}

@inproceedings{Asman:2013aa,
	author = {A. Asman and A. Akhondi-Asl and H. Wang and N. Tustison and B. Avants and S. K. Warfield and B. Landman},
	booktitle = {MICCAI 2013 Challenge Workshop on Segmentation: Algorithms, Theory and Applications.},
	date-added = {2015-09-07 03:03:12 +0000},
	date-modified = {2020-06-02 17:51:35 -0700},
	title = {{MICCAI} 2013 segmentation algorithms, theory and applications ({SATA}) challenge results summary,},
	year = {2013}}

@article{McErlean:2013aa,
	abstract = {PURPOSE: To assess variability of computed tomographic (CT) measurements of lesions of various sizes and margin sharpness in several organs taken by readers with different levels of experience, as would be found in routine clinical practice.
MATERIALS AND METHODS: In this institutional review board-approved, HIPAA-compliant retrospective study, 17 radiologists with varying levels of experience independently obtained bidimensional orthogonal axial measurements of 80 lymph nodes, 120 pulmonary lesions, and 120 hepatic lesions, categorized by size and margin sharpness. Repeat measurements were performed 2 or more weeks later. Intraclass correlation coefficients and Bland-Altman plots were used to assess intra- and interobserver variability.
RESULTS: For long- and short-axis measurements, respectively, overall intraobserver agreement rates were 0.957 (95% confidence interval [CI]: 0.947, 0.966) and 0.945 (95% CI: 0.933, 0.955); interobserver agreement rates were 0.954 (95% CI: 0.943, 0.963) and 0.941 (95% CI: 0.929, 0.951). Both intra- and interobserver agreement differed by lesion size, margin sharpness, location, and reader experience. Agreement ranged from 0.847 to 0.886 for lesions 20 mm or larger versus 0.745-0.785 for lesions smaller than 10 mm, 0.961 to 0.975 for smooth margins versus 0.924-0.942 for irregular margins, 0.955 to 0.97 for lung lesions versus 0.884-0.94 for lymph nodes, and 0.95 to 0.97 for attending radiologists versus 0.928-0.945 for fellows. Measurement variability decreased with increasing lesion size; 95% limits of agreement for short-axis measurements were -11.6% to 6.7% for lesions smaller than 10 mm versus -6.2% to 4.7% for lesions 20 mm or larger.
CONCLUSION: Overall intra- and interobserver variability rates were similar; in clinical practice, serial CT measurements can be safely performed by different radiologists. Smooth margins, larger lesion size, and greater reader experience resulted in a higher consistency of measurements. Depending on lesion size, increases of 4%-6% or greater in long axis and 5%-7% or greater in short axis and decreases of -6% to -10% or greater in long axis and -6% to -12% or greater in short axis at CT can be considered true changes rather than measurement variation, with 95% confidence.},
	author = {McErlean, Aoife and Panicek, David M and Zabor, Emily C and Moskowitz, Chaya S and Bitar, Richard and Motzer, Robert J and Hricak, Hedvig and Ginsberg, Michelle S},
	date-added = {2015-09-06 22:08:47 +0000},
	date-modified = {2015-09-06 22:08:47 +0000},
	doi = {10.1148/radiol.13122665},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Clinical Competence; Female; Humans; Liver Diseases; Lung Diseases; Lymph Nodes; Male; Observer Variation; Retrospective Studies; Tomography, X-Ray Computed},
	month = {Nov},
	number = {2},
	pages = {451-9},
	pmid = {23824993},
	pst = {ppublish},
	title = {Intra- and interobserver variability in CT measurements in oncology},
	volume = {269},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiol.13122665}}

@article{Zhao:2013aa,
	abstract = {OBJECTIVE: Understanding magnitudes of variability when measuring tumor size may be valuable in improving detection of tumor change and thus evaluating tumor response to therapy in clinical trials and care. Our study explored intra- and inter-reader variability of tumor uni-dimensional (1D), bi-dimensional (2D), and volumetric (VOL) measurements using manual and computer-aided methods (CAM) on CT scans reconstructed at different slice intervals.
MATERIALS AND METHODS: Raw CT data from 30 patients enrolled in oncology clinical trials was reconstructed at 5, 2.5, and 1.25 mm slice intervals. 118 lesions in the lungs, liver, and lymph nodes were analyzed. For each lesion, two independent radiologists manually and, separately, using computer software, measured the maximum diameter (1D), maximum perpendicular diameter, and volume (CAM only). One of them blindly repeated the measurements. Intra- and inter-reader variability for the manual method and CAM were analyzed using linear mixed-effects models and Bland-Altman method.
RESULTS: For the three slice intervals, the maximum coefficients of variation for manual intra-/inter-reader variability were 6.9%/9.0% (1D) and 12.3%/18.0% (2D), and for CAM were 5.4%/9.3% (1D), 11.3%/18.8% (2D) and 9.3%/18.0% (VOL). Maximal 95% reference ranges for the percentage difference in intra-reader measurements for manual 1D and 2D, and CAM VOL were (-15.5%, 25.8%), (-27.1%, 51.6%), and (-22.3%, 33.6%), respectively.
CONCLUSIONS: Variability in measuring the diameter and volume of solid tumors, manually and by CAM, is affected by CT slice interval. The 2.5mm slice interval provides the least measurement variability. Among the three techniques, 2D has the greatest measurement variability compared to 1D and 3D.},
	author = {Zhao, Binsheng and Tan, Yongqiang and Bell, Daniel J and Marley, Sarah E and Guo, Pingzhen and Mann, Helen and Scott, Marietta L J and Schwartz, Lawrence H and Ghiorghiu, Dana C},
	date-added = {2015-09-06 22:06:53 +0000},
	date-modified = {2015-09-06 22:06:53 +0000},
	doi = {10.1016/j.ejrad.2013.02.018},
	journal = {Eur J Radiol},
	journal-full = {European journal of radiology},
	mesh = {Algorithms; Humans; Imaging, Three-Dimensional; Neoplasms; Observer Variation; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Tumor Burden},
	month = {Jun},
	number = {6},
	pages = {959-68},
	pmc = {PMC3823057},
	pmid = {23489982},
	pst = {ppublish},
	title = {Exploring intra- and inter-reader variability in uni-dimensional, bi-dimensional, and volumetric measurements of solid tumors on CT scans reconstructed at different slice intervals},
	volume = {82},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.ejrad.2013.02.018}}

@article{Wang:2010aa,
	abstract = {Medical imaging can help answer key questions that arise during the drug development process. The role of medical imaging in new drug clinical trials includes identification of likely responders; detection and diagnosis of lesions and evaluation of their severity; and therapy monitoring and follow-up. Nuclear imaging techniques such as PET can be used to monitor drug pharmacokinetics and distribution and study specific molecular endpoints. In assessing drug efficacy, imaging biomarkers and imaging surrogate endpoints can be more objective and faster to measure than clinical outcomes, and allow small group sizes, quick results and good statistical power. Imaging also has important role in drug safety monitoring, particularly when there is no other suitable biomarkers available. Despite the long history of radiological sciences, its application to the drug development process is relatively recent. This review highlights the processes, opportunities, and challenges of medical imaging in new drug development.},
	author = {Wang, Yi-Xiang and Deng, Min},
	date-added = {2015-09-06 22:01:46 +0000},
	date-modified = {2015-09-06 22:01:46 +0000},
	doi = {10.3978/j.issn.2072-1439.2010.11.10},
	journal = {J Thorac Dis},
	journal-full = {Journal of thoracic disease},
	keywords = {clinical development; clinical trials; diagnostic radiology; medical imaging; new drug; pharmaceutical industry},
	month = {Dec},
	number = {4},
	pages = {245-52},
	pmc = {PMC3256476},
	pmid = {22263053},
	pst = {ppublish},
	title = {Medical imaging in new drug clinical development},
	volume = {2},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.3978/j.issn.2072-1439.2010.11.10}}

@inproceedings{Song:2010aa,
	author = {G. Song and N. Tustison and J. C. Gee},
	booktitle = {Proc. 3rd Int. Workshop Pulmonary Image Analysis},
	date-added = {2015-08-23 23:11:17 +0000},
	date-modified = {2015-08-23 23:26:20 +0000},
	pages = {109-116},
	title = {Airway tree segmentation by removing paths of leakage},
	year = {2010}}

@article{Lo:2012aa,
	abstract = {This paper describes a framework for establishing a reference airway tree segmentation, which was used to quantitatively evaluate fifteen different airway tree extraction algorithms in a standardized manner. Because of the sheer difficulty involved in manually constructing a complete reference standard from scratch, we propose to construct the reference using results from all algorithms that are to be evaluated. We start by subdividing each segmented airway tree into its individual branch segments. Each branch segment is then visually scored by trained observers to determine whether or not it is a correctly segmented part of the airway tree. Finally, the reference airway trees are constructed by taking the union of all correctly extracted branch segments. Fifteen airway tree extraction algorithms from different research groups are evaluated on a diverse set of twenty chest computed tomography (CT) scans of subjects ranging from healthy volunteers to patients with severe pathologies, scanned at different sites, with different CT scanner brands, models, and scanning protocols. Three performance measures covering different aspects of segmentation quality were computed for all participating algorithms. Results from the evaluation showed that no single algorithm could extract more than an average of 74% of the total length of all branches in the reference standard, indicating substantial differences between the algorithms. A fusion scheme that obtained superior results is presented, demonstrating that there is complementary information provided by the different algorithms and there is still room for further improvements in airway segmentation algorithms.},
	author = {Lo, Pechin and van Ginneken, Bram and Reinhardt, Joseph M and Yavarna, Tarunashree and de Jong, Pim A and Irving, Benjamin and Fetita, Catalin and Ortner, Margarete and Pinho, R{\^o}mulo and Sijbers, Jan and Feuerstein, Marco and Fabija{\'n}ska, Anna and Bauer, Christian and Beichel, Reinhard and Mendoza, Carlos S and Wiemker, Rafael and Lee, Jaesung and Reeves, Anthony P and Born, Silvia and Weinheimer, Oliver and van Rikxoort, Eva M and Tschirren, Juerg and Mori, Ken and Odry, Benjamin and Naidich, David P and Hartmann, Ieneke and Hoffman, Eric A and Prokop, Mathias and Pedersen, Jesper H and de Bruijne, Marleen},
	date-added = {2015-08-23 22:55:47 +0000},
	date-modified = {2015-08-23 22:55:47 +0000},
	doi = {10.1109/TMI.2012.2209674},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Analysis of Variance; Databases, Factual; Humans; Lung; Radiographic Image Enhancement; Tomography, X-Ray Computed; Trachea},
	month = {Nov},
	number = {11},
	pages = {2093-107},
	pmid = {22855226},
	pst = {ppublish},
	title = {Extraction of airways from CT (EXACT'09)},
	volume = {31},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2012.2209674}}

@article{Gevenois:1996aa,
	abstract = {PURPOSE: To determine whether measurement of the relative area of lung with attenuation coefficients lower than a certain threshold on thin-section computed tomographic (CT) scans obtained during expiration is a valuable method of quantifying the extent of pulmonary emphysema.
MATERIALS AND METHODS: Eighty-nine patients underwent CT (with 1-mm collimation) preoperatively during inspiration and expiration. Relative areas of lung with attenuation coefficients lower than various thresholds were calculated. These relative areas were compared with areas found macroscopically to have emphysema (59 patients [51 men, eight women; aged 40-77 years]) and with two microscopic indices (35 patients [29 men, six women; aged 42-77 years]) assessed on the resected specimens.
RESULTS: The valid expiratory CT thresholds were found to be -820 and 910 HU for microscopic and macroscopic emphysema, respectively. However, results of stepwise multiple regression analyses showed that the inspiratory threshold of -950 HU was superior for both macroscopically and microscopically quantified emphysema. The correlation coefficients in expiratory CT were higher for the pulmonary volumes but similar for the diffusing capacity.
CONCLUSION: Expiratory quantitative CT is not as accurate as inspiratory CT for quantifying pulmonary emphysema and probably reflects air trapping more than reduction in the alveolar wall surface.},
	author = {Gevenois, P A and De Vuyst, P and Sy, M and Scillia, P and Chaminade, L and de Maertelaer, V and Zanen, J and Yernault, J C},
	date-added = {2015-08-23 22:10:53 +0000},
	date-modified = {2015-08-23 22:10:53 +0000},
	doi = {10.1148/radiology.199.3.8638012},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Adult; Aged; Female; Humans; Lung; Male; Middle Aged; Pulmonary Emphysema; Regression Analysis; Respiration; Respiratory Function Tests; Statistics, Nonparametric; Tomography, X-Ray Computed},
	month = {Jun},
	number = {3},
	pages = {825-9},
	pmid = {8638012},
	pst = {ppublish},
	title = {Pulmonary emphysema: quantitative CT during expiration},
	volume = {199},
	year = {1996},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiology.199.3.8638012}}

@article{Stolk:2007aa,
	abstract = {In patients with airflow limitation caused by cigarette smoking, lung density measured by computed tomography is strongly correlated with quantitative pathology scores of emphysema, but the ability of lung densitometry to detect progression of emphysema is disputed. We assessed the sensitivity of lung densitometry as a parameter of disease progression of emphysema in comparison to FEV(1) and gas transfer. At study baseline and after 30 months we measured computed tomography (CT)-derived lung density, spirometry and carbon monoxide diffusion coefficient in 144 patients with chronic obstructive pulmonary disease (COPD) in five different centers. Annual change in lung density was 1.31 g/L/year (CI 95%: -2.12 to -0.50 HU, p=0.0015, 39.5 mL/year (CI 95%: -100.0-21.0 mL, p=0.2) for FEV(1) (-39.5 mL) and 24.3 micromol/min/kPa/L/year for gas transfer (CI 95%: -61.0-12.5 micromol/min/kPa/L/year, p=0.2). Signal-to-noise ratio (mean change divided by standard error of the change) for the detection of annual change was 3.2 for lung densitometry, but 1.3 for both FEV(1) and gas diffusion. We conclude that detection of progression of emphysema was found to be 2.5-fold more sensitive using lung densitometry than by using currently recommended lung function parameters. Our results support CT scan as an efficacious test for novel drugs for emphysema.},
	author = {Stolk, Jan and Putter, Hein and Bakker, Els M and Shaker, Saher B and Parr, David G and Piitulainen, Eeva and Russi, Erich W and Grebski, Elzbieta and Dirksen, Asger and Stockley, Robert A and Reiber, Johan H C and Stoel, Berend C},
	date-added = {2015-08-23 22:02:29 +0000},
	date-modified = {2015-08-23 22:02:29 +0000},
	doi = {10.1016/j.rmed.2007.04.016},
	journal = {Respir Med},
	journal-full = {Respiratory medicine},
	mesh = {Absorptiometry, Photon; Adult; Aged; Carbon Monoxide; Disease Progression; Female; Forced Expiratory Volume; Humans; Longitudinal Studies; Male; Middle Aged; Pulmonary Emphysema; Spirometry; Tomography, X-Ray Computed; alpha 1-Antitrypsin Deficiency},
	month = {Sep},
	number = {9},
	pages = {1924-30},
	pmid = {17644366},
	pst = {ppublish},
	title = {Progression parameters for emphysema: a clinical investigation},
	volume = {101},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.rmed.2007.04.016}}

@article{Warfield:2004aa,
	abstract = {Characterizing the performance of image segmentation approaches has been a persistent challenge. Performance analysis is important since segmentation algorithms often have limited accuracy and precision. Interactive drawing of the desired segmentation by human raters has often been the only acceptable approach, and yet suffers from intra-rater and inter-rater variability. Automated algorithms have been sought in order to remove the variability introduced by raters, but such algorithms must be assessed to ensure they are suitable for the task. The performance of raters (human or algorithmic) generating segmentations of medical images has been difficult to quantify because of the difficulty of obtaining or estimating a known true segmentation for clinical data. Although physical and digital phantoms can be constructed for which ground truth is known or readily estimated, such phantoms do not fully reflect clinical images due to the difficulty of constructing phantoms which reproduce the full range of imaging characteristics and normal and pathological anatomical variability observed in clinical data. Comparison to a collection of segmentations by raters is an attractive alternative since it can be carried out directly on the relevant clinical imaging data. However, the most appropriate measure or set of measures with which to compare such segmentations has not been clarified and several measures are used in practice. We present here an expectation-maximization algorithm for simultaneous truth and performance level estimation (STAPLE). The algorithm considers a collection of segmentations and computes a probabilistic estimate of the true segmentation and a measure of the performance level represented by each segmentation. The source of each segmentation in the collection may be an appropriately trained human rater or raters, or may be an automated segmentation algorithm. The probabilistic estimate of the true segmentation is formed by estimating an optimal combination of the segmentations, weighting each segmentation depending upon the estimated performance level, and incorporating a prior model for the spatial distribution of structures being segmented as well as spatial homogeneity constraints. STAPLE is straightforward to apply to clinical imaging data, it readily enables assessment of the performance of an automated image segmentation algorithm, and enables direct comparison of human rater and algorithm performance.},
	author = {Warfield, Simon K and Zou, Kelly H and Wells, William M},
	date-added = {2015-08-23 20:06:16 +0000},
	date-modified = {2015-08-23 20:06:16 +0000},
	doi = {10.1109/TMI.2004.828354},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Brain; Decision Making, Computer-Assisted; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Infant, Newborn; Magnetic Resonance Imaging; Markov Chains; Models, Statistical; Observer Variation; Phantoms, Imaging; Reproducibility of Results; Sensitivity and Specificity},
	month = {Jul},
	number = {7},
	pages = {903-21},
	pmc = {PMC1283110},
	pmid = {15250643},
	pst = {ppublish},
	title = {Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation},
	volume = {23},
	year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2004.828354}}

@inproceedings{Tustison:2013ad,
	author = {Nicholas J. Tustison and Benjamin Contrella and Talissa A. Altes and Brian B. Avants and Eduard E. de Lange and John P. Mugler},
	booktitle = {Proc. SPIE 8672, Medical Imaging 2013: Biomedical Applications in Molecular, Structural, and Functional Imaging},
	date-added = {2015-08-23 02:43:04 +0000},
	date-modified = {2015-08-23 02:45:02 +0000},
	month = {March},
	title = {Longitudinal assessment of treatment effects on pulmonary ventilation using 1H/3He MRI multivariate templates},
	year = {2013}}

@article{Manjon:2010aa,
	abstract = {PURPOSE: To adapt the so-called nonlocal means filter to deal with magnetic resonance (MR) images with spatially varying noise levels (for both Gaussian and Rician distributed noise).
MATERIALS AND METHODS: Most filtering techniques assume an equal noise distribution across the image. When this assumption is not met, the resulting filtering becomes suboptimal. This is the case of MR images with spatially varying noise levels, such as those obtained by parallel imaging (sensitivity-encoded), intensity inhomogeneity-corrected images, or surface coil-based acquisitions. We propose a new method where information regarding the local image noise level is used to adjust the amount of denoising strength of the filter. Such information is automatically obtained from the images using a new local noise estimation method.
RESULTS: The proposed method was validated and compared with the standard nonlocal means filter on simulated and real MRI data showing an improved performance in all cases.
CONCLUSION: The new noise-adaptive method was demonstrated to outperform the standard filter when spatially varying noise is present in the images.},
	author = {Manj\'{o}n, Jos\'{e} V and Coup\'{e}, Pierrick and Mart\'{i}-Bonmat\'{i}, Luis and Collins, D Louis and Robles, Montserrat},
	date-added = {2015-08-14 03:25:52 +0000},
	date-modified = {2020-10-17 17:32:58 -0700},
	doi = {10.1002/jmri.22003},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Algorithms; Artifacts; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Phantoms, Imaging; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted},
	month = {Jan},
	number = {1},
	pages = {192-203},
	pmid = {20027588},
	pst = {ppublish},
	title = {Adaptive non-local means denoising of {MR} images with spatially varying noise levels},
	volume = {31},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.22003}}

@article{Das:2009aa,
	abstract = {Cortical thickness is an important biomarker for image-based studies of the brain. A diffeomorphic registration based cortical thickness (DiReCT) measure is introduced where a continuous one-to-one correspondence between the gray matter-white matter interface and the estimated gray matter-cerebrospinal fluid interface is given by a diffeomorphic mapping in the image space. Thickness is then defined in terms of a distance measure between the interfaces of this sheet like structure. This technique also provides a natural way to compute continuous estimates of thickness within buried sulci by preventing opposing gray matter banks from intersecting. In addition, the proposed method incorporates neuroanatomical constraints on thickness values as part of the mapping process. Evaluation of this method is presented on synthetic images. As an application to brain images, a longitudinal study of thickness change in frontotemporal dementia (FTD) spectrum disorder is reported.},
	author = {Das, Sandhitsu R and Avants, Brian B and Grossman, Murray and Gee, James C},
	date-added = {2015-07-30 01:14:16 +0000},
	date-modified = {2015-07-30 01:14:16 +0000},
	doi = {10.1016/j.neuroimage.2008.12.016},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Aged; Algorithms; Brain Mapping; Cerebral Cortex; Dementia; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Middle Aged},
	month = {Apr},
	number = {3},
	pages = {867-79},
	pmc = {PMC2836782},
	pmid = {19150502},
	pst = {ppublish},
	title = {Registration based cortical thickness measurement},
	volume = {45},
	year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2008.12.016}}

@article{Hoffman:2015aa,
	abstract = {Pulmonary x-ray computed tomographic (CT) and magnetic resonance imaging (MRI) research and development has been motivated, in part, by the quest to subphenotype common chronic lung diseases such as chronic obstructive pulmonary disease (COPD). For thoracic CT and MRI, the main COPD research tools, disease biomarkers are being validated that go beyond anatomy and structure to include pulmonary functional measurements such as regional ventilation, perfusion, and inflammation. In addition, there has also been a drive to improve spatial and contrast resolution while at the same time reducing or eliminating radiation exposure. Therefore, this review focuses on our evolving understanding of patient-relevant and clinically important COPD endpoints and how current and emerging MRI and CT tools and measurements may be exploited for their identification, quantification, and utilization. Since reviews of the imaging physics of pulmonary CT and MRI and reviews of other COPD imaging methods were previously published and well-summarized, we focus on the current clinical challenges in COPD and the potential of newly emerging MR and CT imaging measurements to address them. Here we summarize MRI and CT imaging methods and their clinical translation for generating reproducible and sensitive measurements of COPD related to pulmonary ventilation and perfusion as well as parenchyma morphology. The key clinical problems in COPD provide an important framework in which pulmonary imaging needs to rapidly move in order to address the staggering burden, costs, as well as the mortality and morbidity associated with COPD. J. Magn. Reson. Imaging 2015.},
	author = {Hoffman, Eric A and Lynch, David A and Barr, R Graham and van Beek, Edwin J R and Parraga, Grace and {IWPFI Investigators}},
	date-added = {2015-07-29 00:32:04 +0000},
	date-modified = {2015-07-29 00:32:27 +0000},
	doi = {10.1002/jmri.25010},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	keywords = {COPD; dual energy CT; phenotypes; pulmonary MRI; pulmonary ventilation and perfusion; quantitative CT},
	month = {Jul},
	pmid = {26199216},
	pst = {aheadofprint},
	title = {Pulmonary {CT} and {MRI} phenotypes that help explain chronic pulmonary obstruction disease pathophysiology and outcomes},
	year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.25010}}

@article{Tustison:2008aa,
	author = {Tustison, Nicholas J and Gee, James C},
	date-added = {2015-07-23 23:41:39 +0000},
	date-modified = {2015-07-23 23:41:39 +0000},
	journal = {Insight Journal},
	title = {Run-length matrices for texture analysis},
	year = {2008}}

@article{Tustison:2009aa,
	author = {Tustison, Nicholas J and Gee, James C},
	date-added = {2015-07-23 23:34:13 +0000},
	date-modified = {2015-07-23 23:42:17 +0000},
	journal = {Insight Journal},
	title = {Stochastic fractal dimension image},
	year = {2009}}

@incollection{Tustison:2015ab,
	author = {Tustison, Nicholas J and Yang, Yang and Salerno, Michael},
	booktitle = {Statistical Atlases and Computational Models of the Heart - Imaging and Modelling Challenges},
	date-added = {2015-07-23 01:33:49 +0000},
	date-modified = {2015-07-23 01:34:55 +0000},
	doi = {10.1007/978-3-319-14678-2_1},
	editor = {Camara, Oscar and Mansi, Tommaso and Pop, Mihaela and Rhode, Kawal and Sermesant, Maxime and Young, Alistair},
	isbn = {978-3-319-14677-5},
	keywords = {ANTs; Image registration; Motion estimation; Myocardial perfusion},
	language = {English},
	pages = {3-12},
	publisher = {Springer International Publishing},
	series = {Lecture Notes in Computer Science},
	title = {Advanced Normalization Tools for Cardiac Motion Correction},
	url = {http://dx.doi.org/10.1007/978-3-319-14678-2_1},
	volume = {8896},
	year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-14678-2_1}}

@inproceedings{Tustison:2012aa,
	author = {Nicholas J. Tustison and Gang Song and James C. Gee and Brian B. Avants},
	booktitle = {Evaluation of Methods for Pulmonary Image Registration ({EMPIRE10})},
	date-added = {2015-07-23 01:22:31 +0000},
	date-modified = {2020-06-02 19:46:12 -0700},
	title = {Two Greedy {SyN} Variants for Pulmonary Image Registration},
	year = {2012}}

@article{Rikxoort:2013aa,
	abstract = {Computed tomography (CT) is the modality of choice for imaging the lungs in vivo. Sub-millimeter isotropic images of the lungs can be obtained within seconds, allowing the detection of small lesions and detailed analysis of disease processes. The high resolution of thoracic CT and the high prevalence of lung diseases require a high degree of automation in the analysis pipeline. The automated segmentation of pulmonary structures in thoracic CT has been an important research topic for over a decade now. This systematic review provides an overview of current literature. We discuss segmentation methods for the lungs, the pulmonary vasculature, the airways, including airway tree construction and airway wall segmentation, the fissures, the lobes and the pulmonary segments. For each topic, the current state of the art is summarized, and topics for future research are identified.},
	author = {van Rikxoort, Eva M and van Ginneken, Bram},
	date-added = {2015-07-23 01:19:55 +0000},
	date-modified = {2015-07-23 01:19:55 +0000},
	doi = {10.1088/0031-9155/58/17/R187},
	journal = {Phys Med Biol},
	journal-full = {Physics in medicine and biology},
	mesh = {Automation; Blood Vessels; Humans; Image Processing, Computer-Assisted; Lung; Radiography, Thoracic; Tomography, X-Ray Computed},
	month = {Sep},
	number = {17},
	pages = {R187-220},
	pmid = {23956328},
	pst = {ppublish},
	title = {Automated segmentation of pulmonary structures in thoracic computed tomography scans: a review},
	volume = {58},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0031-9155/58/17/R187}}

@webpage{Rothwell:2008aa,
	date-added = {2015-07-23 01:16:12 +0000},
	date-modified = {2015-07-23 01:17:36 +0000},
	lastchecked = {July 22, 2015},
	month = {Aug},
	url = {http://fsmsh.com/2845},
	year = {2008},
	Bdsk-Url-1 = {http://fsmsh.com/2845}}

@inproceedings{Yunwen:2003aa,
	author = {Yunwen, Ye and Kishida, K.},
	booktitle = {Software Engineering, 2003. Proceedings. 25th International Conference on},
	date-added = {2015-07-23 01:13:58 +0000},
	date-modified = {2015-07-23 01:14:25 +0000},
	doi = {10.1109/ICSE.2003.1201220},
	issn = {0270-5257},
	keywords = {human factors;public domain software;software engineering;learning theory;legitimate peripheral participation;motivation;open source software development;software engineering;Collaboration;Collaborative software;Computer languages;Computer science;Open source software;Software engineering},
	month = {May},
	pages = {419-429},
	title = {Toward an understanding of the motivation of open source software developers},
	year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICSE.2003.1201220}}

@inproceedings{Lo:2009aa,
	author = {Lo, P. and van Ginneken, Bram and Reinhardt, Joseph M and de Bruijne, Marleen},
	booktitle = {The Second International Workshop on Pulmonary Image Analysis},
	date-added = {2015-07-22 23:32:12 +0000},
	date-modified = {2015-07-22 23:34:32 +0000},
	title = {Extraction of airways from {CT} ({EXACT} '09)},
	year = {2009}}

@article{Sluimer:2006aa,
	abstract = {Current computed tomography (CT) technology allows for near isotropic, submillimeter resolution acquisition of the complete chest in a single breath hold. These thin-slice chest scans have become indispensable in thoracic radiology, but have also substantially increased the data load for radiologists. Automating the analysis of such data is, therefore, a necessity and this has created a rapidly developing research area in medical imaging. This paper presents a review of the literature on computer analysis of the lungs in CT scans and addresses segmentation of various pulmonary structures, registration of chest scans, and applications aimed at detection, classification and quantification of chest abnormalities. In addition, research trends and challenges are identified and directions for future research are discussed.},
	author = {Sluimer, Ingrid and Schilham, Arnold and Prokop, Mathias and van Ginneken, Bram},
	date-added = {2015-07-22 23:26:24 +0000},
	date-modified = {2015-07-22 23:26:24 +0000},
	doi = {10.1109/TMI.2005.862753},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Animals; Artificial Intelligence; Humans; Image Enhancement; Imaging, Three-Dimensional; Information Storage and Retrieval; Lung; Lung Diseases; Pattern Recognition, Automated; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed},
	month = {Apr},
	number = {4},
	pages = {385-405},
	pmid = {16608056},
	pst = {ppublish},
	title = {Computer analysis of computed tomography scans of the lung: a survey},
	volume = {25},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2005.862753}}

@article{Rosas:2011aa,
	abstract = {BACKGROUND: Automated methods to quantify interstitial lung disease (ILD) on high-resolution CT (HRCT) scans in people at risk for pulmonary fibrosis have not been developed and validated.
METHODS: Cohorts with familial pulmonary fibrosis (n = 126) or rheumatoid arthritis with and without ILD (n = 86) were used to develop and validate a computer program capable of quantifying ILD on HRCT scans, which imaged the lungs semicontinuously from the apices to the lung bases during end-inspiration in the prone position. This method uses segmentation, texture analysis, training, classification, and grading to score ILD.
RESULTS: Quantification of HRCT scan findings of ILD using an automated computer program correlated with radiologist readings and detected disease of varying severity in a derivation cohort with familial pulmonary fibrosis or their first-degree relatives. This algorithm was validated in an independent cohort of subjects with rheumatoid arthritis with and without ILD. Automated classification of HRCT scans as normal or ILD was significant in the derivation and validation cohorts (P < .001 and P < .001, respectively). Areas under receiver operating characteristic curves performed independently for each group were 0.888 for the derivation cohort and 0.885 for the validation cohort. Pulmonary function test results, including FVC and diffusion capacity, correlated with computer-generated HRCT scan scores for ILD (r = -0.483 and r = -0.532, respectively).
CONCLUSIONS: Automated computer scoring of HRCT scans can objectively identify ILD and potentially quantify radiographic severity of lung disease in populations at risk for pulmonary fibrosis.},
	author = {Rosas, Ivan O and Yao, Jianhua and Avila, Nilo A and Chow, Catherine K and Gahl, William A and Gochuico, Bernadette R},
	date-added = {2015-07-22 23:21:55 +0000},
	date-modified = {2015-07-23 01:55:37 +0000},
	doi = {10.1378/chest.10-2545},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Arthritis, Rheumatoid; Case-Control Studies; Cohort Studies; Disease Progression; Female; Humans; Lung Diseases, Interstitial; Male; Middle Aged; Pattern Recognition, Automated; Pulmonary Fibrosis; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Respiratory Function Tests; Risk Assessment; Severity of Illness Index; Tomography, X-Ray Computed},
	month = {Dec},
	number = {6},
	pages = {1590-7},
	pmc = {PMC3231958},
	pmid = {21622544},
	pst = {ppublish},
	title = {Automated quantification of high-resolution {CT} scan findings in individuals at risk for pulmonary fibrosis},
	volume = {140},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.10-2545}}

@article{DeBoer:2014aa,
	abstract = {BACKGROUND: Computer analysis of high-resolution CT (HRCT) scans may improve the assessment of structural lung injury in children with cystic fibrosis (CF). The goal of this cross-sectional pilot study was to validate automated, observer-independent image analysis software to establish objective, simple criteria for bronchiectasis and air trapping.
METHODS: HRCT scans of the chest were performed in 35 children with CF and compared with scans from 12 disease control subjects. Automated image analysis software was developed to count visible airways on inspiratory images and to measure a low attenuation density (LAD) index on expiratory images. Among the children with CF, relationships among automated measures, Brody HRCT scanning scores, lung function, and sputum markers of inflammation were assessed.
RESULTS: The number of total, central, and peripheral airways on inspiratory images and LAD (%) on expiratory images were significantly higher in children with CF compared with control subjects. Among subjects with CF, peripheral airway counts correlated strongly with Brody bronchiectasis scores by two raters (r=0.86, P<.0001; r=0.91, P<.0001), correlated negatively with lung function, and were positively associated with sputum free neutrophil elastase activity. LAD (%) correlated with Brody air trapping scores (r=0.83, P<.0001; r=0.69, P<.0001) but did not correlate with lung function or sputum inflammatory markers.
CONCLUSIONS: Quantitative airway counts and LAD (%) on HRCT scans appear to be useful surrogates for bronchiectasis and air trapping in children with CF. Our automated methodology provides objective quantitative measures of bronchiectasis and air trapping that may serve as end points in CF clinical trials.},
	author = {DeBoer, Emily M and Swiercz, Waldemar and Heltshe, Sonya L and Anthony, Margaret M and Szefler, Paul and Klein, Rebecca and Strain, John and Brody, Alan S and Sagel, Scott D},
	date-added = {2015-07-22 23:20:49 +0000},
	date-modified = {2015-07-23 01:48:51 +0000},
	doi = {10.1378/chest.13-0588},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Automation; Bronchiectasis; Cystic Fibrosis; Disease Progression; Female; Follow-Up Studies; Humans; Male; Middle Aged; Prognosis; Respiratory Function Tests; Retrospective Studies; Survival Rate; Time Factors; Tomography, X-Ray Computed; United States; Vital Capacity},
	month = {Mar},
	number = {3},
	pages = {593-603},
	pmc = {PMC3941250},
	pmid = {24114359},
	pst = {ppublish},
	title = {Automated {CT} scan scores of bronchiectasis and air trapping in cystic fibrosis},
	volume = {145},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.13-0588}}

@article{Doel:2015aa,
	abstract = {The computational detection of pulmonary lobes from CT images is a challenging segmentation problem with important respiratory health care applications, including surgical planning and regional image analysis. Several authors have proposed automated algorithms and we present a methodological review. These algorithms share a number of common stages and we consider each stage in turn, comparing the methods applied by each author and discussing their relative strengths. No standard method has yet emerged and none of the published methods have been demonstrated across a full range of clinical pathologies and imaging protocols. We discuss how improved methods could be developed by combining different approaches, and we use this to propose a workflow for the development of new algorithms.},
	author = {Doel, Tom and Gavaghan, David J and Grau, Vicente},
	date-added = {2015-07-22 23:17:03 +0000},
	date-modified = {2015-07-23 01:49:02 +0000},
	doi = {10.1016/j.compmedimag.2014.10.008},
	journal = {Comput Med Imaging Graph},
	journal-full = {Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society},
	keywords = {CT; Fissure; Lobe; Lung; Pulmonary; Segmentation},
	month = {Mar},
	pages = {13-29},
	pmid = {25467805},
	pst = {ppublish},
	title = {Review of automatic pulmonary lobe segmentation methods from {CT}},
	volume = {40},
	year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.compmedimag.2014.10.008}}

@article{Qi:2014aa,
	abstract = {BACKGROUND: Multi-detector Computed Tomography has become an invaluable tool for the diagnosis of chronic respiratory diseases. Based on CT images, the automatic algorithm to detect the fissures and divide the lung into five lobes will help regionally quantify, amongst others, the lung density, texture, airway and, blood vessel structures, ventilation and perfusion.
METHODS: Sagittal adaptive fissure scanning based on the sparseness of the vessels and bronchi is employed to localize the potential fissure region. Following a Hessian matrix based line enhancement filter in the coronal slice, the shortest path is determined by means of Uniform Cost Search. Implicit surface fitting based on Radial Basis Functions is used to extract the fissure surface for lobe segmentation. By three implicit fissure surface functions, the lung is divided into five lobes. The proposed algorithm is tested by 14 datasets. The accuracy is evaluated by the mean ($\pm$S.D.), root mean square, and the maximum of the shortest Euclidian distance from the manually-defined fissure surface to that extracted by the algorithm.
RESULTS: Averaged over all datasets, the mean ($\pm$S.D.), root mean square, and the maximum of the shortest Euclidian distance are 2.05 $\pm$ 1.80, 2.46 and 7.34 mm for the right oblique fissure. The measures are 2.77 $\pm$ 2.12, 3.13 and 7.75 mm for the right horizontal fissure, 2.31 $\pm$ 1.76, 3.25 and 6.83 mm for the left oblique fissure. The fissure detection works for the data with a small lung nodule nearby the fissure and a small lung subpleural nodule. The volume and emphysema index of each lobe can be calculated. The algorithm is very fast, e.g., to finish the fissure detection and fissure extension for the dataset with 320 slices only takes around 50 seconds.
CONCLUSIONS: The sagittal adaptive fissure scanning can localize the potential fissure regions quickly. After the potential region is enhanced by a Hessian based line enhancement filter, Uniform Cost Search can extract the fissures successfully in 2D. Surface fitting is able to obtain three implicit surface functions for each dataset. The current algorithm shows good accuracy, robustness and speed, may help locate the lesions into each lobe and analyze them regionally.},
	author = {Qi, Shouliang and van Triest, Han J W and Yue, Yong and Xu, Mingjie and Kang, Yan},
	date-added = {2015-07-22 23:15:35 +0000},
	date-modified = {2015-07-23 01:54:57 +0000},
	doi = {10.1186/1475-925X-13-59},
	journal = {Biomed Eng Online},
	journal-full = {Biomedical engineering online},
	mesh = {Adult; Aged; Aged, 80 and over; Algorithms; Automation; Female; Humans; Image Processing, Computer-Assisted; Lung; Male; Middle Aged; Radiography, Thoracic; Tomography, X-Ray Computed},
	pages = {59},
	pmc = {PMC4022789},
	pmid = {24886031},
	pst = {epublish},
	title = {Automatic pulmonary fissure detection and lobe segmentation in {CT} chest images},
	volume = {13},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1186/1475-925X-13-59}}

@article{Korfiatis:2011aa,
	abstract = {The automated segmentation of vessel tree structures is a crucial preprocessing stage in computer aided diagnosis (CAD) schemes of interstitial lung disease (ILD) patterns in multidetector computed tomography (MDCT). The accuracy of such preprocessing stages is expected to influence the accuracy of lung CAD schemes. Although algorithms aimed at improving the accuracy of lung fields segmentation in presence of ILD have been reported, the corresponding vessel tree segmentation stage is under-researched. Furthermore, previously reported vessel tree segmentation methods have only dealt with normal lung parenchyma. In this paper, an automated vessel tree segmentation scheme is proposed, adapted to the presence of pathologies affecting lung parenchyma. The first stage of the method accounts for a recently proposed method utilizing a 3-D multiscale vessel enhancement filter based on eigenvalue analysis of the Hessian matrix and on unsupervised segmentation. The second stage of the method is a texture-based voxel classification refinement to correct possible over-segmentation. The performance of the proposed scheme, and of the previously reported technique, in vessel tree segmentation was evaluated by means of area overlap (previously reported: 0.715 $\pm$ 0.082, proposed: 0.931 $\pm$ 0.027), true positive fraction (previously reported: 0.968 $\pm$ 0.019, proposed: 0.935 $\pm$ 0.036) and false positive fraction (previously reported: 0.400 $\pm$ 0.181, proposed: 0.074 $\pm$ 0.031) on a dataset of 210 axial slices originating from seven ILD affected patient scans (used for performance evaluation out of 15). The proposed method demonstrated a statistically significantly ( p < 0.05) higher performance as compared to the previously reported vessel tree segmentation technique. The impact of suboptimal vessel tree segmentation in a reticular pattern quantification system is also demonstrated.},
	author = {Korfiatis, Panayiotis D and Kalogeropoulou, Cristina and Karahaliou, Anna N and Kazantzi, Alexandra D and Costaridou, Lena I},
	date-added = {2015-07-22 23:14:31 +0000},
	date-modified = {2015-07-23 01:50:49 +0000},
	doi = {10.1109/TITB.2011.2112668},
	journal = {IEEE Trans Inf Technol Biomed},
	journal-full = {IEEE transactions on information technology in biomedicine : a publication of the IEEE Engineering in Medicine and Biology Society},
	mesh = {Algorithms; Diagnosis, Computer-Assisted; Humans; Image Processing, Computer-Assisted; Lung; Lung Diseases, Interstitial; Middle Aged; Radiographic Image Interpretation, Computer-Assisted; Tomography, X-Ray Computed},
	month = {Mar},
	number = {2},
	pages = {214-20},
	pmid = {21317088},
	pst = {ppublish},
	title = {Vessel tree segmentation in presence of interstitial lung disease in {MDCT}},
	volume = {15},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TITB.2011.2112668}}

@article{Agam:2005aa,
	abstract = {Vessel tree reconstruction in volumetric data is a necessary prerequisite in various medical imaging applications. Specifically, when considering the application of automated lung nodule detection in thoracic computed tomography (CT) scans, vessel trees can be used to resolve local ambiguities based on global considerations and so improve the performance of nodule detection algorithms. In this study, a novel approach to vessel tree reconstruction and its application to nodule detection in thoracic CT scans was developed by using correlation-based enhancement filters and a fuzzy shape representation of the data. The proposed correlation-based enhancement filters depend on first-order partial derivatives and so are less sensitive to noise compared with Hessian-based filters. Additionally, multiple sets of eigenvalues are used so that a distinction between nodules and vessel junctions becomes possible. The proposed fuzzy shape representation is based on regulated morphological operations that are less sensitive to noise. Consequently, the vessel tree reconstruction algorithm can accommodate vessel bifurcation and discontinuities. A quantitative performance evaluation of the enhancement filters and of the vessel tree reconstruction algorithm was performed. Moreover, the proposed vessel tree reconstruction algorithm reduced the number of false positives generated by an existing nodule detection algorithm by 38%.},
	author = {Agam, Gady and Armato, 3rd, Samuel G and Wu, Changhua},
	date-added = {2015-07-22 23:10:59 +0000},
	date-modified = {2015-07-23 01:45:59 +0000},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Adult; Aged; Aged, 80 and over; Algorithms; Angiography; Artificial Intelligence; Cluster Analysis; Female; Fuzzy Logic; Humans; Imaging, Three-Dimensional; Lung; Lung Neoplasms; Male; Middle Aged; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity; Solitary Pulmonary Nodule; Tomography, X-Ray Computed},
	month = {Apr},
	number = {4},
	pages = {486-99},
	pmid = {15822807},
	pst = {ppublish},
	title = {Vessel tree reconstruction in thoracic {CT} scans with application to nodule detection},
	volume = {24},
	year = {2005}}

@article{Nakamura:2008aa,
	abstract = {Remarkable advances in computed tomography (CT) technology geared our research toward investigating the integrative function of the lung and the development of a database of the airway tree incorporating anatomical and functional data with computational models. As part of this project, we are developing the algorithm to construct an anatomically realistic geometric model of airways from CT images. The basic concept of the algorithm is to segment as many airway trees as possible from CT images and later correct quantified parameters based on CT values. CT images are acquired with a 64-channel multidetector CT, and the airway is then extracted from them by the region-growing method while maintaining connectivity. Using this method, we extracted 428 airways up to the 14th branching generation. Although the airway diameters up to the 4th generation showed good agreement with those reported in an autopsy study, those in later generations were all greater than the reported values because of the limited resolution of the CT images. We corrected the errors in diameters by assessing the relationship between the diameter and median value of Hounsfield unit (HU) intensity of each airway; consequently, the diameters up to generation 8 agreed well with the reported values. Based on these results, we conclude that the use of HU-based correction algorithm combined with rough segmentation can be another way to improve data accuracy in the morphometric analysis of airways from CTs.},
	author = {Nakamura, Masanori and Wada, Shigeo and Miki, Takahito and Shimada, Yasuhiro and Suda, Yuji and Tamura, Gen},
	date-added = {2015-07-22 23:10:05 +0000},
	date-modified = {2015-07-23 01:53:39 +0000},
	doi = {10.2170/physiolsci.RP007408},
	journal = {J Physiol Sci},
	journal-full = {The journal of physiological sciences : JPS},
	mesh = {Algorithms; Computer Simulation; Humans; Imaging, Three-Dimensional; Lung; Models, Anatomic; Models, Biological; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Tomography, X-Ray Computed},
	month = {Dec},
	number = {7},
	pages = {493-8},
	pmid = {19055856},
	pst = {ppublish},
	title = {Automated segmentation and morphometric analysis of the human airway tree from multidetector {CT} images},
	volume = {58},
	year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.2170/physiolsci.RP007408}}

@article{Zheng:2007aa,
	abstract = {We developed and tested a fully automated computerized scheme that identifies pulmonary airway sections depicted on computed tomography (CT) images and computes their sizes including the lumen and airway wall areas. The scheme includes four processing modules that (1) segment left and right lung areas, (2) identify airway locations, (3) segment airway walls from neighboring pixels, and (4) compute airway sizes. The scheme uses both a raster scanning and a labeling algorithm complemented by simple classification rules for region size and circularity to automatically search for and identify airway sections of interest. A profile tracking method is used to segment airway walls from neighboring pixels including those associated with dense tissue (i.e., pulmonary arteries) along scanning radial rays. A partial pixel membership method is used to compute airway size. The scheme was tested on ten randomly selected CT studies that included 26 sets of CT images acquired using both low and conventional dose CT examinations with one of four reconstruction algorithms (namely, "bone," "lung," "soft," and "standard" convolution kernels). Three image section thicknesses (1.25, 2.5, and 5 mm) were evaluated. The scheme detected a large number of quantifiable airway sections when the CT images were reconstructed using high spatial frequency convolution kernels. The detection results demonstrated a consistent trend for all test image sets in that as airway lumen size increases, on average the airway wall area increases as well and the wall area percentage decreases. The study suggested that CT images reconstructed using high spatial frequency convolution kernels and thin-section thickness were most amenable to automated detection, reasonable segmentation, and quantified assessment when the airways are close to being perpendicular to the CT image plane.},
	author = {Zheng, Bin and Leader, Joseph K and McMurray, Jessica M and Park, Sang Cheol and Fuhrman, Carl R and Gur, David and Sciurba, Frank C},
	date-added = {2015-07-22 23:09:17 +0000},
	date-modified = {2015-07-23 02:01:09 +0000},
	journal = {Med Phys},
	journal-full = {Medical physics},
	month = {Jul},
	number = {7},
	pages = {2844-52},
	pmid = {17821992},
	pst = {ppublish},
	title = {Automated detection and quantitative assessment of pulmonary airways depicted on {CT} images},
	volume = {34},
	year = {2007}}

@article{Rikxoort:2009aa,
	abstract = {Lung segmentation is a prerequisite for automated analysis of chest CT scans. Conventional lung segmentation methods rely on large attenuation differences between lung parenchyma and surrounding tissue. These methods fail in scans where dense abnormalities are present, which often occurs in clinical data. Some methods to handle these situations have been proposed, but they are too time consuming or too specialized to be used in clinical practice. In this article, a new hybrid lung segmentation method is presented that automatically detects failures of a conventional algorithm and, when needed, resorts to a more complex algorithm, which is expected to produce better results in abnormal cases. In a large quantitative evaluation on a database of 150 scans from different sources, the hybrid method is shown to perform substantially better than a conventional approach at a relatively low increase in computational cost.},
	author = {van Rikxoort, Eva M and de Hoop, Bartjan and Viergever, Max A and Prokop, Mathias and van Ginneken, Bram},
	date-added = {2015-07-22 23:08:28 +0000},
	date-modified = {2015-07-22 23:08:28 +0000},
	journal = {Med Phys},
	journal-full = {Medical physics},
	mesh = {Algorithms; Automation; Databases, Factual; Humans; Lung; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Tomography, X-Ray Computed},
	month = {Jul},
	number = {7},
	pages = {2934-47},
	pmid = {19673192},
	pst = {ppublish},
	title = {Automatic lung segmentation from thoracic computed tomography scans using a hybrid approach with error detection},
	volume = {36},
	year = {2009}}

@article{Wang:2009aa,
	abstract = {PURPOSE: Accurate segmentation of lungs with severe interstitial lung disease (ILD) in thoracic computed tomography (CT) is an important and difficult task in the development of computer-aided diagnosis (CAD) systems. Therefore, we developed in this study a texture analysis-based method for accurate segmentation of lungs with severe ILD in multidetector CT scans.
METHODS: Our database consisted of 76 CT scans, including 31 normal cases and 45 abnormal cases with moderate or severe ILD. The lungs in three selected slices for each CT scan were first manually delineated by a medical physicist, and then confirmed or revised by an expert chest radiologist, and they were used as the reference standard for lung segmentation. To segment the lungs, we first employed a CT value thresholding technique to obtain an initial lung estimate, including normal and mild ILD lung parenchyma. We then used texture-feature images derived from the co-occurrence matrix to further identify abnormal lung regions with severe ILD. Finally, we combined the identified abnormal lung regions with the initial lungs to generate the final lung segmentation result. The overlap rate, volume agreement, mean absolute distance (MAD), and maximum absolute distance (dmax) between the automatically segmented lungs and the reference lungs were employed to evaluate the performance of the segmentation method.
RESULTS: Our segmentation method achieved a mean overlap rate of 96.7%, a mean volume agreement of 98.5%, a mean MAD of 0.84 mm, and a mean dmax of 10.84 mm for all the cases in our database; a mean overlap rate of 97.7%, a mean volume agreement of 99.0%, a mean MAD of 0.66 mm, and a mean dmax of 9.59 mm for the 31 normal cases; and a mean overlap rate of 96.1%, a mean volume agreement of 98.1%, a mean MAD of 0.96 mm, and a mean dmax of 11.71 mm for the 45 abnormal cases with ILD.
CONCLUSIONS: Our lung segmentation method provided accurate segmentation results for abnormal CT scans with severe ILD and would be useful for developing CAD systems for quantification, detection, and diagnosis of ILD.},
	author = {Wang, Jiahui and Li, Feng and Li, Qiang},
	date-added = {2015-07-22 23:07:07 +0000},
	date-modified = {2015-07-23 01:58:52 +0000},
	journal = {Med Phys},
	journal-full = {Medical physics},
	mesh = {Algorithms; Artificial Intelligence; Cluster Analysis; Humans; Lung; Lung Diseases, Interstitial; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed},
	month = {Oct},
	number = {10},
	pages = {4592-9},
	pmc = {PMC2771715},
	pmid = {19928090},
	pst = {ppublish},
	title = {Automated segmentation of lungs with severe interstitial lung disease in {CT}},
	volume = {36},
	year = {2009}}

@article{Prasad:2008aa,
	abstract = {RATIONALE AND OBJECTIVES: Segmentation of lungs using high-resolution computer tomographic images in the setting of diffuse lung diseases is a major challenge in medical image analysis. Threshold-based techniques tend to leave out lung regions that have increased attenuation, such as in the presence of interstitial lung disease. In contrast, streak artifacts can cause the lung segmentation to "leak" into the chest wall. The purpose of this work was to perform segmentation of the lungs using a technique that selects an optimal threshold for a given patient by comparing the curvature of the lung boundary to that of the ribs.
METHODS: Our automated technique goes beyond fixed threshold-based approaches to include lung boundary curvature features. One would expect the curvature of the ribs and the curvature of the lung boundary around the ribs to be very close. Initially, the ribs are segmented by applying a threshold algorithm followed by morphologic operations. The lung segmentation scheme uses a multithreshold iterative approach. The threshold value is verified until the curvature of the ribs and the curvature of the lung boundary are closely matched. The curve of the ribs is represented using polynomial interpolation, and the lung boundary is matched in such a way that there is minimal deviation from this representation. Performance of this technique was compared with conventional (fixed threshold) lung segmentation techniques on 25 subjects using a volumetric overlap fraction measure.
RESULTS: The performance of the rib segmentation technique was significantly different from conventional techniques with an average higher mean volumetric overlap fraction of about 5%.
CONCLUSIONS: The technique described here allows for accurate quantification of volumetric computed tomography and more advanced segmentation of abnormal areas.},
	author = {Prasad, Mithun N and Brown, Matthew S and Ahmad, Shama and Abtin, Fereidoun and Allen, Jared and da Costa, Irene and Kim, Hyun J and McNitt-Gray, Michael F and Goldin, Jonathan G},
	date-added = {2015-07-22 23:06:05 +0000},
	date-modified = {2015-07-22 23:06:05 +0000},
	doi = {10.1016/j.acra.2008.02.004},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Algorithms; Humans; Lung; Lung Diseases; Ribs; Tomography, X-Ray Computed},
	month = {Sep},
	number = {9},
	pages = {1173-80},
	pmid = {18692759},
	pst = {ppublish},
	title = {Automatic segmentation of lung parenchyma in the presence of diseases based on curvature of ribs},
	volume = {15},
	year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2008.02.004}}

@article{De-Nunzio:2011aa,
	abstract = {A fully automated and three-dimensional (3D) segmentation method for the identification of the pulmonary parenchyma in thorax X-ray computed tomography (CT) datasets is proposed. It is meant to be used as pre-processing step in the computer-assisted detection (CAD) system for malignant lung nodule detection that is being developed by the Medical Applications in a Grid Infrastructure Connection (MAGIC-5) Project. In this new approach the segmentation of the external airways (trachea and bronchi), is obtained by 3D region growing with wavefront simulation and suitable stop conditions, thus allowing an accurate handling of the hilar region, notoriously difficult to be segmented. Particular attention was also devoted to checking and solving the problem of the apparent 'fusion' between the lungs, caused by partial-volume effects, while 3D morphology operations ensure the accurate inclusion of all the nodules (internal, pleural, and vascular) in the segmented volume. The new algorithm was initially developed and tested on a dataset of 130 CT scans from the Italung-CT trial, and was then applied to the ANODE09-competition images (55 scans) and to the LIDC database (84 scans), giving very satisfactory results. In particular, the lung contour was adequately located in 96% of the CT scans, with incorrect segmentation of the external airways in the remaining cases. Segmentation metrics were calculated that quantitatively express the consistency between automatic and manual segmentations: the mean overlap degree of the segmentation masks is 0.96 $\pm$ 0.02, and the mean and the maximum distance between the mask borders (averaged on the whole dataset) are 0.74 $\pm$ 0.05 and 4.5 $\pm$ 1.5, respectively, which confirms that the automatic segmentations quite correctly reproduce the borders traced by the radiologist. Moreover, no tissue containing internal and pleural nodules was removed in the segmentation process, so that this method proved to be fit for the use in the framework of a CAD system. Finally, in the comparison with a two-dimensional segmentation procedure, inter-slice smoothness was calculated, showing that the masks created by the 3D algorithm are significantly smoother than those calculated by the 2D-only procedure.},
	author = {De Nunzio, Giorgio and Tommasi, Eleonora and Agrusti, Antonella and Cataldo, Rosella and De Mitri, Ivan and Favetta, Marco and Maglio, Silvio and Massafra, Andrea and Quarta, Maurizio and Torsello, Massimo and Zecca, Ilaria and Bellotti, Roberto and Tangaro, Sabina and Calvini, Piero and Camarlinghi, Niccol{\`o} and Falaschi, Fabio and Cerello, Piergiorgio and Oliva, Piernicola},
	date-added = {2015-07-22 23:05:01 +0000},
	date-modified = {2015-07-23 01:48:29 +0000},
	doi = {10.1007/s10278-009-9229-1},
	journal = {J Digit Imaging},
	journal-full = {Journal of digital imaging},
	mesh = {Algorithms; Humans; Lung; Lung Neoplasms; Radiographic Image Interpretation, Computer-Assisted; Retrospective Studies; Tomography, X-Ray Computed},
	month = {Feb},
	number = {1},
	pages = {11-27},
	pmc = {PMC3046791},
	pmid = {19826872},
	pst = {ppublish},
	title = {Automatic lung segmentation in {CT} images with accurate handling of the hilar region},
	volume = {24},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10278-009-9229-1}}

@inproceedings{Avants:2010ab,
	author = {Avants, Brian B and Klein, Arno and Tustison, Nicholas J and Woo, J and Gee, James C},
	booktitle = {16th annual meeting for the Organization of Human Brain Mapping},
	date-added = {2015-07-22 22:57:19 +0000},
	date-modified = {2015-07-22 22:59:24 +0000},
	title = {Evaluation of open-access, automated brain extraction methods on multi-site multi-disorder data},
	year = {2010}}

@article{Zaporozhan:2004aa,
	abstract = {OBJECTIVE: To develop and evaluate a postprocessing tool to quantify ventilated split-lung volumes on the basis of (3)He-MRI and to apply it in patients after single-lung transplantation (SLTX). High-resolution CT (HRCT) was employed as a reference modality providing split air-filled lung volumes. Lung volumes derived from pulmonary function test results served as clinical parameters and were used as the "gold standard."
MATERIAL AND METHODS: Eight patients (mean age, 54 years) with emphysema and six patients (mean age, 58 years) with idiopathic pulmonary fibrosis. All patients were evaluated following SLTX. HRCT was performed during inspiration (slice thickness, 1 mm; increment, 10 mm). For correlation with (3)He-MRI, HRCT images were reconstructed in coronal orientation to match the same anatomic levels. Aerated lung was determined by threshold-based segmentation of CT. (3)He-MRI was performed on a 1.5-T scanner using a two-dimensional, fast low-angle shot sequence in coronal orientation covering the whole lung after inhalation of a 300-mL bolus of hyperpolarized (3)He gas followed by normal room air for the rest of the tidal volume. Lung segmentation on (3)He-MRI was done using different thresholds.
RESULTS: In emphysematous patients, (3)He-MRI showed excellent correlation (r = 0.9) with vital capacity, while CT correlated (r = 0.8) with total lung capacity. (3)He-MRI correlated well with CT (r > 0.8) for grafts and native fibrotic lungs. In emphysematous lungs, MRI showed a good correlation (r = 0.7) with the nonemphysematous lung volume from CT. Increasing thresholds in (3)He-MRI reveal differences between aerated and ventilated lung areas with a different distribution in emphysema and fibrosis.
CONCLUSIONS: (3)He-MRI is superior to CT in emphysema to demonstrate ventilated lung areas that participate in gas exchange. In fibrosis, (3)He-MRI and CT have a similar impact. The decrease pattern and the intraindividual ratio between ventilation of native and transplanted lungs will have to be investigated as a new surrogate for the ventilatory follow-up in patients undergoing SLTX.},
	author = {Zaporozhan, Julia and Ley, Sebastian and Gast, Klaus Kurt and Schmiedeskamp, J{\"o}rg and Biedermann, Alexander and Eberle, Balthasar and Kauczor, Hans-Ulrich},
	date-added = {2015-07-22 02:37:15 +0000},
	date-modified = {2015-07-23 02:00:48 +0000},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Female; Helium; Humans; Image Processing, Computer-Assisted; Isotopes; Lung; Lung Transplantation; Magnetic Resonance Imaging; Male; Middle Aged; Pulmonary Emphysema; Pulmonary Fibrosis; Pulmonary Gas Exchange; Respiratory Function Tests; Tomography, X-Ray Computed; Total Lung Capacity; Vital Capacity},
	month = {Jan},
	number = {1},
	pages = {173-81},
	pmid = {14718438},
	pst = {ppublish},
	title = {Functional analysis in single-lung transplant recipients: a comparative study of high-resolution {CT}, {3He-MRI}, and pulmonary function tests},
	volume = {125},
	year = {2004}}

@article{Zaporozhan:2005aa,
	abstract = {PURPOSES: The aim of the study was to use three-dimensional high-resolution CT scan data sets in inspiration and expiration for the quantitative evaluation of emphysema. Using an advanced dedicated semiautomatic analysis tool, the functional inspiratory/expiratory shifts of emphysema volume and clusters were quantified. The pulmonary function test (PFT) served as the clinical "gold standard."
MATERIALS AND METHODS: Thirty-one patients (9 women and 22 men; mean [+/- SD] age, 60 +/- 8 years) who had severe emphysema due to COPD (Global Initiative for Chronic Obstructive Lung Disease [GOLD] class III and IV) were included in the study. All patients underwent paired inspiratory/expiratory multidetector CT scans (slice thickness, 1/0.8 mm) and pulmonary function tests (PFTs). CT scan data were analyzed with self-written emphysema detection solftware. It provides lung volume (LV), emphysema volume (EV), emphysema index (EI), and four clusters of emphysema with different volumes (from 2, 8, 65, and 120 mm(3)). These results were correlated with total lung capacity (TLC), intrathoracic gas volume (ITGV), and residual volume (RV) derived from PFT results.
RESULTS: Inspiratory LV correlated with TLC (r = 0.9), expiratory LV with ITGV (r = 0.87), and RV (r = 0.83). Expiratory EV correlated better with ITGV (r = 0.88) and RV (r = 0.93) than with inspiratory EV (r = 0.83 and 0.88, respectively). The mean inspiratory EI was 54 +/- 13%, and it decreased to 43 +/- 15% in expiration. However, the individuals showed a broad spectrum of changes of EI (mean, 11%; range, 1 to 28%), and no differences in inspiratory/expiratory EI and changes in EI or LV were found between GOLD III and GOLD IV patients. In expiration, there was a change from the large emphysema cluster (-37%) to the intermediate cluster (+15%) and small cluster (+13% and +11%, respectively). The change of volume of the large emphysema cluster after expiration correlated well with the changes in LV (r = 0.9), EV (r = 0.99), EI (r = 0.85), and MLD (r = 0.76).
CONCLUSION: Emphysema volumes measured from expiratory MDCT scans better reflect PFT abnormalities in patients with severe emphysema than those from inspiratory scans. Volumetric cluster analysis provided deeper insights into the local hyperinflation and expiratory obstruction of large emphysematous clusters.},
	author = {Zaporozhan, Julia and Ley, Sebastian and Eberhardt, Ralf and Weinheimer, Oliver and Iliyushenko, Svitlana and Herth, Felix and Kauczor, Hans-Ulrich},
	date-added = {2015-07-22 02:37:03 +0000},
	date-modified = {2015-07-23 02:00:59 +0000},
	doi = {10.1378/chest.128.5.3212},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Adult; Aged; Exhalation; Female; Humans; Image Processing, Computer-Assisted; Inhalation; Lung Volume Measurements; Male; Middle Aged; Pulmonary Emphysema; Respiratory Function Tests; Tomography, X-Ray Computed},
	month = {Nov},
	number = {5},
	pages = {3212-20},
	pmid = {16304264},
	pst = {ppublish},
	title = {Paired inspiratory/expiratory volumetric thin-slice {CT} scan for emphysema analysis: comparison of different quantitative evaluations and pulmonary function test},
	volume = {128},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.128.5.3212}}

@article{Yuan:2007aa,
	abstract = {BACKGROUND: To evaluate the effect of radiation dose and scanner manufacturer on quantitative CT scan measurements of lung morphology in smokers.
METHODS: Low-dose and high-dose, inspiratory, multislice CT scans were obtained in 50 subjects at intervals of approximately 6 months (mean [+/- SD] interval, 0.5 +/- 0.2 years). In another 30 subjects, multislice CT scans were acquired first using a GE LightSpeed Ultra (General Electric Healthcare; Milwaukee, WI), followed a mean time of 1.2 +/- 0.4 years later by using a Siemens Sensation 16 scanner (Siemens Medical Solutions; Erlangen, Germany). Custom software was used to measure lung volume, mass, mean density, and the extent of emphysema using threshold cutoffs of -950, -910, and -856 Hounsfield units (HU) and the lowest 15th and 5th percentile points.
RESULTS: The change in radiograph dose significantly affected measurements of emphysema assessed using mean lung density, threshold, or percentile methods. There were also interactions between dose and total lung volume for all of the measurements except the -950-HU threshold and the lowest fifth percentile point. These two emphysema measurements suggest that there was more emphysema found in the CT scans obtained using a lower radiograph dose. Only the mean lung density and -856-HU threshold showed significant effects between CT scanner manufacturers and interactions between total lung volume and scanner. All other measures of lung structure were not different between the two CT scanners.
CONCLUSION: CT scan measurements of very low density lung structures are significantly affected by radiation dose but are less sensitive to the lung volume. Image acquisition parameters including radiation dose, scanner type, and the subject's breath size should be standardized to estimate emphysema severity in longitudinal studies.},
	author = {Yuan, Ren and Mayo, John R and Hogg, James C and Par{\'e}, Peter D and McWilliams, Annette M and Lam, Stephen and Coxson, Harvey O},
	date-added = {2015-07-22 02:36:43 +0000},
	date-modified = {2015-07-23 02:00:27 +0000},
	doi = {10.1378/chest.06-2325},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Densitometry; Equipment Design; Female; Follow-Up Studies; Humans; Male; Middle Aged; Pulmonary Emphysema; Radiation Dosage; Reproducibility of Results; Severity of Illness Index; Tomography, X-Ray Computed; Total Lung Capacity},
	month = {Aug},
	number = {2},
	pages = {617-23},
	pmid = {17573501},
	pst = {ppublish},
	title = {The effects of radiation dose and {CT} manufacturer on measurements of lung densitometry},
	volume = {132},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.06-2325}}

@article{Yoo:2005aa,
	author = {Yoo, Terry S and Metaxas, Dimitris N},
	date-added = {2015-07-22 02:36:25 +0000},
	date-modified = {2015-07-23 02:00:12 +0000},
	doi = {10.1016/j.media.2005.04.008},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Databases, Factual; Diagnostic Imaging; Image Interpretation, Computer-Assisted; Information Dissemination; Medical Informatics Applications; National Library of Medicine (U.S.); Science; Software; United States},
	month = {Dec},
	number = {6},
	pages = {503-6},
	pmid = {16169766},
	pst = {ppublish},
	title = {Open science--combining open data and open source software: medical image analysis with the {I}nsight {T}oolkit},
	volume = {9},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2005.04.008}}

@article{Xu:2006ab,
	abstract = {RATIONALE AND OBJECTIVES: Computer-aided detection algorithms applied to multidetector row CT (MDCT) lung image data sets have the potential to significantly alter clinical practice through the early, quantitative detection of pulmonary pathology. In this project, we have further developed a computer-aided detection tool, the adaptive multiple feature method (AMFM), for the detection of interstitial lung diseases based on MDCT-generated volumetric data.
MATERIALS AND METHODS: We performed MDCT (Siemens Sensation 16 or 64 120 kV, B50f convolution kernel, and <or=0.75-mm slice thickness) on 20 human volunteers recruited from four cohorts studied under an National Institutes of Health-sponsored Bioengineering Research Partnership Grant: 1) normal never smokers; 2) normal smokers; 3) those with emphysema, and 4) those with interstitial lung disease (total: 11 males, 9 females; age range 20-75 years, mean age 40 years). A total of 1,184 volumes of interest (VOIs; 21 x 21 pixels in plane) were marked by a senior radiologist and a senior pulmonologist as emphysema (EMPH, n = 287); ground-glass (GG, n = 147), honeycombing (HC, n = 137), normal nonsmokers (NN, n = 287), and normal smokers (NS, n = 326). For each VOI, we calculated 24 volumetric features, including statistical features (first-order features, run-length, and co-occurrence features), histogram, and fractal features. We compared two methods of classification (a Support Vector Machine (SVM) and a Bayesian classifier) using a 10-fold cross validation method and McNemar's test.
RESULTS: The sensitivity of five patterns in the form of Bayesian/SVM was: EMPH: 91/93%; GG: 89/86%; HC: 93/90%; NN: 90/73%; and NS: 75/82%. The specificity of five patterns in the form of Bayesian/support vector machine was: EMPH: 98/98%; GG: 98/98%; HC: 99/99%; NN: 90/94%; and NS: 96/91%.
CONCLUSION: We conclude that volumetric features including statistical features, histogram and fractal features can be successfully used in differentiation of parenchymal pathology associated with both emphysema and interstitial lung diseases. Additionally, support vector machine and Bayesian methods are comparable classifiers for characterization of interstitial lung diseases on MDCT images.},
	author = {Xu, Ye and van Beek, Edwin J R and Hwanjo, Yu and Guo, Junfeng and McLennan, Geoffrey and Hoffman, Eric A},
	date-added = {2015-07-22 02:36:10 +0000},
	date-modified = {2015-07-23 01:59:43 +0000},
	doi = {10.1016/j.acra.2006.04.017},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Adult; Aged; Algorithms; Artifacts; Bayes Theorem; Female; Humans; Imaging, Three-Dimensional; Lung Diseases, Interstitial; Male; Middle Aged; Pattern Recognition, Automated; Radiographic Image Interpretation, Computer-Assisted; Sensitivity and Specificity; Tomography, X-Ray Computed},
	month = {Aug},
	number = {8},
	pages = {969-78},
	pmid = {16843849},
	pst = {ppublish},
	title = {Computer-aided classification of interstitial lung diseases via {MDCT}: {3D} adaptive multiple feature method (3D AMFM)},
	volume = {13},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2006.04.017}}

@article{Xu:2006aa,
	abstract = {Our goal is to enhance the ability to differentiate normal lung from subtle pathologies via multidetector row CT (MDCT) by extending a two-dimensional (2-D) texturebased tissue classification [adaptive multiple feature method (AMFM)] to use three-dimensional (3-D) texture features. We performed MDCT on 34 humans and classified volumes of interest (VOIs) in the MDCT images into five categories: EC, emphysema in severe chronic obstructive pulmonary disease (COPD); MC, mild emphysema in mild COPD; NC, normal appearing lung in mild COPD; NN, normal appearing lung in normal nonsmokers; and NS, normal appearing lung in normal smokers. COPD severity was based upon pulmonary function tests (PFTs). Airways and vessels were excluded from VOIs; 24 3-D texture features were calculated; and a Bayesian classifier was used for discrimination. A leave-one-out method was employed for validation. Sensitivity of the four-class classification in the form of 3-D/2-D was: EC: 85%/71%, MC: 90%/82%; NC: 88%/50%; NN: 100%/60%. Sensitivity and specificity for NN using a two-class classification of NN and NS in the form of 3-D/2-D were: 99%/72% and 100%/75%, respectively. We conclude that 3-D AMFM analysis of lung parenchyma improves discrimination compared to 2-D AMFM of the same VOIs. Furthermore, our results suggest that the 3-D AMFM may provide a means of discriminating subtle differences between smokers and nonsmokers both with normal PFTs.},
	author = {Xu, Ye and Sonka, Milan and McLennan, Geoffrey and Guo, Junfeng and Hoffman, Eric A},
	date-added = {2015-07-22 02:35:50 +0000},
	date-modified = {2015-07-23 01:59:26 +0000},
	doi = {10.1109/TMI.2006.870889},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Artifacts; Artificial Intelligence; Female; Humans; Imaging, Three-Dimensional; Information Storage and Retrieval; Male; Middle Aged; Pattern Recognition, Automated; Pulmonary Emphysema; Radiation Dosage; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Severity of Illness Index; Smoking; Stochastic Processes; Tomography, X-Ray Computed; Transducers},
	month = {Apr},
	number = {4},
	pages = {464-75},
	pmid = {16608061},
	pst = {ppublish},
	title = {{MDCT}-based {3-D} texture classification of emphysema and early smoking related lung pathologies},
	volume = {25},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2006.870889}}

@article{Woodhouse:2005aa,
	abstract = {PURPOSE: To use a combination of helium-3 (3-He) magnetic resonance imaging (MRI) and proton single-shot fast spin echo (SSFSE) to compare ventilated lung volumes in groups of "healthy" smokers, smokers diagnosed with moderate chronic obstructive pulmonary disease (COPD), and never-smokers.
MATERIALS AND METHODS: All study participants were assessed with spirometry prior to imaging. 3-He images were collected during an arrested breath hold, after inhaling a mixture of 200 mL of hyperpolarized 3-He/800 mL of N2. Proton SSFSE images were acquired after inhaling 1 liter of room air. The ventilated volume for each study participant was calculated from the 3-He images, and a ratio was calculated to give a percentage ventilated lung volume.
RESULTS: Never-smokers exhibited a 90% mean ventilated volume. The mean ventilated lung volumes for healthy smokers and smokers diagnosed with COPD were 75.2% and 67.6%, respectively. No correlation with spirometry was demonstrated for either of the smoking groups.
CONCLUSION: Combined 3-He/Proton SSFSE MRI of the lungs is a noninvasive method, using nonionizing radiation, which demonstrates ventilated airspaces and enables the calculation of ventilated lung volumes. This method appears to be sensitive to early obstructive changes in the lungs of smokers.},
	author = {Woodhouse, Neil and Wild, Jim M and Paley, Martyn N J and Fichele, Stanislao and Said, Zead and Swift, Andrew J and van Beek, Edwin J R},
	date-added = {2015-07-22 02:35:23 +0000},
	date-modified = {2015-07-22 02:35:23 +0000},
	doi = {10.1002/jmri.20290},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Adult; Female; Helium; Humans; Isotopes; Lung Volume Measurements; Magnetic Resonance Imaging; Male; Middle Aged; Pulmonary Disease, Chronic Obstructive; Respiratory Physiological Phenomena; Smoking},
	month = {Apr},
	number = {4},
	pages = {365-9},
	pmid = {15779032},
	pst = {ppublish},
	title = {Combined helium-3/proton magnetic resonance imaging measurement of ventilated lung volumes in smokers compared to never-smokers},
	volume = {21},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.20290}}

@article{Wood:1995aa,
	abstract = {A method was devised to computationally segment and measure three-dimensional pulmonary trees in situ. Bronchi and pulmonary vessels were computationally extracted from volumetric computed tomography data based on radiopacity differences between airway wall and airway lumen and between blood and parenchyma, respectively. The tree was reduced to a central axis to facilitate measurement of branch segment length and angle. Cross-sectional area was measured on a reconstructed computed tomography slice perpendicular to this central axis. The method was validated by scanning two Plexiglas phantoms and an intact lung. Reconstructed diameters in the phantoms were accurate for branches > 2 mm. In the lung airway branches between 1 and 2 mm in diameter were often unresolved when their angle of orientation with respect to the axis of the scanner was > 45 degrees. However, if a branch was resolved, its reconstructed diameter was little affected by orientation. This method represents a significant improvement in the analysis of complex pulmonary structures in three dimensions.},
	author = {Wood, S A and Zerhouni, E A and Hoford, J D and Hoffman, E A and Mitzner, W},
	date-added = {2015-07-22 02:34:15 +0000},
	date-modified = {2015-07-22 02:34:15 +0000},
	journal = {J Appl Physiol (1985)},
	journal-full = {Journal of applied physiology (Bethesda, Md. : 1985)},
	mesh = {Algorithms; Animals; Dogs; In Vitro Techniques; Lung; Pulmonary Artery; Tomography, X-Ray Computed},
	month = {Nov},
	number = {5},
	pages = {1687-97},
	pmid = {8594030},
	pst = {ppublish},
	title = {Measurement of three-dimensional lung tree structures by using computed tomography},
	volume = {79},
	year = {1995}}

@article{Vikgren:2003aa,
	abstract = {PURPOSE: To test the hypothesis that diffuse and/or focal air trapping are sensitive indicators of airflow obstruction in smoker's small airways disease, when age, gender and presence of emphysematous lesions were allowed for.
MATERIAL AND METHODS: Fifty-eight smokers and 34 never smokers, recruited from a randomized population study of men born in 1933, were investigated by HRCT and by extended pulmonary function tests, including a sensitive test for small airways disease (N2 slope). Diffuse air trapping was evaluated by calculating a quotient of mean lung density at expiration and inspiration. Focal air trapping was scored visually by consensus.
RESULTS: Diffuse air trapping did not differ between non-emphysematous smokers and never smokers. Furthermore, diffuse air trapping correlated well to the quotient between the residual volume and total lung capacity (RV/TLC, p = 0.01) and was consequently higher in emphysematous smokers than in never smokers. Focal air trapping was found as frequently in smokers without emphysema as in never smokers. Smokers with emphysema showed significantly less focal air trapping. Neither the N2 slope nor any of the other lung function variables differed between those with and without focal air trapping among non-emphysematous smokers.
CONCLUSION: Neither diffuse nor focal air trapping are sensitive indicators of smoker's small airways disease.},
	author = {Vikgren, J and Bake, B and Ekberg-Jansson, A and Larsson, S and Tyl{\'e}n, U},
	date-added = {2015-07-22 02:33:56 +0000},
	date-modified = {2015-07-22 02:33:56 +0000},
	journal = {Acta Radiol},
	journal-full = {Acta radiologica (Stockholm, Sweden : 1987)},
	mesh = {Aged; Air; Emphysema; Humans; Lung Diseases, Obstructive; Male; Smoking},
	month = {Sep},
	number = {5},
	pages = {517-24},
	pmid = {14510759},
	pst = {ppublish},
	title = {Value of air trapping in detection of small airways disease in smokers},
	volume = {44},
	year = {2003}}

@article{Uppaluri:1997aa,
	abstract = {A texture-based adaptive multiple feature method (AMFM) for evaluating pulmonary parenchyma from computed tomography (CT) images is described. This method incorporates multiple statistical and fractal texture features. The AMFM was compared to two previously published methods, namely, mean lung density (MLD) and the lowest fifth percentile of the histogram (HIST). First, the ability of these methods to detect subtle differences in ventral-dorsal lung density gradient in the prone normal lung was studied. Second, their abilities to differentiate between normal and emphysematous whole lung slices were compared. Finally, regional analyses comparing normal and emphysematous regions were performed by dividing the lungs. In the CT slices into six equal regions, ventral to dorsal, and analyzing each region separately. The results demonstrated that the AMFM could separate the ventral from the dorsal one-third of the normal prone lung with 89.8% accuracy, compared to an accuracy of 74.6% with the MLD and 64.4% with the HIST methods. The normal and emphysematous slices were separated on a global basis with 100.0% accuracy using the AMFM as compared to an accuracy of 94.7% and 97.4% using the MLD and HIST methods, respectively. The regional normal and emphysematous tissues were discriminated with an average accuracy of 97.9%, 89.9%, and 99.1% with the AMFM, MLD, and HIST methods, respectively. The three methods and the pulmonary function tests in the normal and emphysema groups were poorly correlated. Quantitative texture analysis using adaptive multiple features holds promise for the objective noninvasive evaluation of the pulmonary parenchyma.},
	author = {Uppaluri, R and Mitsa, T and Sonka, M and Hoffman, E A and McLennan, G},
	date-added = {2015-07-22 02:33:39 +0000},
	date-modified = {2015-07-22 02:33:39 +0000},
	doi = {10.1164/ajrccm.156.1.9606093},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Humans; Lung; Pulmonary Emphysema; Reference Values; Sensitivity and Specificity; Severity of Illness Index; Tomography, X-Ray Computed},
	month = {Jul},
	number = {1},
	pages = {248-54},
	pmid = {9230756},
	pst = {ppublish},
	title = {Quantification of pulmonary emphysema from lung computed tomography images},
	volume = {156},
	year = {1997},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm.156.1.9606093}}

@article{Uppaluri:1999aa,
	abstract = {We have developed an objective, reproducible, and automated means for the regional evaluation of the pulmonary parenchyma from computed tomography (CT) scans. This method, known as the Adaptive Multiple Feature Method (AMFM) assesses as many as 22 independent texture features in order to classify a tissue pattern. In this study, the six tissue patterns characterized were: honeycombing, ground glass, bronchovascular, nodular, emphysemalike, and normal. The lung slices were evaluated regionally using 31 x 31 pixel regions of interest. In each region of interest, an optimal subset of texture features was evaluated to determine which of the six patterns the region could be characterized as. The computer output was validated against experienced observers in three settings. In the first two readings, when the observers were blinded to the primary diagnosis of the subject, the average computer versus observer agreement was 44.4 +/- 8.7% and 47.3 +/- 9.0%, respectively. The average interobserver agreement for the same two readings were 48.8 +/- 9.1% and 52.2 +/- 10.0%, respectively. In the third reading, when the observers were provided the primary diagnosis, the average computer versus observer agreement was 51.7 +/- 2.9% where as the average interobserver agreement was 53.9 +/- 6.2%. The kappa statistic of agreement between the regions, for which the majority of the observers agreed on a pattern type, versus the computer was found to be 0.62. For regional tissue characterization, the AMFM is 100% reproducible and performs as well as experienced human observers who have been told the patient diagnosis.},
	author = {Uppaluri, R and Hoffman, E A and Sonka, M and Hartley, P G and Hunninghake, G W and McLennan, G},
	date-added = {2015-07-22 02:32:53 +0000},
	date-modified = {2015-07-22 02:32:53 +0000},
	doi = {10.1164/ajrccm.160.2.9804094},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Diagnosis, Computer-Assisted; Humans; Lung; Lung Diseases; Lung, Hyperlucent; Observer Variation; Pulmonary Emphysema; Pulmonary Fibrosis; Radiographic Image Interpretation, Computer-Assisted; Reference Values; Sarcoidosis, Pulmonary; Sensitivity and Specificity; Tomography, X-Ray Computed},
	month = {Aug},
	number = {2},
	pages = {648-54},
	pmid = {10430742},
	pst = {ppublish},
	title = {Computer recognition of regional lung disease patterns},
	volume = {160},
	year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm.160.2.9804094}}

@article{Temizoz:2007aa,
	abstract = {We aimed to determine the degree and extent of parenchymal abnormalities on pulmo-CT in patients with emphysema. The study group consisted of 29 patients (18 male, 11 female; mean age 57.9+/-13). The diagnosis was based on clinical symptoms, pulmonary function tests (PFT) values, and chest CT findings. All of the patients CT scans were obtained during suspended deep inspiration from the apices to the costophrenic angles. The mean lung attenuation (MLD) and parenchymal abnormalities related to emphysema were quantitatively calculated with tables, histograms and graphics at the whole lung. The lung density measurements revealed a mean density of -898.48+/-51.37 HU in patients with emphysema and -825.1+/-25.5 HU in control group. In addition, mean percentage of subthreshold attenuation values was found as 12.03+/-15.75 and 1.07+/-0.83 in patients with emphysema and control group, respectively. Compared with control group, the patients with emphysema had a significantly lower inspiratory MLD (p<0.05). Additionally, statistically significant correlations were seen between the MLD and percentage of subthreshold values (r=0.44, p<0.05). In contrast, there was poor correlation between PFT measurements and the subthreshold values. In conclusion, pulmo-CT is a quick, simple method for quantitative confirmation of the presence of parenchymal abnormalities of lung as mosaic attenuation and should be used in combination with other radiological methods and PFT as it gives additional information to routine examinations in patients with emphysema.},
	author = {Temizoz, Osman and Etlik, Omer and Sakarya, Mehmet Emin and Uzun, Kursat and Arslan, Halil and Harman, Mustafa and Demir, Mustafa Kemal},
	date-added = {2015-07-22 02:32:08 +0000},
	date-modified = {2015-07-23 01:56:45 +0000},
	doi = {10.1016/j.compmedimag.2007.06.003},
	journal = {Comput Med Imaging Graph},
	journal-full = {Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society},
	mesh = {Aged; Female; Humans; Image Interpretation, Computer-Assisted; Male; Middle Aged; Pulmonary Emphysema; Tomography, X-Ray Computed; Turkey},
	month = {Oct},
	number = {7},
	pages = {542-8},
	pmid = {17689224},
	pst = {ppublish},
	title = {Detection and quantification of the parenchymal abnormalities in emphysema using pulmo-{CT}},
	volume = {31},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.compmedimag.2007.06.003}}

@article{Stoel:2004aa,
	abstract = {Currently, lung densitometry for the assessment of pulmonary emphysema has been fully validated against pathology, pulmonary function, and health status, and it is therefore being applied in pharmacotherapeutic trials. Nevertheless, its application for the early detection of emphysema has not yet been introduced in daily clinical practice. The main reason for this is the fact that it is not yet regarded a fully optimized and standardized technique. In this work, an overview is given on the current status of different standardization aspects that play an important role in this, ie, image acquisition, choice of densitometric parameter and image processing. To address these issues, solutions have been sought from the literature and from original data from previous studies. Standardization and optimization of lung densitometry has reached a more advanced stage than has been reported so far. If normal values will become available, this technique will be feasible for clinical practice. As a result, standardization for the detection and assessment of other density-related lung diseases can be achieved in a shorter period of time.},
	author = {Stoel, Berend C and Stolk, Jan},
	date-added = {2015-07-22 02:30:54 +0000},
	date-modified = {2015-07-22 02:30:54 +0000},
	journal = {Invest Radiol},
	journal-full = {Investigative radiology},
	mesh = {Absorptiometry, Photon; Calibration; Humans; Image Processing, Computer-Assisted; Pulmonary Emphysema; Tomography, X-Ray Computed},
	month = {Nov},
	number = {11},
	pages = {681-8},
	pmid = {15486529},
	pst = {ppublish},
	title = {Optimization and standardization of lung densitometry in the assessment of pulmonary emphysema},
	volume = {39},
	year = {2004}}

@article{Stavngaard:2006aa,
	abstract = {PURPOSE: To compare objective and subjective assessment of the distribution of emphysema in unselected patients with chronic obstructive pulmonary disease (COPD).
MATERIAL AND METHODS: 167 patients were computed tomography (CT) scanned, and the relative area (RA-910) of emphysema in each CT slice was plotted against table position. The craniocaudal distribution was calculated as the slope of the regression line, and grouped as upper-lung-zone predominance (ULP), lower-lung-zone predominance (LLP), or mild/homogeneous distribution (MHE). CT scans were also classified as ULP, LLP, and MHE based on visual assessment of three high-resolution CT (HRCT) slices, and the leading pattern of emphysema was classified as centrilobular (CLE), paraseptal (PSE), panlobular (PLE), or no emphysema (NE).
RESULTS: By objective classification, scans were divided into almost equal numbers of ULP, LLP, and MHE, whereas visual evaluation classified more scans as ULP (P<0.001) and very few as LLP (P<0.0001). In patients with CLE, 49% had ULP by objective classification, whereas LLP was the commonest leading pattern in PSE, PLE, and NE.
CONCLUSION: We found significant discrepancies between the objective and subjective distributions of emphysema in various morphological patterns, which may be of clinical importance in, for instance, lung-volume-reduction surgery.},
	author = {Stavngaard, T and Shaker, S B and Bach, K S and Stoel, B C and Dirksen, A},
	date-added = {2015-07-22 02:30:43 +0000},
	date-modified = {2015-07-22 02:30:43 +0000},
	doi = {10.1080/02841850600917170},
	journal = {Acta Radiol},
	journal-full = {Acta radiologica (Stockholm, Sweden : 1987)},
	mesh = {Aged; Female; Humans; Male; Pulmonary Disease, Chronic Obstructive; Pulmonary Emphysema; Tomography, X-Ray Computed},
	month = {Nov},
	number = {9},
	pages = {914-21},
	pmid = {17077040},
	pst = {ppublish},
	title = {Quantitative assessment of regional emphysema distribution in patients with chronic obstructive pulmonary disease (COPD)},
	volume = {47},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/02841850600917170}}

@article{Sled:1998aa,
	abstract = {A novel approach to correcting for intensity nonuniformity in magnetic resonance (MR) data is described that achieves high performance without requiring a model of the tissue classes present. The method has the advantage that it can be applied at an early stage in an automated data analysis, before a tissue model is available. Described as nonparametric nonuniform intensity normalization (N3), the method is independent of pulse sequence and insensitive to pathological data that might otherwise violate model assumptions. To eliminate the dependence of the field estimate on anatomy, an iterative approach is employed to estimate both the multiplicative bias field and the distribution of the true tissue intensities. The performance of this method is evaluated using both real and simulated MR data.},
	author = {Sled, J G and Zijdenbos, A P and Evans, A C},
	date-added = {2015-07-22 02:30:03 +0000},
	date-modified = {2015-07-23 01:56:11 +0000},
	doi = {10.1109/42.668698},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Brain; Humans; Magnetic Resonance Imaging; Models, Theoretical},
	month = {Feb},
	number = {1},
	pages = {87-97},
	pmid = {9617910},
	pst = {ppublish},
	title = {A nonparametric method for automatic correction of intensity nonuniformity in {MRI} data},
	volume = {17},
	year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.668698}}

@article{Schoenemann:2007aa,
	abstract = {A crucial component of research on brain evolution has been the comparison of fossil endocranial surfaces with modern human and primate endocrania. The latter have generally been obtained by creating endocasts out of rubber latex shells filled with plaster. The extent to which the method of production introduces errors in endocast replicas is unknown. We demonstrate a powerful method of comparing complex shapes in 3-dimensions (3D) that is broadly applicable to a wide range of paleoanthropological questions. Pairs of virtual endocasts (VEs) created from high-resolution CT scans of corresponding latex/plaster endocasts and their associated crania were rigidly registered (aligned) in 3D space for two Homo sapiens and two Pan troglodytes specimens. Distances between each cranial VE and its corresponding latex/plaster VE were then mapped on a voxel-by-voxel basis. The results show that between 79.7% and 91.0% of the voxels in the four latex/plaster VEs are within 2 mm of their corresponding cranial VEs surfaces. The average error is relatively small, and variation in the pattern of error across the surfaces appears to be generally random overall. However, inferior areas around the cranial base and the temporal poles were somewhat overestimated in both human and chimpanzee specimens, and the area overlaying Broca's area in humans was somewhat underestimated. This study gives an idea of the size of possible error inherent in latex/plaster endocasts, indicating the level of confidence we can have with studies relying on comparisons between them and, e.g., hominid fossil endocasts.},
	author = {Schoenemann, P Thomas and Gee, James and Avants, Brian and Holloway, Ralph L and Monge, Janet and Lewis, Jason},
	date-added = {2015-07-22 02:29:47 +0000},
	date-modified = {2015-07-23 01:55:56 +0000},
	doi = {10.1002/ajpa.20499},
	journal = {Am J Phys Anthropol},
	journal-full = {American journal of physical anthropology},
	mesh = {Animals; Fossils; Humans; Imaging, Three-Dimensional; Paleontology; Pan troglodytes; Skull; Tomography, X-Ray Computed},
	month = {Feb},
	number = {2},
	pages = {183-92},
	pmid = {17103425},
	pst = {ppublish},
	title = {Validation of plaster endocast morphology through {3D} {CT} image analysis},
	volume = {132},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/ajpa.20499}}

@article{Samee:2003aa,
	abstract = {BACKGROUND: Imaging of gas distribution in the lungs of patients with asthma has been restricted because of the lack of a suitable gaseous contrast agent. Hyperpolarized helium-3 (HHe3) provides a new technique for magnetic resonance imaging of lung diseases.
OBJECTIVE: We sought to investigate the use of HHe3 gas to image the lungs of patients with moderate or severe asthma and to assess changes in gas distribution after methacholine and exercise challenge.
METHODS: Magnetic resonance imaging was performed in asthmatic patients immediately after inhalation of HHe3 gas. In addition, images were obtained before and after methacholine challenge and a standard exercise test.
RESULTS: Areas of the lung with no signal or sharply reduced HHe3 signal (ventilation defects) are common in patients with asthma, and the number of defects was inversely related to the percent predicted FEV(1) (r = 0.71, P <.002). After methacholine challenge (n = 3), the number of defects increased. Similarly, imaging of the lungs after exercise (n = 6) showed increased ventilation defects in parallel with decreases in FEV(1). The increase in defects after challenge in these 9 asthmatic patients was significant both for the number (P <.02) and extent (P <.02) of the defects. The variability and speed of changes in ventilation and the complete lack of signal in many areas is in keeping with a model in which the defects result from airway closure.
CONCLUSION: HHe3 magnetic resonance provides a new technique for imaging the distribution of inhaled air in the lungs. The technique is suitable for following responses to treatment of asthma and changes after methacholine or exercise challenge.},
	author = {Samee, Saba and Altes, Talissa and Powers, Patrick and de Lange, Eduard E and Knight-Scott, Jack and Rakes, Gary and Mugler, 3rd, John P and Ciambotti, Jonathan M and Alford, Bennet A and Brookeman, James R and Platts-Mills, Thomas A E},
	date-added = {2015-07-22 02:29:35 +0000},
	date-modified = {2015-07-22 02:29:35 +0000},
	journal = {J Allergy Clin Immunol},
	journal-full = {The Journal of allergy and clinical immunology},
	mesh = {Adolescent; Adult; Asthma; Bronchoconstrictor Agents; Exercise Test; Helium; Humans; Lung; Magnetic Resonance Imaging; Methacholine Chloride},
	month = {Jun},
	number = {6},
	pages = {1205-11},
	pmid = {12789218},
	pst = {ppublish},
	title = {Imaging the lungs in asthmatic patients by using hyperpolarized helium-3 magnetic resonance: assessment of response to methacholine and exercise challenge},
	volume = {111},
	year = {2003}}

@article{Reinhardt:1997aa,
	abstract = {Airway geometry measurements can provide information regarding pulmonary physiology and pathophysiology. There has been considerable interest in measuring intrathoracic airways in two-dimensional (2-D) slices from volumetric X-ray computed tomography (CT). Such measurements can be used to evaluate and track the progression of diseases affecting the airways. A popular airway measurement method uses the "half-max" criteria, in which the gray level at the airway wall is estimated to be halfway between the minimum and maximum gray levels along a ray crossing the edge. However, because the scanning process introduces blurring, the half-max approach may not be applicable across all airway sizes. We propose a new measurement method based on a model of the scanning process. In our approach, we examine the gray-level profile of a ray crossing the airway wall and use a maximum-likelihood method to estimate the airway inner and outer radius. Using CT scans of a physical phantom, we present results showing that the new approach is more accurate than the half-max method at estimating wall location for thin-walled airways.},
	author = {Reinhardt, J M and D'Souza, N D and Hoffman, E A},
	date-added = {2015-07-22 02:29:20 +0000},
	date-modified = {2015-07-22 02:29:20 +0000},
	doi = {10.1109/42.650878},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Bronchi; Bronchography; Humans; Image Processing, Computer-Assisted; Phantoms, Imaging; Tomography, X-Ray Computed},
	month = {Dec},
	number = {6},
	pages = {820-7},
	pmid = {9533582},
	pst = {ppublish},
	title = {Accurate measurement of intrathoracic airways},
	volume = {16},
	year = {1997},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.650878}}

@article{Reinhardt:2007aa,
	abstract = {The main function of the respiratory system is gas exchange. Since many disease or injury conditions can cause biomechanical or material property changes that can alter lung function, there is a great interest in measuring regional lung ventilation. We describe a registration-based technique for estimating local lung expansion from multiple respiratory-gated CT images of the thorax. The degree of regional lung expansion is measured using the Jacobian of the registration displacement field. We compare lung expansion estimated across five pressure changes to a xenon CT based measure of specific ventilation, and have shown good agreement (linear regression, r2 = 0.89 during gas wash-in) in one animal.},
	author = {Reinhardt, Joseph M and Christensen, Gary E and Hoffman, Eric A and Ding, Kai and Cao, Kunlin},
	date-added = {2015-07-22 02:29:03 +0000},
	date-modified = {2015-07-22 02:29:03 +0000},
	journal = {Inf Process Med Imaging},
	journal-full = {Information processing in medical imaging : proceedings of the ... conference},
	mesh = {Algorithms; Artificial Intelligence; Cluster Analysis; Humans; Imaging, Three-Dimensional; Lung; Lung Volume Measurements; Pattern Recognition, Automated; Pulmonary Ventilation; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Tomography, X-Ray Computed},
	pages = {763-74},
	pmid = {17633746},
	pst = {ppublish},
	title = {Registration-derived estimates of local lung expansion as surrogates for regional ventilation},
	volume = {20},
	year = {2007}}

@article{Vestbo:2013aa,
	abstract = {Chronic obstructive pulmonary disease (COPD) is a global health problem, and since 2001, the Global Initiative for Chronic Obstructive Lung Disease (GOLD) has published its strategy document for the diagnosis and management of COPD. This executive summary presents the main contents of the second 5-year revision of the GOLD document that has implemented some of the vast knowledge about COPD accumulated over the last years. Today, GOLD recommends that spirometry is required for the clinical diagnosis of COPD to avoid misdiagnosis and to ensure proper evaluation of severity of airflow limitation. The document highlights that the assessment of the patient with COPD should always include assessment of (1) symptoms, (2) severity of airflow limitation, (3) history of exacerbations, and (4) comorbidities. The first three points can be used to evaluate level of symptoms and risk of future exacerbations, and this is done in a way that splits patients with COPD into four categories-A, B, C, and D. Nonpharmacologic and pharmacologic management of COPD match this assessment in an evidence-based attempt to relieve symptoms and reduce risk of exacerbations. Identification and treatment of comorbidities must have high priority, and a separate section in the document addresses management of comorbidities as well as COPD in the presence of comorbidities. The revised document also contains a new section on exacerbations of COPD. The GOLD initiative will continue to bring COPD to the attention of all relevant shareholders and will hopefully inspire future national and local guidelines on the management of COPD.},
	author = {Vestbo, J{\o}rgen and Hurd, Suzanne S and Agust{\'\i}, Alvar G and Jones, Paul W and Vogelmeier, Claus and Anzueto, Antonio and Barnes, Peter J and Fabbri, Leonardo M and Martinez, Fernando J and Nishimura, Masaharu and Stockley, Robert A and Sin, Don D and Rodriguez-Roisin, Roberto},
	date-added = {2015-07-22 02:28:49 +0000},
	date-modified = {2015-07-23 01:58:34 +0000},
	doi = {10.1164/rccm.201204-0596PP},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Global Health; Humans; Internationality; Practice Guidelines as Topic; Pulmonary Disease, Chronic Obstructive; Risk Factors; Severity of Illness Index; Spirometry},
	month = {Feb},
	number = {4},
	pages = {347-65},
	pmid = {22878278},
	pst = {ppublish},
	title = {Global strategy for the diagnosis, management, and prevention of chronic obstructive pulmonary disease: {GOLD} executive summary},
	volume = {187},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/rccm.201204-0596PP}}

@article{Perez:2005aa,
	abstract = {STUDY OBJECTIVES: CT estimates of lung density have been used to estimate the extent and severity of emphysema. The present study was designed to test the hypothesis that quantitative CT can track the changes that occur in diffuse lung disease.
DESIGN: The study was based on five patients with pulmonary alveolar proteinosis (PAP) who underwent lung lavage. Pulmonary function was measured before and after each individual lung lavage, and the CT scans before and after lavage were used to compare total lung volume, airspace volume, lung weight, and regional lung inflation. The dry weight of proteinaceous material lavaged from the lung was measured and compared to the change in CT lung weight.
RESULTS: All the patients showed improvements in dyspnea, percentage of predicted diffusion capacity of the lung for carbon monoxide, and FVC. There was no change in CT-measured total lung volume or airspace volume, but there was a reduction in lung weight following lavage (p = 0.001), which correlated with the dry weight of the lavage effluent (R(2) = 0.73). Therefore, there was a shift in the regional lung inflation toward a more inflated lung with a corresponding increase in the mean lung inflation (p = 0.001).
CONCLUSION: These data show that quantitative CT can objectively track the changes in lung weight and airspace inflation produced by a standard intervention in PAP, and we postulate that it can provide similar information about the progression of other diffuse lung diseases.},
	author = {Perez, 4th, Andrew and Coxson, Harvey O and Hogg, James C and Gibson, Kevin and Thompson, Paul F and Rogers, Robert M},
	date-added = {2015-07-22 02:28:17 +0000},
	date-modified = {2015-07-23 01:54:46 +0000},
	doi = {10.1378/chest.128.4.2471},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Bronchoalveolar Lavage; Forced Expiratory Volume; Gases; Humans; Lung; Organ Size; Pulmonary Alveolar Proteinosis; Reference Values; Tomography, X-Ray Computed; Vital Capacity},
	month = {Oct},
	number = {4},
	pages = {2471-7},
	pmid = {16236911},
	pst = {ppublish},
	title = {Use of {CT} morphometry to detect changes in lung weight and gas volume},
	volume = {128},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.128.4.2471}}

@article{Peng:2005aa,
	abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
	date-added = {2015-07-22 02:28:00 +0000},
	date-modified = {2015-07-22 02:28:00 +0000},
	doi = {10.1109/TPAMI.2005.159},
	journal = {IEEE Trans Pattern Anal Mach Intell},
	journal-full = {IEEE transactions on pattern analysis and machine intelligence},
	mesh = {Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Diagnosis, Computer-Assisted; Humans; Information Storage and Retrieval; Models, Statistical; Neoplasms; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated},
	month = {Aug},
	number = {8},
	pages = {1226-38},
	pmid = {16119262},
	pst = {ppublish},
	title = {Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy},
	volume = {27},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TPAMI.2005.159}}

@article{Parraga:2007aa,
	abstract = {OBJECTIVE: Hyperpolarized 3He magnetic resonance imaging (3He MRI) at 3.0 Tesla of healthy volunteers and chronic obstructive pulmonary disease (COPD) patients was performed for quantitative evaluation of ventilation defects and apparent diffusion coefficients (ADC) and for comparison to published results acquired at 1.5 Tesla. The reproducibility of 3He ADC and ventilation defects was also assessed in subjects scanned 3 times, twice within 10 minutes, and again within 7 +/- 2 days of the first MRI visit.
MATERIALS AND METHODS: Hyperpolarized 3He MRI was performed in 6 subjects. Two interleaved images with and without additional diffusion sensitization were acquired with the first image serving as a ventilation image from which defect score and volume were measured and the combination of the 2 images used to compute ADC maps and ADC histograms.
RESULTS: He MRI at 3.0 Tesla showed increased mean ADC and ADC standard deviation for subjects with COPD compared with healthy volunteers (ADC healthy volunteer (0.24 +/- 0.12 cm2/s), mild-moderate COPD (0.34 +/- 0.14 cm2/s), and severe COPD (0.47 +/- 0.21 cm2/s), and these values were similar to previously reported results acquired at 1.5 Tesla. Reproducibility of mean ADC was high (coefficient of variation 2% in severe COPD, 3% in mild-moderate COPD, 4% in healthy volunteers) across all 3 scans. Higher same-day scan reproducibility was observed for ventilation defect volume compared with 1-week scan reproducibility in this small group of subjects.
CONCLUSIONS: ADC values for emphysematous lungs were significantly increased compared with healthy lungs in age-matched subjects, and all values were comparable to those reported previously at 1.5 Tesla. Ventilation defect score and ventilation defect volume results were also comparable to results previously reported in COPD subjects Reproducibility of ADC for same-day scan-rescan and 7-day rescan was high and similar to previously reported results.},
	author = {Parraga, Grace and Ouriadov, Alexei and Evans, Andrea and McKay, Shayna and Lam, Wilfred W and Fenster, Aaron and Etemad-Rezai, Roya and McCormack, David and Santyr, Giles},
	date-added = {2015-07-22 02:27:32 +0000},
	date-modified = {2015-07-23 01:54:30 +0000},
	doi = {10.1097/01.rli.0000262571.81771.66},
	journal = {Invest Radiol},
	journal-full = {Investigative radiology},
	mesh = {Aged; Case-Control Studies; Diffusion Magnetic Resonance Imaging; Female; Helium; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Male; Middle Aged; Pulmonary Disease, Chronic Obstructive; Reproducibility of Results; Respiratory Function Tests},
	month = {Jun},
	number = {6},
	pages = {384-91},
	pmid = {17507809},
	pst = {ppublish},
	title = {Hyperpolarized 3{H}e ventilation defects and apparent diffusion coefficients in chronic obstructive pulmonary disease: preliminary results at 3.0 {T}esla},
	volume = {42},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1097/01.rli.0000262571.81771.66}}

@article{Park:1998aa,
	abstract = {Three-dimensional (3-D) analysis of airway trees extracted from computed tomography (CT) image data can provide objective information about lung structure and function. However, manual analysis of 3-D lung CT images is tedious, time consuming and, thus, impractical for routine clinical care. We have previously reported an automated rule-based method for extraction of airway trees from 3-D CT images using a priori knowledge about airway-tree anatomy. Although the method's sensitivity was quite good, its specificity suffered from a large number of falsely detected airways. We present a new approach to airway-tree detection based on fuzzy logic that increases the method's specificity without compromising its sensitivity. The method was validated in 32 CT image slices randomly selected from five volumetric canine electron-beam CT data sets. The fuzzy-logic method significantly outperformed the previously reported rule-based method (p < 0.002).},
	author = {Park, W and Hoffman, E A and Sonka, M},
	date-added = {2015-07-22 02:27:20 +0000},
	date-modified = {2015-07-22 02:27:20 +0000},
	doi = {10.1109/42.730394},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Animals; Dogs; Fuzzy Logic; Lung; Random Allocation; Sensitivity and Specificity; Tomography, X-Ray Computed},
	month = {Aug},
	number = {4},
	pages = {489-97},
	pmid = {9845305},
	pst = {ppublish},
	title = {Segmentation of intrathoracic airway trees: a fuzzy logic approach},
	volume = {17},
	year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.730394}}

@article{Orlandi:2005aa,
	abstract = {PURPOSE: To prospectively evaluate airway wall thickness and lung attenuation at spirometrically gated thin-section computed tomography (CT) in patients with chronic obstructive pulmonary disease (COPD) and to correlate gated CT findings with pulmonary function test (PFT) results.
MATERIALS AND METHODS: The ethical committee approved the study, and all patients gave informed consent. Forty-two consecutive patients with COPD (20 with and 22 without chronic bronchitis [CB]) underwent gated thin-section CT and PFTs on the same day. The percentage wall area (PWA) and the thickness-to-diameter ratio (TDR) for all depicted bronchi that were round and larger than 2 mm in diameter, the mean lung attenuation (MLA), and the pixel index (PI) at -950 HU were determined. The reproducibility of the airway measurements was preliminarily tested by performing a five-trial examination in a patient with COPD and in a control patient. Differences in airway and lung attenuation measurements between the patients with and those without CB were evaluated at Mann-Whitney U testing. Simple and multiple regression analyses were used to assess the correlation between thin-section CT and PFT measurements.
RESULTS: The mean intraoperator coefficient of variation for airway measurements was 7.8% (range, 3.8%-13.4%). An average of nine bronchi per patient were assessed. Patients with CB had significantly higher PWAs, TDRs, and MLAs and significantly lower PIs than patients without CB (P < .05 for all values). The combination of PWA, TDR, and PWA normalized to body weight correlated significantly (P < .05) with the forced expiratory volume in 1 second-to-slow vital capacity ratio and the diffusing capacity of the lung for carbon monoxide in patients with but not in patients without CB. PFT results correlated better with MLA and PI in patients without CB.
CONCLUSION: Bronchial wall measurements differ between patients who have COPD with CB and those who have COPD without CB. The correlation between airway dimensions and indexes of airway obstruction in patients with COPD and CB indicates that the bronchial tree is the site of anatomic-functional alterations in this patient group.},
	author = {Orlandi, Ilaria and Moroni, Chiara and Camiciottoli, Gianna and Bartolucci, Maurizio and Pistolesi, Massimo and Villari, Natale and Mascalchi, Mario},
	date-added = {2015-07-22 02:27:06 +0000},
	date-modified = {2015-07-23 01:54:14 +0000},
	doi = {10.1148/radiol.2342040013},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Adult; Aged; Bronchitis; Bronchography; Chronic Disease; Female; Forced Expiratory Volume; Humans; Lung; Male; Middle Aged; Observer Variation; Prospective Studies; Pulmonary Disease, Chronic Obstructive; Regression Analysis; Reproducibility of Results; Respiratory Function Tests; Spirometry; Tomography, X-Ray Computed; Vital Capacity},
	month = {Feb},
	number = {2},
	pages = {604-10},
	pmid = {15671010},
	pst = {ppublish},
	title = {Chronic obstructive pulmonary disease: thin-section {CT} measurement of airway wall thickness and lung attenuation},
	volume = {234},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiol.2342040013}}

@article{Nakano:1999aa,
	abstract = {BACKGROUND: The low attenuation areas on computed tomographic (CT) scans have been reported to represent emphysematous changes of the lung. However, the regional distribution of emphysema between the inner and outer segments of the lung has not been adequately studied. In this study the regional distribution of low attenuation areas has been compared by quantitative CT analysis and the contribution of the regional distribution to pulmonary function tests evaluated in patients with chronic obstructive pulmonary disease (COPD).
METHODS: Chest CT images and the results of pulmonary function tests were obtained from 73 patients with COPD. The lung images were divided into inner and outer segments in the upper (cranial), middle, and lower (caudal) sections. The percentage ratio of low attenuation area to corresponding lung area (LAA%) was then calculated. The LAA% of each segment was also compared with the results of pulmonary function tests.
RESULTS: The mean (SD) LAA% of the inner segment was 39.1 (18.5) compared with 28.1 (13.2) for the outer segment (p<0.0001). Linear and multiple regression analyses revealed that airflow limitation is closely correlated with the inner segment LAA% of the lower lung. In contrast, the carbon monoxide transfer factor is closely correlated with the inner segment LAA% of the upper lung.
CONCLUSION: Low attenuation areas on CT scans are more often found in the inner segment of the lung than in the outer segment, and the contribution of the inner segment to pulmonary function tests may be greater than the outer segment.},
	author = {Nakano, Y and Sakai, H and Muro, S and Hirai, T and Oku, Y and Nishimura, K and Mishima, M},
	date-added = {2015-07-22 02:26:50 +0000},
	date-modified = {2015-07-22 02:26:50 +0000},
	journal = {Thorax},
	journal-full = {Thorax},
	mesh = {Aged; Analysis of Variance; Forced Expiratory Volume; Humans; Lung; Lung Diseases, Obstructive; Male; Radiographic Image Interpretation, Computer-Assisted; Smoking; Tomography, X-Ray Computed; Vital Capacity},
	month = {May},
	number = {5},
	pages = {384-9},
	pmc = {PMC1763774},
	pmid = {10212100},
	pst = {ppublish},
	title = {Comparison of low attenuation areas on computed tomographic scans between inner and outer segments of the lung in patients with chronic obstructive pulmonary disease: incidence and contribution to lung function},
	volume = {54},
	year = {1999}}

@article{Nakano:2000aa,
	abstract = {Chronic obstructive pulmonary disease (COPD) is characterized by the presence of airflow obstruction caused by emphysema or airway narrowing, or both. Low attenuation areas (LAA) on computed tomography (CT) have been shown to represent macroscopic or microscopic emphysema, or both. However CT has not been used to quantify the airway abnormalities in smokers with or without airflow obstruction. In this study, we used CT to evaluate both emphysema and airway wall thickening in 114 smokers. The CT measurements revealed that a decreased FEV(1) (%predicted) is associated with an increase of airway wall area and an increase of emphysema. Although both airway wall thickening and emphysema (LAA) correlated with measurements of lung function, stepwise multiple regression analysis showed that the combination of airway and emphysema measurements improved the estimate of pulmonary function test abnormalities. We conclude that both CT measurements of airway dimensions and emphysema are useful and complementary in the evaluation of the lung of smokers.},
	author = {Nakano, Y and Muro, S and Sakai, H and Hirai, T and Chin, K and Tsukino, M and Nishimura, K and Itoh, H and Par{\'e}, P D and Hogg, J C and Mishima, M},
	date-added = {2015-07-22 02:26:32 +0000},
	date-modified = {2015-07-22 02:26:32 +0000},
	doi = {10.1164/ajrccm.162.3.9907120},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Adult; Aged; Aged, 80 and over; Airway Resistance; Female; Humans; Image Processing, Computer-Assisted; Lung Diseases, Obstructive; Male; Middle Aged; Predictive Value of Tests; Pulmonary Emphysema; Smoking; Tomography, X-Ray Computed},
	month = {Sep},
	number = {3 Pt 1},
	pages = {1102-8},
	pmid = {10988137},
	pst = {ppublish},
	title = {Computed tomographic measurements of airway dimensions and emphysema in smokers. Correlation with lung function},
	volume = {162},
	year = {2000},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm.162.3.9907120}}

@article{Nakano:2001aa,
	abstract = {Computed tomography (CT) has shown that emphysema is more extensive in the inner (core) region than in the outer (rind) region of the lung. It has been suggested that the concentration of emphysematous lesions in the outer rind leads to a better outcome following lung volume reduction surgery (LVRS) because these regions tend to be more surgically accessible. The present study used a recently described, computer-based CT scan analysis to quantify severe emphysema (lung inflation > 10.2 ml gas/g tissue), mild/moderate emphysema (lung inflation = 10.2 to 6.0 ml gas/g tissue), and normal lung tissue (lung inflation < 6.0 ml gas/g tissue) present in the core and rind of the lung in 21 LVRS patients. The results show that the quantification of severe emphysema independently predicts change in maximal exercise response and FEV(1). We conclude that a greater extent of severe emphysema in the rind of the upper lung predicts greater benefit from LVRS because it identifies the lesions most accessible to removal by LVRS.},
	author = {Nakano, Y and Coxson, H O and Bosan, S and Rogers, R M and Sciurba, F C and Keenan, R J and Walley, K R and Par{\'e}, P D and Hogg, J C},
	date-added = {2015-07-22 02:25:41 +0000},
	date-modified = {2015-07-22 02:25:41 +0000},
	doi = {10.1164/ajrccm.164.12.2012140},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Female; Humans; Lung; Lung Volume Measurements; Male; Pneumonectomy; Pulmonary Emphysema; Regression Analysis; Tomography, X-Ray Computed; Treatment Outcome},
	month = {Dec},
	number = {12},
	pages = {2195-9},
	pmid = {11751187},
	pst = {ppublish},
	title = {Core to rind distribution of severe emphysema predicts outcome of lung volume reduction surgery},
	volume = {164},
	year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm.164.12.2012140}}

@article{Muller:1988aa,
	abstract = {We used a computed tomography (CT) scanner program ("density mask") that highlights voxels within a given density range to quantitate emphysema by defining areas of abnormally low attenuation. We compared different density masks, mean lung attenuation, visual assessment of emphysema and the pathologic grade of emphysema in 28 patients undergoing lung resection for tumor. In each patient, a single representative CT image was compared with corresponding pathologic specimens of tissue. There was good correlation between the extent of emphysema as assessed by the density mask and the pathologic grade of emphysema. The optimal attenuation level to define areas of emphysema may vary in different scanners, but, once determined for a particular scanner, the density mask accurately assesses the extent of emphysema and eliminates interobserver and intraobserver variability. It has the added advantage of determining the exact percentage of lung parenchyma showing changes consistent with emphysema.},
	author = {M{\"u}ller, N L and Staples, C A and Miller, R R and Abboud, R T},
	date-added = {2015-07-22 02:25:22 +0000},
	date-modified = {2015-07-22 02:25:22 +0000},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Adult; Aged; Female; Humans; Lung; Male; Middle Aged; Pulmonary Emphysema; Radiographic Image Enhancement; Tomography, X-Ray Computed},
	month = {Oct},
	number = {4},
	pages = {782-7},
	pmid = {3168574},
	pst = {ppublish},
	title = {"Density mask". An objective method to quantitate emphysema using computed tomography},
	volume = {94},
	year = {1988}}

@article{Miller:1989aa,
	abstract = {Thirty-eight patients undergoing lobectomy or pneumonectomy for carcinoma had preoperative computed tomography (CT) of the chest. Twenty-seven had both 1.5 mm and 10 mm collimation scans, and eleven had 10 mm collimation images only. These images were analyzed for the extent and severity of emphysema, and the analysis compared to the pathologic findings in the corresponding transverse slice of lung. The latter was graded by a modification of a panel of standards and by a grid system numerically expressing extent and severity. The grid system is theoretically superior to the panel of standards because it allows better quantitation of early emphysema and, contrary to the set of standards, is designed to analyze transverse CT images and corresponding pathologic slices. There was good correlation between the CT score and the pathologic score using the panel of standards (r = 0.81, p less than 0.001) but a lower correlation with the grid system (r = 0.70, p less than 0.001). The correlation improved slightly with 1.5 as compared to 10 mm collimation scans. Close comparison between the CT and grid scores showed that CT was sensitive in demonstrating early distal acinar and irregular emphysema. However, CT consistently underestimated the extent of centriacinar and panacinar emphysema because most lesions less than 0.5 cm in diameter were missed. We conclude that CT is insensitive in detecting the earliest lesions of emphysema.},
	author = {Miller, R R and M{\"u}ller, N L and Vedal, S and Morrison, N J and Staples, C A},
	date-added = {2015-07-22 02:25:00 +0000},
	date-modified = {2015-07-22 02:25:00 +0000},
	doi = {10.1164/ajrccm/139.4.980},
	journal = {Am Rev Respir Dis},
	journal-full = {The American review of respiratory disease},
	mesh = {Humans; Lung; Prospective Studies; Pulmonary Emphysema; Tomography, X-Ray Computed},
	month = {Apr},
	number = {4},
	pages = {980-3},
	pmid = {2930075},
	pst = {ppublish},
	title = {Limitations of computed tomography in the assessment of emphysema},
	volume = {139},
	year = {1989},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm/139.4.980}}

@article{Mentore:2005aa,
	abstract = {RATIONALE AND OBJECTIVES: The purpose of this study is to determine hyperpolarized helium 3 (HHe) magnetic resonance (MR) findings of the lung in patients with cystic fibrosis (CF) compared with healthy subjects and determine whether HHe MR can detect changes after bronchodilator therapy or mechanical airway mucus clearance treatment.
MATERIALS AND METHODS: Thirty-one subjects, 16 healthy volunteers and 15 patients with CF, underwent HHe lung ventilation MR imaging and spirometry at baseline. Eight patients with CF then were treated with nebulized albuterol, after which a follow-up HHe MR scan was obtained. Subsequently, recombinant human deoxyribonuclease (DNase) treatment and chest physical therapy were performed in these eight subjects, followed by a third HHe MR scan. For each MR study, the number of ventilation defects was scored by a human reader.
RESULTS: Patients with CF had significantly more HHe MR ventilation defects per image than healthy subjects (mean, 8.2 defects in patients with CF vs 1.6 defects in healthy subjects; P < .05). Even the four subjects with CF with a normal forced expiratory volume in 1 second had significantly more ventilation defects than healthy subjects (mean, 6.5 defects in these patients with CF; P = .0002). After treatment with albuterol, there was a small, but statistically significant, decrease in number of ventilation defects (mean, 9.6-8.0 defects; P = .025). After DNase and chest physical therapy, there was a trend toward increasing ventilation defects (mean, 8.3 defects; P = .096), but with a residual net improvement relative to baseline.
CONCLUSION: In patients with CF, HHe MR ventilation defects correlate with spirometry, change with treatment, and are elevated in number in patients with CF with normal spirometry results. Thus, HHe MR appears to possess many of the characteristics required of a biomarker for pulmonary CF and may be useful in the evaluation of CF pulmonary disease severity or progression.},
	author = {Mentore, Kimiknu and Froh, Deborah K and de Lange, Eduard E and Brookeman, James R and Paget-Brown, Alix O and Altes, Talissa A},
	date-added = {2015-07-22 02:24:39 +0000},
	date-modified = {2015-07-23 01:52:50 +0000},
	doi = {10.1016/j.acra.2005.07.008},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Administration, Inhalation; Adult; Albuterol; Bronchodilator Agents; Cystic Fibrosis; Deoxyribonuclease I; Female; Helium; Humans; Isotopes; Lung; Magnetic Resonance Imaging; Male; Pulmonary Ventilation; Recombinant Proteins; Respiratory Function Tests; Respiratory Therapy; Spirometry},
	month = {Nov},
	number = {11},
	pages = {1423-9},
	pmid = {16253854},
	pst = {ppublish},
	title = {Hyperpolarized {HH}e 3 {MRI} of the lung in cystic fibrosis: assessment at baseline and after bronchodilator and airway clearance treatment},
	volume = {12},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2005.07.008}}

@article{McNitt-Gray:1997aa,
	abstract = {PURPOSE: Our goal was to develop a protocol and image-processing methods to quantitate both bronchial and lung attenuation changes in patients imaged with helical high-resolution CT (HRCT).
METHOD: Human subjects underwent helical HRCT at two suspended breath-hold conditions, functional residual capacity and residual volume, at baseline and following methacholine-induced bronchoprovocation. A semiautomated contouring program was used to define anatomically like bronchi and axial lung sections from the different physiologic sequences, from which automated measurements of area, shape, and attenuation were made. Because the gray level threshold for contouring directly affects the measured area of an anatomic structure, two types of evaluation studies were performed. These included in vivo measurements using baseline parameters of human subjects as the standard of reference and in vitro measurements of a CT phantom designed to simulate the air-soft tissue interfaces of bronchi.
RESULTS: Phantom tests showed that the minimum difference between actual and measured areas of holes occurred at a threshold of -500 HU. The smallest diameter holes were most sensitive to changes in threshold value. However, although absolute area measurements of both simulated and human bronchi varied with threshold level, the percent changes in airway areas between baseline and bronchoprovocation sequences were relatively stable at any given threshold.
CONCLUSION: These image-processing tools provide reproducible measurements of area as well as attenuation characteristics of pulmonary structures and may offer insights into the practical use of functional imaging in evaluating conditions of airflow obstruction.},
	author = {McNitt-Gray, M F and Goldin, J G and Johnson, T D and Tashkin, D P and Aberle, D R},
	date = {1997 Nov-Dec},
	date-added = {2015-07-22 02:23:57 +0000},
	date-modified = {2015-07-23 01:52:36 +0000},
	journal = {J Comput Assist Tomogr},
	journal-full = {Journal of computer assisted tomography},
	mesh = {Bronchial Hyperreactivity; Bronchography; Humans; Image Processing, Computer-Assisted; Lung; Phantoms, Imaging; Tomography, X-Ray Computed},
	number = {6},
	pages = {939-47},
	pmid = {9386287},
	pst = {ppublish},
	title = {Development and testing of image-processing methods for the quantitative assessment of airway hyperresponsiveness from high-resolution {CT} images},
	volume = {21},
	year = {1997}}

@article{McMahon:2006aa,
	abstract = {The purpose of this study was to compare hyperpolarized 3helium magnetic resonance imaging (3He MRI) of the lungs in adults with cystic fibrosis (CF) with high-resolution computed tomography (HRCT) and spirometry. Eight patients with stable CF prospectively underwent 3He MRI, HRCT, and spirometry within 1 week. Three-dimensional (3D) gradient-echo sequence was used during an 18-s breath-hold following inhalation of hyperpolarized 3He. Each lung was divided into six zones; 3He MRI was scored as percentage ventilation per lung zone. HRCT was scored using a modified Bhalla scoring system. Univariate (Spearman rank) and multivariate correlations were performed between 3He MRI, HRCT, and spirometry. Results are expressed as mean+/-SD (range). Spirometry is expressed as percent predicted. There were four men and four women, mean age = 31.9+/-9 (20-46). Mean forced expiratory volume in 1 s (FEV)1 = 52%+/-29 (27-93). Mean 3He MRI score = 74%+/-25 (55-100). Mean HRCT score = 48.8+/-24 (13.5-83). The correlation between 3He MRI and HRCT was strong (R = +/-0.89, p < 0.001). Bronchiectasis was the only independent predictor of 3He MRI; 3He MRI correlated better with FEV1 and forced vital capacity (FVC) (R = 0.86 and 0.93, p < 0.01, respectively) than HRCT (R = +/-0.72 and +/-0.81, p < 0.05, respectively). This study showed that 3He MRI correlates strongly with structural HRCT abnormalities and is a stronger correlate of spirometry than HRCT in CF.},
	author = {McMahon, Colm J and Dodd, Jonathan D and Hill, Catherine and Woodhouse, Neil and Wild, Jim M and Fichele, Stan and Gallagher, Charles G and Skehan, Stephen J and van Beek, Edwin J R and Masterson, James B},
	date-added = {2015-07-22 02:23:31 +0000},
	date-modified = {2015-07-23 01:52:27 +0000},
	doi = {10.1007/s00330-006-0311-5},
	journal = {Eur Radiol},
	journal-full = {European radiology},
	mesh = {Adult; Analysis of Variance; Bronchiectasis; Cystic Fibrosis; Female; Follow-Up Studies; Forced Expiratory Volume; Helium; Humans; Image Processing, Computer-Assisted; Isotopes; Lung; Magnetic Resonance Imaging; Male; Middle Aged; Observer Variation; Research Design; Severity of Illness Index; Spirometry; Tomography, X-Ray Computed; Vital Capacity},
	month = {Nov},
	number = {11},
	pages = {2483-90},
	pmid = {16871384},
	pst = {ppublish},
	title = {Hyperpolarized 3helium magnetic resonance ventilation imaging of the lung in cystic fibrosis: comparison with high resolution {CT} and spirometry},
	volume = {16},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00330-006-0311-5}}

@article{Madani:2007aa,
	abstract = {PURPOSE: To prospectively investigate the effects of radiation dose and section thickness on quantitative multidetector computed tomographic (CT) indexes of pulmonary emphysema.
MATERIALS AND METHODS: The institutional review board approved this protocol. Written informed consent was obtained from all patients. Seventy patients (49 men, 21 women; age range, 38-79 years) referred for surgical resection of a lung tumor underwent multidetector CT with 4 x 1-mm collimation, 120 kVp, and 20 and 120 effective mAs. At each radiation dose, 1.25-, 5.0-, and 10.0-mm-thick sections were reconstructed at 10-mm intervals. From scans of the lobe or whole lung to be resected, relative areas (RAs) of lung with attenuation coefficients lower than nine thresholds and eight percentiles of the distribution of attenuation coefficients were compared with the histopathologic extent of emphysema, which was measured microscopically--by using the corrected mean interwall distance (MIWD) and the corrected mean perimeter (MP)--and macroscopically. Correlations between the data obtained by using attenuation thresholds and percentiles and the parameters macroscopic extent of emphysema, MIWD, and MP were investigated by using Spearman coefficients.
RESULTS: The 1st percentile (r range, -0.394 to -0.675; P < .001) and attenuation coefficients of -980, -970, and -960 HU (r range, 0.478-0.664; P < .001) yielded the strongest correlations with macroscopic extent, MIWD, and MP, regardless of radiation dose or section thickness. The effects of radiation dose and section thickness on RAs of lung with attenuation coefficients lower than -960 HU (P = .007 and P < .001, respectively) and lower than -970 HU (P = .001 and P < .001, respectively) were significant. The effect of section thickness on the 1st percentile was significant (P < .001), whereas the effect of dose was not (P = .910).
CONCLUSION: At CT quantification of pulmonary emphysema, the tube current-time product can be reduced to 20 mAs, but both tube current-time product and section thickness should be kept constant in follow-up examinations.},
	author = {Madani, Afarine and De Maertelaer, Viviane and Zanen, Jacqueline and Gevenois, Pierre Alain},
	date-added = {2015-07-22 02:22:00 +0000},
	date-modified = {2015-07-23 01:52:14 +0000},
	doi = {10.1148/radiol.2431060194},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Adult; Aged; Dose-Response Relationship, Radiation; Female; Humans; Lung; Male; Middle Aged; Prospective Studies; Pulmonary Emphysema; Radiation Dosage; Statistics, Nonparametric; Tomography, X-Ray Computed},
	month = {Apr},
	number = {1},
	pages = {250-7},
	pmid = {17392257},
	pst = {ppublish},
	title = {Pulmonary emphysema: radiation dose and section thickness at multidetector {CT} quantification--comparison with macroscopic and microscopic morphometry},
	volume = {243},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiol.2431060194}}

@article{Koumellis:2005aa,
	abstract = {PURPOSE: To investigate regional airways obstruction in patients with cystic fibrosis (CF) with quantitative analysis of dynamic hyperpolarized (HP) (3)He MRI.
MATERIALS AND METHODS: Dynamic radial projection MRI of HP (3)He gas was used to study respiratory dynamics in a group of eight children with CF. Signal kinetics in a total of seven regions of interest (ROIs; three in each lung, and one in the trachea) were compared with the results of spirometric pulmonary function tests (PFTs). The tracheal signal intensity was used as a form of "input function" to normalize for input flow effects.
RESULTS: A pattern of low flow rate in the upper lobes was observed. When the flow measurements from the peripheral ROIs were averaged to obtain an index of flow in the peripheral lung, a good correlation was found (P = 3.74 x 10(-5)) with the forced expired volume in one second (FEV1).
CONCLUSION: These results suggest that a quantitative measurement of localized airways obstruction in the early stages of CF may be obtained from dynamic (3)He MRI by using the slope of the signal rise as a measure of air flow into the peripheral lung. This study also demonstrates that children can cooperate well with the (3)He MRI technique.},
	author = {Koumellis, Panos and van Beek, Edwin J R and Woodhouse, Neil and Fichele, Stan and Swift, Andrew J and Paley, Martyn N J and Hill, Catherine and Taylor, Chris J and Wild, Jim M},
	date-added = {2015-07-22 02:16:33 +0000},
	date-modified = {2015-07-23 01:51:05 +0000},
	doi = {10.1002/jmri.20402},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Adolescent; Airway Obstruction; Child; Cystic Fibrosis; Female; Humans; Magnetic Resonance Imaging; Male; Spirometry; Tritium},
	month = {Sep},
	number = {3},
	pages = {420-6},
	pmid = {16104046},
	pst = {ppublish},
	title = {Quantitative analysis of regional airways obstruction using dynamic hyperpolarized {3He} {MRI}-preliminary results in children with cystic fibrosis},
	volume = {22},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.20402}}

@article{Kiryu:2008aa,
	abstract = {PURPOSE: To test the feasibility of a method to quantify regional pulmonary parenchymal motion via nonrigid registration algorithm at small animal scales.
MATERIALS AND METHODS: Voxel-wise displacement vector field maps were generated between end-inspiratory and end-expiratory coronal thoracic MR images on normal mice (N = 5) to analyze the magnitude and direction of parenchymal motion in the segmented regions. The analysis was repeated before and after short-term exposure to hypoxia to demonstrate the effect of hypoxia on the respiratory motion in transgenic (Tg) mice with sickle cell disease (SCD) (N = 4).
RESULTS: Normal mice revealed that the right and left lungs moved symmetrically but that there was greater movement in the lower regions than in the upper regions. Calculated strain was uniform in the entire lung. In the Tg mice, the pulmonary motion before hypoxia was similar to that observed in the normal mice. Upon exposure to hypoxia, the displacement magnitude reduced and the direction of motion in some areas became distorted.
CONCLUSION: MR quantification of pulmonary motion was feasible in mice and the principle that the method could detect mechanical abnormalities due to pathologic changes was proven. Quantification of pulmonary motion has the potential to lead to earlier disease diagnosis and better monitoring of disease treatments.},
	author = {Kiryu, Shigeru and Sundaram, Tessa and Kubo, Shigeto and Ohtomo, Kuni and Asakura, Toshio and Gee, James C and Hatabu, Hiroto and Takahashi, Masaya},
	date-added = {2015-07-22 02:16:17 +0000},
	date-modified = {2015-07-23 01:50:33 +0000},
	doi = {10.1002/jmri.21035},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Analysis of Variance; Anemia, Sickle Cell; Animals; Anoxia; Biomechanical Phenomena; Feasibility Studies; Lung; Magnetic Resonance Imaging; Mice; Mice, Inbred BALB C; Mice, Transgenic; Movement; Statistics, Nonparametric},
	month = {Jan},
	number = {1},
	pages = {49-56},
	pmid = {18050323},
	pst = {ppublish},
	title = {{MRI} assessment of lung parenchymal motion in normal mice and transgenic mice with sickle cell disease},
	volume = {27},
	year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.21035}}

@article{Lee:2015aa,
	abstract = {OBJECTIVE: To explore the barriers to and facilitators of adapting and expanding a primary care memory clinic model to integrate care of additional complex chronic geriatric conditions (heart failure, falls, chronic obstructive pulmonary disease, and frailty) into care processes with the goal of improving outcomes for seniors.
DESIGN: Mixed-methods study using quantitative (questionnaires) and qualitative (interviews) methods.
SETTING: Ontario.
PARTICIPANTS: Family physicians currently working in primary care memory clinic teams and supporting geriatric specialists.
METHODS: Family physicians currently working in memory clinic teams (n = 29) and supporting geriatric specialists(n = 9) were recruited as survey participants. Interviews were conducted with memory clinic lead physicians (n = 16).Statistical analysis was done to assess differences between family physician ratings and geriatric specialist ratings related to the capacity for managing complex chronic geriatric conditions, the role of interprofessional collaboration within primary care, and funding and staffing to support geriatric care. Results from both study methods were compared to identify common findings.
MAIN FINDINGS: Results indicate overall support for expanding the memory clinic model to integrate care for other complex conditions. However, the current primary care structure is challenged to support optimal management of patients with multiple comorbidities, particularly as related to limited funding and staffing resources. Structured training, interprofessional teams, and an active role of geriatric specialists within primary care were identified as important facilitators.
CONCLUSION: The memory clinic model, as applied to other complex chronic geriatric conditions, has the potential to build capacity for high-quality primary care, improve health outcomes,promote efficient use of health care resources, and reduce healthcare costs.},
	author = {Lee, Linda and Heckman, George and McKelvie, Robert and Jong, Philip and D'Elia, Teresa and Hillier, Loretta M},
	date-added = {2015-07-22 02:15:32 +0000},
	date-modified = {2015-07-22 02:15:32 +0000},
	journal = {Can Fam Physician},
	journal-full = {Canadian family physician M{\'e}decin de famille canadien},
	month = {Mar},
	number = {3},
	pages = {e148-57},
	pmc = {PMC4369631},
	pmid = {25932482},
	pst = {ppublish},
	title = {Physicians' perceptions of capacity building for managing chronic disease in seniors using integrated interprofessional care models},
	volume = {61},
	year = {2015}}

@article{Hu:2001aa,
	abstract = {Segmentation of pulmonary X-ray computed tomography (CT) images is a precursor to most pulmonary image analysis applications. This paper presents a fully automatic method for identifying the lungs in three-dimensional (3-D) pulmonary X-ray CT images. The method has three main steps. First, the lung region is extracted from the CT images by gray-level thresholding. Then, the left and right lungs are separated by identifying the anterior and posterior junctions by dynamic programming. Finally, a sequence of morphological operations is used to smooth the irregular boundary along the mediastinum in order to obtain results consistent with those obtained by manual analysis, in which only the most central pulmonary arteries are excluded from the lung region. The method has been tested by processing 3-D CT data sets from eight normal subjects, each imaged three times at biweekly intervals with lungs at 90% vital capacity. We present results by comparing our automatic method to manually traced borders from two image analysts. Averaged over all volumes, the root mean square difference between the computer and human analysis is 0.8 pixels (0.54 mm). The mean intrasubject change in tissue content over the three scans was 2.75% +/- 2.29% (mean +/- standard deviation).},
	author = {Hu, S and Hoffman, E A and Reinhardt, J M},
	date-added = {2015-07-22 02:14:37 +0000},
	date-modified = {2015-07-23 01:50:08 +0000},
	doi = {10.1109/42.929615},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Humans; Imaging, Three-Dimensional; Lung; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Tomography, X-Ray Computed},
	month = {Jun},
	number = {6},
	pages = {490-8},
	pmid = {11437109},
	pst = {ppublish},
	title = {Automatic lung segmentation for accurate quantitation of volumetric {X}-ray {CT} images},
	volume = {20},
	year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.929615}}

@article{Hoffman:2006aa,
	abstract = {With advances in multidetector-row computed tomography (MDCT), it is now possible to image the lung in 10 s or less and accurately extract the lungs, lobes, and airway tree to the fifth- through seventh-generation bronchi and to regionally characterize lung density, texture, ventilation, and perfusion. These methods are now being used to phenotype the lung in health and disease and to gain insights into the etiology of pathologic processes. This article outlines the application of these methodologies with specific emphasis on chronic obstructive pulmonary disease. We demonstrate the use of our methods for assessing regional ventilation and perfusion and demonstrate early data that show, in a sheep model, a regionally intact hypoxic pulmonary vasoconstrictor (HPV) response with an apparent inhibition of HPV regionally in the presence of inflammation. We present the hypothesis that, in subjects with pulmonary emphysema, one major contributing factor leading to parenchymal destruction is the lack of a regional blunting of HPV when the regional hypoxia is related to regional inflammatory events (bronchiolitis or alveolar flooding). If maintaining adequate blood flow to inflamed lung regions is critical to the nondestructive resolution of inflammatory events, the pathologic condition whereby HPV is sustained in regions of inflammation would likely have its greatest effect in the lung apices where blood flow is already reduced in the upright body posture.},
	author = {Hoffman, Eric A and Simon, Brett A and McLennan, Geoffrey},
	date-added = {2015-07-22 02:13:05 +0000},
	date-modified = {2015-07-22 02:13:05 +0000},
	doi = {10.1513/pats.200603-086MS},
	journal = {Proc Am Thorac Soc},
	journal-full = {Proceedings of the American Thoracic Society},
	mesh = {Animals; Humans; Lung; Pulmonary Emphysema; Reproducibility of Results; Tomography, X-Ray Computed},
	month = {Aug},
	number = {6},
	pages = {519-32},
	pmc = {PMC2647643},
	pmid = {16921136},
	pst = {ppublish},
	title = {State of the Art. A structural and functional assessment of the lung via multidetector-row computed tomography: phenotyping chronic obstructive pulmonary disease},
	volume = {3},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1513/pats.200603-086MS}}

@article{Sundaram:2005aa,
	abstract = {The lungs are highly elastic organs, composed of a variety of structures: vasculature, airways and parenchyma. The unique mechanical properties of each of these structures form the composite material of the lung. Numerous pulmonary diseases affect these material properties. Clinically, these structural changes cannot be directly quantified. However, medical imaging modalities such as computed tomography and magnetic resonance imaging can be used to observe lung morphology. It would be helpful to be able to correlate regional morphological changes with changes in pulmonary function. We present an approach toward the quantification of pulmonary deformation via non-rigid registration of serial MR images of the lung using the variational framework implemented in the Insight toolkit. Conventional registration methods, as exemplified by a finite element implementation of the classic elastic matching technique, are shown to perform well over a set of vascular landmarks in the measurement of lung motion. This performance is maintained in an augmented system, which combines inhomogeneous material properties with the use of domain discretizations tailored to reflect the apparent geometry within the image and to reduce background effects. These adaptations lay the groundwork for biomechanical modeling of the lung using the finite element method.},
	author = {Sundaram, Tessa A and Gee, James C},
	date-added = {2015-07-22 02:10:14 +0000},
	date-modified = {2015-07-22 02:10:14 +0000},
	doi = {10.1016/j.media.2005.04.002},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Algorithms; Animals; Biomechanical Phenomena; Elasticity; Finite Element Analysis; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Lung; Magnetic Resonance Imaging; Male; Mice; Models, Biological; Movement; Reproducibility of Results; Respiratory Mechanics; Sensitivity and Specificity; Software; Subtraction Technique},
	month = {Dec},
	number = {6},
	pages = {524-37},
	pmid = {15896996},
	pst = {ppublish},
	title = {Towards a model of lung biomechanics: pulmonary kinematics via registration of serial lung images},
	volume = {9},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2005.04.002}}

@article{Gould:1988aa,
	abstract = {We used a computerized microscopic image analysis system to directly measure the surface area of distal air spaces in methacrylate-embedded blocks randomly selected from inflation-fixed lobes that were resected from 45 patients as treatment of their peripheral lung tumors. In 28 of these patients, a preoperative computer tomography (CT) scan, at 6 and 10 cm below the sternal notch, was used to generate frequency histograms of CT numbers (measured as EMI units), a measure of lung density, in pixels from the lung or lobe that was subsequently resected. A similar CT number histogram was also derived from the lateral two fifths of the area of lobe/lung that was to be resected. The EMI unit that defined the lowest fifth percentile of this latter histogram correlated (n = 28, r = -0.77, p less than 0.001) with the mean value of the surface area of the walls of distal airspaces per unit lung volume (AWUV) in the five 1 mm x 1 mm microscopic fields with the lowest AWUV values, out of the 20 to 35 such fields examined in each patient. In the 34 of the 45 patients in whom we also measured volume-corrected diffusing capacity (DLCO/VA), this also correlated (n = 34, r = 0.84, p less than 0.001) with this value of AWUV, which measures the surface area of airspaces distal to the terminal bronchioles--reflecting an increase in airspace size, a defining characteristic of emphysema. However, a low DLCO/VA is nonspecific, whereas an abnormally low regional lung density is more likely to be specific for emphysema. In addition, highlighting those pixels of the CT display with low CT numbers (i.e., EMI units -500 [air] to -450, where zero = water) can locate areas of macroscopic emphysema, as shown by subsequent pathologic examination. Thus the quantitative CT scan can diagnose, quantitate, and locate mild to moderate emphysema, in humans, in life, noninvasively.},
	author = {Gould, G A and MacNee, W and McLean, A and Warren, P M and Redpath, A and Best, J J and Lamb, D and Flenley, D C},
	date-added = {2015-07-22 02:08:25 +0000},
	date-modified = {2015-07-23 01:49:39 +0000},
	doi = {10.1164/ajrccm/137.2.380},
	journal = {Am Rev Respir Dis},
	journal-full = {The American review of respiratory disease},
	mesh = {Aged; Female; Humans; Lung; Male; Middle Aged; Pneumonectomy; Preoperative Care; Pulmonary Emphysema; Respiratory Function Tests; Tomography, X-Ray Computed},
	month = {Feb},
	number = {2},
	pages = {380-92},
	pmid = {3341629},
	pst = {ppublish},
	title = {{CT} measurements of lung density in life can quantitate distal airspace enlargement--an essential defining feature of human emphysema},
	volume = {137},
	year = {1988},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm/137.2.380}}

@article{Gee:2003aa,
	abstract = {RATIONALE AND OBJECTIVES: The aim of this study was to investigate a method for quantifying lung motion from the registration of successive images in serial magnetic resonance imaging acquisitions during normal respiration.
MATERIALS AND METHODS: Estimates of pulmonary motion were obtained by summing the normalized cross-correlation over serially acquired lung images to identify corresponding locations between the images. The estimated motions were modeled as deformations of an elastic body and thus reflect to a first order approximation the true physical behavior of lung parenchyma. The Lagrangian strain, derived from the calculated motion fields, were used to quantify the tissue deformation induced in the lung over the serial acquisition.
RESULTS: The method was validated on a magnetic resonance imaging study, for which breath-hold images were acquired of a healthy volunteer at different phases of the respiratory cycle. Regional parenchymal strain was observed to be oriented toward the pulmonary hilum, with strain magnitude maximal at the midcycle of the expiratory phase.
CONCLUSION: In vivo magnetic resonance imaging quantification of lung motion holds the potential of providing a new diagnostic dimension in the assessment of pulmonary function, augmenting the information provided by studies of ventilation and perfusion.},
	author = {Gee, James and Sundaram, Tessa and Hasegawa, Ichiro and Uematsu, Hidemasa and Hatabu, Hiroto},
	date-added = {2015-07-22 02:05:59 +0000},
	date-modified = {2015-07-22 02:05:59 +0000},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Adult; Biomechanical Phenomena; Humans; Lung; Magnetic Resonance Imaging; Male; Movement; Respiratory Mechanics},
	month = {Oct},
	number = {10},
	pages = {1147-52},
	pmid = {14587632},
	pst = {ppublish},
	title = {Characterization of regional pulmonary mechanics from serial magnetic resonance imaging data},
	volume = {10},
	year = {2003}}

@article{Ederle:2003aa,
	abstract = {The aim of this study was to improve the understanding of interdependencies of dynamic changes in central airway dimensions, lung area and lung density on HRCT. The HRCT scans of 156 patients obtained at full inspiratory and expiratory position were evaluated retrospectively. Patients were divided into four groups according to lung function tests: normal subjects ( n=47); obstructive ( n=74); restrictive ( n=19); or mixed ventilatory impairment ( n=16). Mean lung density (MLD) was correlated with cross-sectional area of the lung (CSA(L)), cross-sectional area of the trachea (CSA(T)) and diameter of main-stem bronchi (D(B)). The CSA(L) was correlated with CSA(T) and D(B). MLD correlated with CSA(L) in normal subjects ( r=-0.66, p<0.0001) and patients with obstructive ( r=-0.62, p<0.0001), restrictive ( r=-0.83, p<0.0001) and mixed ventilatory impairment ( r=-0.86, p<0.0001). The MLD correlated with CSA(T) in the control group ( r=-0.50, p<0.0001) and in patients with obstructive lung impairment ( r-0.27, p<0.05). In patients with normal lung function a correlation between MLD and D(B) was found ( r=-0.52, p<0.0001). CSA(L) and CSA(T) correlated in the control group ( r=0.67, p<0.0001) and in patients with obstructive lung disease ( r=0.51, p<0.0001). The CSA(L) and D(B) correlated in the control group ( r=0.42, p<0.0001) and in patients with obstructive lung disease ( r=0.24, p<0.05). Correlations for patients with restrictive and mixed lung disease were constantly lower. Dependencies between central and peripheral airway dimensions and lung parenchyma are demonstrated by HRCT. Best correlations are observed in normal subjects and patients with obstructive lung disease. Based on these findings we postulate that the dependencies are the result of air-flow and pressure patterns.},
	author = {Ederle, J R and Heussel, C P and Hast, J and Fischer, B and Van Beek, E J R and Ley, S and Thelen, M and Kauczor, H U},
	date-added = {2015-07-22 02:04:56 +0000},
	date-modified = {2015-07-22 02:04:56 +0000},
	doi = {10.1007/s00330-003-1909-5},
	journal = {Eur Radiol},
	journal-full = {European radiology},
	mesh = {Adolescent; Adult; Aged; Aged, 80 and over; Exhalation; Female; Humans; Inhalation; Lung; Lung Diseases; Male; Middle Aged; Retrospective Studies; Tomography, X-Ray Computed},
	month = {Nov},
	number = {11},
	pages = {2454-61},
	pmid = {12811503},
	pst = {ppublish},
	title = {Evaluation of changes in central airway dimensions, lung area and mean lung density at paired inspiratory/expiratory high-resolution computed tomography},
	volume = {13},
	year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00330-003-1909-5}}

@article{Dougherty:2006aa,
	abstract = {RATIONALE AND OBJECTIVES: Serial CT lung studies are difficult to compare due to misregistration between image sets. An optical flow method (OFM) was adapted for use on CT lung images to register images and visualize changes between studies. Three applications were investigated: lung nodule assessment; evaluation of pulmonary enhancement; and functional changes due to air trapping.
MATERIALS AND METHODS: From an initial clinical study, a follow-up study was created by digitally manipulating the images to simulate patient positioning errors and nodule growth. Nodule growth was measured from the temporal subtraction of registered images. In application to the assessment of pulmonary enhancement, pre and postcontrast images from a patient with acute pulmonary embolism (PE) were registered. A map of the perfused blood volume was computed from the ratio of aligned lung volumes. Functional changes in the lung were demonstrated using images from a patient with air trapping. End-inspiratory and end-expiratory volumes were aligned and displacement fields estimated using the OFM. Principal strains were computed from the displacement fields.
RESULTS: All image volumes were aligned with at least 0.95 correlation. OFM estimates of displacement showed excellent agreement with the prescribed displacements with 0.33 pixel RMS error. Nodule growth was evident in the presence of significant positioning errors. In the PE case, enhancement ratios indicated a hypoperfused area consistent with an occlusive hypodense filling defect. For the air trapping case, a strain map showed functional changes along the interface of the air trap.
CONCLUSIONS: The OFM can facilitate the detection and quantification of changes between serial CT lung studies.},
	author = {Dougherty, Lawrence and Torigian, Drew A and Affusso, John D and Asmuth, Jane C and Gefter, Warren B},
	date-added = {2015-07-22 02:04:31 +0000},
	date-modified = {2015-07-23 01:49:10 +0000},
	doi = {10.1016/j.acra.2005.09.081},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Humans; Lung Neoplasms; Pulmonary Embolism; Radiographic Image Interpretation, Computer-Assisted; Retrospective Studies; Tomography, X-Ray Computed},
	month = {Jan},
	number = {1},
	pages = {14-23},
	pmid = {16399029},
	pst = {ppublish},
	title = {Use of an optical flow method for the analysis of serial {CT} lung images},
	volume = {13},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2005.09.081}}

@article{Lange:1999aa,
	abstract = {Thirty-two magnetic resonance imaging examinations of the lungs were performed in 16 subjects after inhalation of 1-2 L of helium 3 gas that was laser polarized to 10%-25%. The distribution of the gas was generally uniform, with visualization of the fissures in most cases. Ventilation defects were demonstrated in smokers and in a subject with allergies. The technique has potential for evaluating small airways disease.},
	author = {de Lange, E E and Mugler, 3rd, J P and Brookeman, J R and Knight-Scott, J and Truwit, J D and Teates, C D and Daniel, T M and Bogorad, P L and Cates, G D},
	date-added = {2015-07-22 02:04:07 +0000},
	date-modified = {2015-07-23 01:51:28 +0000},
	doi = {10.1148/radiology.210.3.r99fe08851},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Administration, Inhalation; Adult; Aged; Asthma; Female; Helium; Humans; Image Processing, Computer-Assisted; Isotopes; Lasers; Lung; Magnetic Resonance Imaging; Male; Middle Aged; Observer Variation; Oxygen; Pulmonary Emphysema; Respiration; Rhinitis, Allergic, Seasonal; Smoking},
	month = {Mar},
	number = {3},
	pages = {851-7},
	pmid = {10207491},
	pst = {ppublish},
	title = {Lung air spaces: {MR} imaging evaluation with hyperpolarized 3{H}e gas},
	volume = {210},
	year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiology.210.3.r99fe08851}}

@article{Lange:2007aa,
	abstract = {BACKGROUND: It is unknown whether focal changes of airflow obstruction within the lungs of patients with asthma vary or are fixed in location with time or repeated bronchoconstriction. With hyperpolarized helium-3 magnetic resonance (H(3)HeMR) imaging, the airspaces are depicted and focal areas of airflow obstruction are shown as "ventilation defects."
OBJECTIVE: To investigate the regional changes of airflow obstruction with time and repeated bronchoconstriction.
METHODS: H(3)HeMR and spirometry were performed before (pre) and immediately after (post) methacholine challenge in 10 young patients with asthma on 2 days that were 7-476 days (mean, 185.3 +/- 37.2 days) apart. Pair-wise image comparisons were performed to determine the change in location of ventilation defects within the lung and their change in size.
RESULTS: When comparing premethacholine versus premethacholine and postmethacholine versus post-methacholine images of the 2 days, 41% +/- 10% and 69% +/- 5% (P = .017) of defects, respectively, were in the same location, and of those, 69% +/- 12% and 43% +/- 5% (P = .022), respectively, did not change size. Comparing premethacholine versus postmethacholine images, 58% +/- 9% of defects were in the same location on day 1 and 73% +/- 7% (P = .088) on day 2. On both days, the percent increase in defect number from premethacholine to postmethacholine was much greater than the percent decrease in spirometric values (P < .001).
CONCLUSION: Many of the ventilation defects persisted or recurred in the same location with time or repeated bronchoconstriction, suggesting that the regional changes of airflow obstruction are relatively fixed within the lung.
CLINICAL IMPLICATIONS: The findings give new insight into the regional airflow variability within the lungs of patients with asthma.},
	author = {de Lange, Eduard E and Altes, Talissa A and Patrie, James T and Parmar, Jaywant and Brookeman, James R and Mugler, 3rd, John P and Platts-Mills, Thomas A E},
	date-added = {2015-07-22 02:03:55 +0000},
	date-modified = {2015-07-22 02:03:55 +0000},
	doi = {10.1016/j.jaci.2006.12.659},
	journal = {J Allergy Clin Immunol},
	journal-full = {The Journal of allergy and clinical immunology},
	mesh = {Adult; Airway Obstruction; Asthma; Bronchial Provocation Tests; Bronchoconstrictor Agents; Female; Helium; Humans; Lung; Magnetic Resonance Imaging; Methacholine Chloride; Middle Aged; Radioisotopes; Radiopharmaceuticals; Spirometry},
	month = {May},
	number = {5},
	pages = {1072-8},
	pmid = {17353032},
	pst = {ppublish},
	title = {The variability of regional airflow obstruction within the lungs of patients with asthma: assessment with hyperpolarized helium-3 magnetic resonance imaging},
	volume = {119},
	year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.jaci.2006.12.659}}

@article{Lange:2006aa,
	abstract = {BACKGROUND: Accurate characterization of asthma severity is difficult due to the variability of symptoms. Hyperpolarized helium-3 MRI (H(3)HeMR) is a new technique in which the airspaces are visualized, depicting regions with airflow obstruction as "ventilation defects." The objective of this study was to compare the extent of H(3)HeMR ventilation defects with measures of asthma severity and spirometry.
METHODS: Patients with a physician diagnosis of asthma and normal control subjects underwent H(3)HeMR. For each person, the number and size of ventilation defects were scored and the average number of ventilation defects per slice (VDS) was calculated. The correlations of the imaging findings with measures of asthma severity and spirometry were determined.
RESULTS: There were 58 patients with asthma (mild-intermittent, n = 13; mild-persistent, n = 13; moderate-persistent, n = 20; and severe-persistent, n = 12) and 18 control subjects. Mean +/- SE VDS for asthmatics was significantly greater than for control subjects (0.99 +/- 0.15 vs 0.26 +/- 0.22, p = 0.004). Among asthmatics, VDS was significantly higher for the group with moderate-persistent and severe-persistent disease than for the group with mild-intermittent and mild-persistent disease (1.37 +/- 0.24 vs 0.53 +/- 0.12, p < 0.001). VDS correlated significantly with FEV(1)/FVC (r = - 0.51, p = 0.002), forced expiratory flow between 25% and 75% from the beginning of FVC (FEF(25-75%)) percentage of predicted for height, sex, and race (%predicted) [r = - 0.50, p = 0.001], and FEV(1) %predicted (r = - 0.40, p = 0.002), but not with FVC %predicted (r = - 0.26, p = 0.057) and peak expiratory flow %predicted (r = - 0.16, p = 0.231). Many asthmatics had an elevated VDS, but their spirometric indexes, except FEF(25%-75%), were normal. Most ventilation defects were < 3 cm in size for all asthmatics. In the group of patients with moderate-to-severe persistent asthma, there were more defects > or =3 cm than in the group with mild-intermittent and mild-persistent disease (p = 0.021).
CONCLUSIONS: Regional changes of airflow obstruction in asthmatics depicted by H(3)HeMR correlate with measures of asthma severity and spirometry.},
	author = {de Lange, Eduard E and Altes, Talissa A and Patrie, James T and Gaare, John D and Knake, Jeffrey J and Mugler, 3rd, John P and Platts-Mills, Thomas A},
	date-added = {2015-07-22 01:55:44 +0000},
	date-modified = {2015-07-23 01:51:40 +0000},
	doi = {10.1378/chest.130.4.1055},
	journal = {Chest},
	journal-full = {Chest},
	mesh = {Adolescent; Adult; Airway Obstruction; Asthma; Forced Expiratory Volume; Helium; Humans; Image Enhancement; Image Processing, Computer-Assisted; Isotopes; Lung; Magnetic Resonance Imaging; Male; Maximal Midexpiratory Flow Rate; Spirometry; Statistics as Topic; Vital Capacity},
	month = {Oct},
	number = {4},
	pages = {1055-62},
	pmid = {17035438},
	pst = {ppublish},
	title = {Evaluation of asthma with hyperpolarized helium-3 {MRI}: correlation with clinical severity and spirometry},
	volume = {130},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.130.4.1055}}

@article{Coxson:1999aa,
	abstract = {Quantitative analysis of computed tomography (CT) has been combined with a stereologically based histologic analysis of lung structure to assess regional lung inflation and the structural features of the lung parenchyma. In this study, CT measurements of lung inflation were compared with histologic estimates of surface area in order to develop prediction equations that allow lung surface to volume ratio and surface area to be predicted from an analysis of the CT scan. The results show that mild emphysema is associated with an increase in lung volume and a reduction in surface to volume ratio, whereas surface area and tissue weight were only decreased in severe disease. The CT predicted surface to volume ratio correlated with histology, and both predicted and measured surface areas correlated with the diffusing capacity. We conclude that this CT analysis can be used to monitor the progression of emphysematous lung destruction in individual patients, and to assess the impact of both surgical and medical treatments for emphysema.},
	author = {Coxson, H O and Rogers, R M and Whittall, K P and D'yachkova, Y and Par{\'e}, P D and Sciurba, F C and Hogg, J C},
	date-added = {2015-07-22 01:54:36 +0000},
	date-modified = {2015-07-22 01:54:36 +0000},
	doi = {10.1164/ajrccm.159.3.9805067},
	journal = {Am J Respir Crit Care Med},
	journal-full = {American journal of respiratory and critical care medicine},
	mesh = {Aged; Female; Humans; Lung; Lung Volume Measurements; Male; Middle Aged; Pulmonary Emphysema; Tomography, X-Ray Computed},
	month = {Mar},
	number = {3},
	pages = {851-6},
	pmid = {10051262},
	pst = {ppublish},
	title = {A quantification of the lung surface area in emphysema using computed tomography},
	volume = {159},
	year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1164/ajrccm.159.3.9805067}}

@article{Coxson:2005aa,
	abstract = {Chronic obstructive pulmonary disease (COPD) is a complex genetic disorder in which environmental factors, such as tobacco smoke, interact with genetic susceptibility to cause disease. Airway obstruction in COPD is due to an exaggerated inflammatory response that ultimately destroys the lung parenchyma (emphysema) and increases airway resistance by remodeling the airway wall. Until recently, assessment of these disease processes required the examination of resected tissue. However, computed tomography (CT) now allows researchers to measure the structure of the lung parenchyma and airway wall without having to remove the tissue. This review describes some of the new CT techniques for quantitative assessment of lung structure. These techniques are extremely important to study the pathogenesis of COPD as well as differentiate patients with predominantly emphysema disease from those with airway wall remodeling, and to assess the effects of therapeutic interventions.},
	author = {Coxson, Harvey O and Rogers, Robert M},
	date-added = {2015-07-22 01:53:29 +0000},
	date-modified = {2015-07-23 01:48:20 +0000},
	doi = {10.1055/s-2005-869540},
	journal = {Semin Respir Crit Care Med},
	journal-full = {Seminars in respiratory and critical care medicine},
	mesh = {Female; Humans; Male; Pulmonary Disease, Chronic Obstructive; Pulmonary Emphysema; Radiographic Image Enhancement; Sensitivity and Specificity; Severity of Illness Index; Tomography, X-Ray Computed},
	month = {Apr},
	number = {2},
	pages = {211-20},
	pmid = {16088438},
	pst = {ppublish},
	title = {New concepts in the radiological assessment of {COPD}},
	volume = {26},
	year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1055/s-2005-869540}}

@article{Coxson:1995aa,
	abstract = {The total and regional lung volumes were estimated from computed tomography (CT), and the pleural pressure gradient was determined by using the milliliters of gas per gram of tissue estimated from the X-ray attenuation values and the pressure-volume curve of the lung. The data show that CT accurately estimated the volume of the resected lobe but overestimated its weight by 24 +/- 19%. The volume of gas per gram of tissue was less in the gravity-dependent regions due to a pleural pressure gradient of 0.24 +/- 0.08 cmH2O/cm of descent in the thorax. The proportion of tissue to air obtained with CT was similar to that obtained by quantitative histology. We conclude that the CT scan can be used to estimate total and regional lung volumes and that measurements of the proportions of tissue and air within the thorax by CT can be used in conjunction with quantitative histology to evaluate lung structure.},
	author = {Coxson, H O and Mayo, J R and Behzad, H and Moore, B J and Verburgt, L M and Staples, C A and Par{\'e}, P D and Hogg, J C},
	date-added = {2015-07-22 01:52:28 +0000},
	date-modified = {2015-07-22 01:52:28 +0000},
	journal = {J Appl Physiol (1985)},
	journal-full = {Journal of applied physiology (Bethesda, Md. : 1985)},
	mesh = {Biopsy; Humans; Lung; Middle Aged; Respiratory Function Tests; Tomography, X-Ray Computed},
	month = {Nov},
	number = {5},
	pages = {1525-30},
	pmid = {8594009},
	pst = {ppublish},
	title = {Measurement of lung expansion with computed tomography and comparison with quantitative histology},
	volume = {79},
	year = {1995}}

@article{Cook:2007aa,
	abstract = {Assessing the quality of motion estimation in the lung remains challenging. We approach the problem by imaging isolated porcine lungs within an artificial thorax with four-dimensional computed tomography (4DCT). Respiratory kinematics are estimated via pairwise non-rigid registration using different metrics and image resolutions. Landmarks are manually identified on the images and used to assess accuracy by comparing known displacements to the registration-derived displacements. We find that motion quantitation becomes less precise as the inflation interval between images increases. In addition, its sensitivity to image resolution varies anatomically. Mutual information and cross-correlation perform similarly, while mean squares is significantly poorer. However, none of the metrics compensate for the difficulty of registering over a large inflation interval. We intend to use the results of these experiments to more effectively and efficiently quantify pulmonary kinematics in future, and to explore additional parameter combinations.},
	author = {Cook, Tessa Sundaram and Tustison, Nicholas and Biederer, J{\"u}rgen and Tetzlaff, Ralf and Gee, James},
	date-added = {2015-07-22 01:52:13 +0000},
	date-modified = {2015-07-22 01:52:13 +0000},
	journal = {Med Image Comput Comput Assist Interv},
	journal-full = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	mesh = {Algorithms; Animals; Biomechanical Phenomena; Computer Simulation; Imaging, Three-Dimensional; In Vitro Techniques; Lung; Models, Biological; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Respiratory Mechanics; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique; Swine; Tomography, X-Ray Computed},
	number = {Pt 1},
	pages = {817-24},
	pmid = {18051134},
	pst = {ppublish},
	title = {How do registration parameters affect quantitation of lung kinematics?},
	volume = {10},
	year = {2007}}

@article{Chen:1989aa,
	abstract = {Following B.B. Mandelbrot's fractal theory (1982), it was found that the fractal dimension could be obtained in medical images by the concept of fractional Brownian motion. An estimation concept for determination of the fractal dimension based upon the concept of fractional Brownian motion is discussed. Two applications are found: (1) classification; (2) edge enhancement and detection. For the purpose of classification, a normalized fractional Brownian motion feature vector is defined from this estimation concept. It represented the normalized average absolute intensity difference of pixel pairs on a surface of different scales. The feature vector uses relatively few data items to represent the statistical characteristics of the medial image surface and is invariant to linear intensity transformation. For edge enhancement and detection application, a transformed image is obtained by calculating the fractal dimension of each pixel over the whole medical image. The fractal dimension value of each pixel is obtained by calculating the fractal dimension of 7x7 pixel block centered on this pixel.},
	author = {Chen, C C and Daponte, J S and Fox, M D},
	date-added = {2015-07-22 01:51:25 +0000},
	date-modified = {2015-07-22 01:51:25 +0000},
	doi = {10.1109/42.24861},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	number = {2},
	pages = {133-42},
	pmid = {18230510},
	pst = {ppublish},
	title = {Fractal feature analysis and classification in medical imaging},
	volume = {8},
	year = {1989},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/42.24861}}

@article{Aykac:2003aa,
	abstract = {The lungs exchange air with the external environment via the pulmonary airways. Computed tomography (CT) scanning can be used to obtain detailed images of the pulmonary anatomy, including the airways. These images have been used to measure airway geometry, study airway reactivity, and guide surgical interventions. Prior to these applications, airway segmentation can be used to identify the airway lumen in the CT images. Airway tree segmentation can be performed manually by an image analyst, but the complexity of the tree makes manual segmentation tedious and extremely time-consuming. We describe a fully automatic technique for segmenting the airway tree in three-dimensional (3-D) CT images of the thorax. We use grayscale morphological reconstruction to identify candidate airways on CT slices and then reconstruct a connected 3-D airway tree. After segmentation, we estimate airway branchpoints based on connectivity changes in the reconstructed tree. Compared to manual analysis on 3-mm-thick electron-beam CT images, the automatic approach has an overall airway branch detection sensitivity of approximately 73%.},
	author = {Aykac, Deniz and Hoffman, Eric A and McLennan, Geoffrey and Reinhardt, Joseph M},
	date-added = {2015-07-22 01:50:30 +0000},
	date-modified = {2015-07-23 01:47:17 +0000},
	doi = {10.1109/TMI.2003.815905},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Anatomy, Cross-Sectional; Bronchography; Humans; Imaging, Three-Dimensional; Lung; Pulmonary Alveoli; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Tomography, X-Ray Computed; Trachea},
	month = {Aug},
	number = {8},
	pages = {940-50},
	pmid = {12906248},
	pst = {ppublish},
	title = {Segmentation and analysis of the human airway tree from three-dimensional {X}-ray {CT} images},
	volume = {22},
	year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2003.815905}}

@article{Avants:2008aa,
	abstract = {One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals.},
	author = {Avants, B B and Epstein, C L and Grossman, M and Gee, J C},
	date-added = {2015-07-22 01:50:15 +0000},
	date-modified = {2015-07-22 01:50:15 +0000},
	doi = {10.1016/j.media.2007.06.004},
	journal = {Med Image Anal},
	journal-full = {Medical image analysis},
	mesh = {Algorithms; Atrophy; Cerebral Cortex; Dementia; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging},
	month = {Feb},
	number = {1},
	pages = {26-41},
	pmc = {PMC2276735},
	pmid = {17659998},
	pst = {ppublish},
	title = {Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain},
	volume = {12},
	year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.media.2007.06.004}}

@article{Avants:2004aa,
	abstract = {The goal of this research is to promote variational methods for anatomical averaging that operate within the space of the underlying image registration problem. This approach is effective when using the large deformation viscous framework, where linear averaging is not valid, or in the elastic case. The theory behind this novel atlas building algorithm is similar to the traditional pairwise registration problem, but with single image forces replaced by average forces. These group forces drive an average transport ordinary differential equation allowing one to estimate the geodesic that moves an image toward the mean shape configuration. This model gives large deformation atlases that are optimal with respect to the shape manifold as defined by the data and the image registration assumptions. We use the techniques in the large deformation context here, but they also pertain to small deformation atlas construction. Furthermore, a natural, inherently inverse consistent image registration is gained for free, as is a tool for constant arc length geodesic shape interpolation. The geodesic atlas creation algorithm is quantitatively compared to the Euclidean anatomical average to elucidate the need for optimized atlases. The procedures generate improved average representations of highly variable anatomy from distinct populations.},
	author = {Avants, Brian and Gee, James C},
	date-added = {2015-07-22 01:50:03 +0000},
	date-modified = {2015-07-22 01:50:03 +0000},
	doi = {10.1016/j.neuroimage.2004.07.010},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Animals; Brain; Brain Mapping; Databases, Factual; Humans; Linear Models; Magnetic Resonance Imaging; Models, Anatomic; Models, Statistical; Pan troglodytes; Population},
	pages = {S139-50},
	pmid = {15501083},
	pst = {ppublish},
	title = {Geodesic estimation for large deformation anatomical shape averaging and interpolation},
	volume = {23 Suppl 1},
	year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2004.07.010}}

@article{Altes:2001aa,
	abstract = {Asthma is a disease characterized by chronic inflammation and reversible obstruction of the small airways resulting in impaired pulmonary ventilation. Hyperpolarized 3He magnetic resonance (MR) lung imaging is a new technology that provides a detailed image of lung ventilation. Hyperpolarized 3He lung imaging was performed in 10 asthmatics and 10 healthy subjects. Seven asthmatics had ventilation defects distributed throughout the lungs compared with none of the normal subjects. These ventilation defects were more numerous and larger in the two symptomatic asthmatics who had abnormal spirometry. Ventilation defects studied over time demonstrated no change in appearance over 30-60 minutes. One asthmatic subject was studied twice in a three-week period and had ventilation defects which resolved and appeared in that time. This same subject was studied before and after bronchodilator therapy, and all ventilation defects resolved after therapy. Hyperpolarized 3He lung imaging can detect the small, reversible ventilation defects that characterize asthma. The ability to visualize lung ventilation offers a direct method of assessing asthmatics and their response to therapy.},
	author = {Altes, T A and Powers, P L and Knight-Scott, J and Rakes, G and Platts-Mills, T A and de Lange, E E and Alford, B A and Mugler, 3rd, J P and Brookeman, J R},
	date-added = {2015-07-22 01:49:52 +0000},
	date-modified = {2015-07-23 01:46:15 +0000},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Adult; Asthma; Female; Helium; Humans; Image Enhancement; Image Processing, Computer-Assisted; Isotopes; Magnetic Resonance Imaging; Male; Observer Variation; Pulmonary Gas Exchange; Spirometry},
	month = {Mar},
	number = {3},
	pages = {378-84},
	pmid = {11241810},
	pst = {ppublish},
	title = {Hyperpolarized {3He} {MR} lung ventilation imaging in asthmatics: preliminary findings},
	volume = {13},
	year = {2001}}

@article{Ashburner:2012aa,
	abstract = {Karl Friston began the SPM project around 1991. The rest is history.},
	author = {Ashburner, John},
	date-added = {2015-07-22 00:40:39 +0000},
	date-modified = {2015-07-23 01:46:23 +0000},
	doi = {10.1016/j.neuroimage.2011.10.025},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Brain; Brain Mapping; History, 20th Century; History, 21st Century; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Software},
	month = {Aug},
	number = {2},
	pages = {791-800},
	pmc = {PMC3480642},
	pmid = {22023741},
	pst = {ppublish},
	title = {{SPM}: a history},
	volume = {62},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2011.10.025}}

@article{Cox:2012aa,
	abstract = {AFNI is an open source software package for the analysis and display of functional MRI data. It originated in 1994 to meet the specific needs of researchers at the Medical College of Wisconsin, in particular the mapping of activation maps to Talairach-Tournoux space, but has been expanded steadily since then into a wide-ranging set of tool for FMRI data analyses. AFNI was the first platform for real-time 3D functional activation and registration calculations. One of AFNI's main strengths is its flexibility and transparency. In recent years, significant efforts have been made to increase the user-friendliness of AFNI's FMRI processing stream, with the introduction of "super-scripts" to setup the entire analysis, and graphical front-ends for these managers.},
	author = {Cox, Robert W},
	date-added = {2015-07-22 00:40:13 +0000},
	date-modified = {2015-07-23 01:47:45 +0000},
	doi = {10.1016/j.neuroimage.2011.08.056},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Brain; History, 20th Century; History, 21st Century; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Software},
	month = {Aug},
	number = {2},
	pages = {743-7},
	pmc = {PMC3246532},
	pmid = {21889996},
	pst = {ppublish},
	title = {{AFNI}: what a long strange trip it's been},
	volume = {62},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2011.08.056}}

@article{Jenkinson:2012aa,
	abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on "20 years of fMRI" we have been asked to write about the history, developments and current status of FSL. We also include some descriptions of parts of FSL that are not well covered in the existing literature. We hope that some of this content might be of interest to users of FSL, and also maybe to new research groups considering creating, releasing and supporting new software packages for brain image analysis.},
	author = {Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E J and Woolrich, Mark W and Smith, Stephen M},
	date-added = {2015-07-22 00:39:55 +0000},
	date-modified = {2015-07-23 01:50:25 +0000},
	doi = {10.1016/j.neuroimage.2011.09.015},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Brain; Brain Mapping; Diffusion Magnetic Resonance Imaging; History, 20th Century; History, 21st Century; Humans; Image Processing, Computer-Assisted; Software},
	month = {Aug},
	number = {2},
	pages = {782-90},
	pmid = {21979382},
	pst = {ppublish},
	title = {{FSL}},
	volume = {62},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2011.09.015}}

@article{Fischl:2012aa,
	abstract = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source.},
	author = {Fischl, Bruce},
	date-added = {2015-07-22 00:39:07 +0000},
	date-modified = {2015-07-23 01:49:27 +0000},
	doi = {10.1016/j.neuroimage.2012.01.021},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Brain; Brain Mapping; History, 20th Century; History, 21st Century; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Software},
	month = {Aug},
	number = {2},
	pages = {774-81},
	pmc = {PMC3685476},
	pmid = {22248573},
	pst = {ppublish},
	title = {{FreeSurfer}},
	volume = {62},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2012.01.021}}

@article{Menze:2014aa,
	abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all subregions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
	author = {Menze, Bjoern and Reyes, Mauricio and Van Leemput, Koen},
	date-added = {2015-07-22 00:14:43 +0000},
	date-modified = {2015-07-23 01:53:07 +0000},
	doi = {10.1109/TMI.2014.2377694},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	month = {Dec},
	pmid = {25494501},
	pst = {aheadofprint},
	title = {The Multimodal Brain Tumor Image Segmentation Benchmark ({BRATS})},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2014.2377694}}

@article{Wang:2013aa,
	abstract = {Label fusion based multi-atlas segmentation has proven to be one of the most competitive techniques for medical image segmentation. This technique transfers segmentations from expert-labeled images, called atlases, to a novel image using deformable image registration. Errors produced by label transfer are further reduced by label fusion that combines the results produced by all atlases into a consensus solution. Among the proposed label fusion strategies, weighted voting with spatially varying weight distributions derived from atlas-target intensity similarity is a simple and highly effective label fusion technique. However, one limitation of most weighted voting methods is that the weights are computed independently for each atlas, without taking into account the fact that different atlases may produce similar label errors. To address this problem, we recently developed the joint label fusion technique and the corrective learning technique, which won the first place of the 2012 MICCAI Multi-Atlas Labeling Challenge and was one of the top performers in 2013 MICCAI Segmentation: Algorithms, Theory and Applications (SATA) challenge. To make our techniques more accessible to the scientific research community, we describe an Insight-Toolkit based open source implementation of our label fusion methods. Our implementation extends our methods to work with multi-modality imaging data and is more suitable for segmentation problems with multiple labels. We demonstrate the usage of our tools through applying them to the 2012 MICCAI Multi-Atlas Labeling Challenge brain image dataset and the 2013 SATA challenge canine leg image dataset. We report the best results on these two datasets so far.},
	author = {Wang, Hongzhi and Yushkevich, Paul A},
	date-added = {2015-07-22 00:12:46 +0000},
	date-modified = {2015-07-22 00:12:46 +0000},
	doi = {10.3389/fninf.2013.00027},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	keywords = {Insight-Toolkit; corrective learning; joint label fusion; multi-atlas label fusion; open source implementation},
	pages = {27},
	pmc = {PMC3837555},
	pmid = {24319427},
	pst = {epublish},
	title = {Multi-atlas segmentation with joint label fusion and corrective learning-an open source implementation},
	volume = {7},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fninf.2013.00027}}

@article{Avants:2010aa,
	abstract = {We evaluate the impact of template choice on template-based segmentation of the hippocampus in epilepsy. Four dataset-specific strategies are quantitatively contrasted: the "closest to average" individual template, the average shape version of the closest to average template, a best appearance template and the best appearance and shape template proposed here and implemented in the open source toolkit Advanced Normalization Tools (ANTS). The cross-correlation similarity metric drives the correspondence model and is used consistently to determine the optimal appearance. Minimum shape distance in the diffeomorphic space determines optimal shape. Our evaluation results show that, with respect to gold-standard manual labeling of hippocampi in epilepsy, optimal shape and appearance template construction outperforms the other strategies for gaining data-derived templates. Our results also show the improvement is most significant on the diseased side and insignificant on the healthy side. Thus, the importance of the template increases when used to study pathology and may be less critical for normal control studies. Furthermore, explicit geometric optimization of the shape component of the unbiased template positively impacts the study of diseased hippocampi.},
	author = {Avants, Brian B and Yushkevich, Paul and Pluta, John and Minkoff, David and Korczykowski, Marc and Detre, John and Gee, James C},
	date-added = {2015-07-22 00:11:08 +0000},
	date-modified = {2015-07-22 00:11:08 +0000},
	doi = {10.1016/j.neuroimage.2009.09.062},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Atlases as Topic; Epilepsy; Hippocampus; Humans; Image Interpretation, Computer-Assisted},
	month = {Feb},
	number = {3},
	pages = {2457-66},
	pmc = {PMC2818274},
	pmid = {19818860},
	pst = {ppublish},
	title = {The optimal template effect in hippocampus studies of diseased populations},
	volume = {49},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2009.09.062}}

@article{Klein:2009aa,
	abstract = {All fields of neuroscience that employ brain imaging need to communicate their results with reference to anatomical regions. In particular, comparative morphometry and group analysis of functional and physiological data require coregistration of brains to establish correspondences across brain structures. It is well established that linear registration of one brain to another is inadequate for aligning brain structures, so numerous algorithms have emerged to nonlinearly register brains to one another. This study is the largest evaluation of nonlinear deformation algorithms applied to brain image registration ever conducted. Fourteen algorithms from laboratories around the world are evaluated using 8 different error measures. More than 45,000 registrations between 80 manually labeled brains were performed by algorithms including: AIR, ANIMAL, ART, Diffeomorphic Demons, FNIRT, IRTK, JRD-fluid, ROMEO, SICLE, SyN, and four different SPM5 algorithms ("SPM2-type" and regular Normalization, Unified Segmentation, and the DARTEL Toolbox). All of these registrations were preceded by linear registration between the same image pairs using FLIRT. One of the most significant findings of this study is that the relative performances of the registration methods under comparison appear to be little affected by the choice of subject population, labeling protocol, and type of overlap measure. This is important because it suggests that the findings are generalizable to new subject populations that are labeled or evaluated using different labeling protocols. Furthermore, we ranked the 14 methods according to three completely independent analyses (permutation tests, one-way ANOVA tests, and indifference-zone ranking) and derived three almost identical top rankings of the methods. ART, SyN, IRTK, and SPM's DARTEL Toolbox gave the best results according to overlap and distance measures, with ART and SyN delivering the most consistently high accuracy across subjects and label sets. Updates will be published on the http://www.mindboggle.info/papers/ website.},
	author = {Klein, Arno and Andersson, Jesper and Ardekani, Babak A and Ashburner, John and Avants, Brian and Chiang, Ming-Chang and Christensen, Gary E and Collins, D Louis and Gee, James and Hellier, Pierre and Song, Joo Hyun and Jenkinson, Mark and Lepage, Claude and Rueckert, Daniel and Thompson, Paul and Vercauteren, Tom and Woods, Roger P and Mann, J John and Parsey, Ramin V},
	date-added = {2015-07-22 00:05:12 +0000},
	date-modified = {2015-07-23 01:50:42 +0000},
	doi = {10.1016/j.neuroimage.2008.12.037},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Adult; Algorithms; Artificial Intelligence; Brain; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Nonlinear Dynamics; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique},
	month = {Jul},
	number = {3},
	pages = {786-802},
	pmc = {PMC2747506},
	pmid = {19195496},
	pst = {ppublish},
	title = {Evaluation of 14 nonlinear deformation algorithms applied to human brain {MRI} registration},
	volume = {46},
	year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2008.12.037}}

@article{Tustison:2014ad,
	abstract = {Recent discussions within the neuroimaging community have highlighted the problematic presence of selection bias in experimental design. Although initially centering on the selection of voxels during the course of fMRI studies, we demonstrate how this bias can potentially corrupt voxel-based analyses. For such studies, template-based registration plays a critical role in which a representative template serves as the normalized space for group alignment. A standard approach maps each subject's image to a representative template before performing statistical comparisons between different groups. We analytically demonstrate that in these scenarios the popular sum of squared difference (SSD) intensity metric, implicitly surrogating as a quantification of anatomical alignment, instead explicitly maximizes effect size--an experimental design flaw referred to as "circularity bias." We illustrate how this selection bias varies in strength with the similarity metric used during registration under the hypothesis that while SSD-related metrics, such as Demons, will manifest similar effects, other metrics which are not formulated based on absolute intensity differences will produce less of an effect. Consequently, given the variability in voxel-based analysis outcomes with similarity metric choice, we caution researchers specifically in the use of SSD and SSD-related measures where normalization and statistical analysis involve the same image set. Instead, we advocate a more cautious approach where normalization of the individual subject images to the reference space occurs through corresponding image sets which are independent of statistical testing. Alternatively, one can use similarity terms that are less sensitive to this bias.},
	author = {Tustison, Nicholas J and Avants, Brian B and Cook, Philip A and Kim, Junghoon and Whyte, John and Gee, James C and Stone, James R},
	date-added = {2014-12-07 05:50:19 +0000},
	date-modified = {2014-12-07 05:50:19 +0000},
	doi = {10.1002/hbm.22211},
	journal = {Hum Brain Mapp},
	journal-full = {Human brain mapping},
	keywords = {image registration; methodological bias; morphometry; nonindependent analysis},
	month = {Mar},
	number = {3},
	pages = {745-59},
	pmid = {23151955},
	pst = {ppublish},
	title = {Logical circularity in voxel-based analysis: normalization strategy may induce statistical bias},
	volume = {35},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/hbm.22211}}

@article{Durst:2014aa,
	abstract = {INTRODUCTION: Gliomas remain difficult to treat, in part, due to our inability to accurately delineate the margins of the tumor. The goal of our study was to evaluate if a combination of advanced MR imaging techniques and a multimodal imaging model could be used to predict tumor infiltration in patients with diffuse gliomas.
METHODS: Institutional review board approval and written consent were obtained. This prospective pilot study enrolled patients undergoing stereotactic biopsy for a suspected de novo glioma. Stereotactic biopsy coordinates were coregistered with multiple standard and advanced neuroimaging sequences in 10 patients. Objective imaging values were assigned to the biopsy sites for each of the imaging sequences. A principal component analysis was performed to reduce the dimensionality of the imaging dataset without losing important information. A univariate analysis was performed to identify the statistically relevant principal components. Finally, a multivariate analysis was used to build the final model describing nuclear density.
RESULTS: A univariate analysis identified three principal components as being linearly associated with the observed nuclear density (p values 0.021, 0.016, and 0.046, respectively). These three principal component composite scores are predominantly comprised of DTI (mean diffusivity or average diffusion coefficient and fractional anisotropy) and PWI data (rMTT, Ktrans). The p value of the model was <0.001. The correlation between the predicted and observed nuclear density was 0.75.
CONCLUSION: A multi-input, single output imaging model may predict the extent of glioma invasion with significant correlation with histopathology.},
	author = {Durst, Christopher R and Raghavan, Prashant and Shaffrey, Mark E and Schiff, David and Lopes, M Beatriz and Sheehan, Jason P and Tustison, Nicholas J and Patrie, James T and Xin, Wenjun and Elias, W Jeff and Liu, Kenneth C and Helm, Greg A and Cupino, A and Wintermark, Max},
	date-added = {2014-12-07 05:41:56 +0000},
	date-modified = {2014-12-07 05:47:32 +0000},
	doi = {10.1007/s00234-013-1308-9},
	journal = {Neuroradiology},
	journal-full = {Neuroradiology},
	mesh = {Adult; Aged; Algorithms; Brain Neoplasms; Computer Simulation; Female; Glioma; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Models, Statistical; Multimodal Imaging; Neoplasm Invasiveness; Pilot Projects; Reproducibility of Results; Sensitivity and Specificity; Young Adult},
	month = {Feb},
	number = {2},
	pages = {107-15},
	pmid = {24337609},
	pst = {ppublish},
	title = {Multimodal {MR} imaging model to predict tumor infiltration in patients with gliomas},
	volume = {56},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00234-013-1308-9}}

@article{Qing:2014aa,
	abstract = {PURPOSE: To develop and validate a method for acquiring helium-3 ((3) He) and proton ((1) H) three-dimensional (3D) image sets of the human lung with isotropic spatial resolution within a 10-s breath-hold by using compressed sensing (CS) acceleration, and to assess the fidelity of undersampled images compared with fully sampled images.
METHODS: The undersampling scheme for CS acceleration was optimized and tested using (3) He ventilation data. Rapid 3D acquisition of both (3) He and (1) H data during one breath-hold was then implemented, based on a balanced steady-state free-precession pulse sequence, by random undersampling of k-space with reconstruction by means of minimizing the L1 norm and total variance. CS-reconstruction fidelity was evaluated quantitatively by comparing fully sampled and retrospectively undersampled image sets.
RESULTS: Helium-3 and (1) H 3D image sets of the lung with isotropic 3.9-mm resolution were acquired during a single breath-hold in 12 s and 8 s using acceleration factors of 2 and 3, respectively. Comparison of fully sampled and retrospectively undersampled (3) He and (1) H images yielded mean absolute errors <10% and structural similarity indices >0.9.
CONCLUSION: By randomly undersampling k-space and using CS reconstruction, high-quality (3) He and (1) H 3D image sets with isotropic 3.9-mm resolution can be acquired within an 8-s breath-hold. Magn Reson Med, 2014. {\copyright} 2014 Wiley Periodicals, Inc.},
	author = {Qing, Kun and Altes, Talissa A and Tustison, Nicholas J and Feng, Xue and Chen, Xiao and Mata, Jaime F and Miller, G Wilson and de Lange, Eduard E and Tobias, William A and Cates, Jr, Gordon D and Brookeman, James R and Mugler, 3rd, John P},
	date-added = {2014-12-07 05:41:50 +0000},
	date-modified = {2014-12-07 05:41:50 +0000},
	doi = {10.1002/mrm.25499},
	journal = {Magn Reson Med},
	journal-full = {Magnetic resonance in medicine : official journal of the Society of Magnetic Resonance in Medicine / Society of Magnetic Resonance in Medicine},
	keywords = {compressed sensing; hyperpolarized gas; pulmonary imaging},
	month = {Oct},
	pmid = {25335080},
	pst = {aheadofprint},
	title = {Rapid acquisition of helium-3 and proton three-dimensional image sets of the human lung in a single breath-hold using compressed sensing},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/mrm.25499}}

@article{Yoder:2014aa,
	abstract = {Study objectives were to develop, validate, and apply a method to measure three-dimensional (3D) internal strains in intact human discs under axial compression. A custom-built loading device applied compression and permitted load-relaxation outside of the magnet while also maintaining compression and hydration during imaging. Strain was measured through registration of 300 μm isotropic resolution images. Excellent registration accuracy was achieved, with 94% and 65% overlap of disc volume and lamellae compared to manual segmentation, and an average Hausdorff, a measure of distance error, of 0.03 and 0.12 mm for disc volume and lamellae boundaries, respectively. Strain maps enabled qualitative visualization and quantitative regional annulus fibrosus (AF) strain analysis. Axial and circumferential strains were highest in the lateral AF and lowest in the anterior and posterior AF. Radial strains were lowest in the lateral AF, but highly variable. Overall, this study provided new methods that will be valuable in the design and evaluation surgical procedures and therapeutic interventions.},
	author = {Yoder, Jonathon H and Peloquin, John M and Song, Gang and Tustison, Nick J and Moon, Sung M and Wright, Alexander C and Vresilovic, Edward J and Gee, James C and Elliott, Dawn M},
	date-added = {2014-12-07 05:41:48 +0000},
	date-modified = {2014-12-07 05:41:48 +0000},
	doi = {10.1115/1.4028250},
	journal = {J Biomech Eng},
	journal-full = {Journal of biomechanical engineering},
	month = {Nov},
	number = {11},
	pmc = {PMC4181341},
	pmid = {25109533},
	pst = {ppublish},
	title = {Internal three-dimensional strains in human intervertebral discs under axial compression quantified noninvasively by magnetic resonance imaging and image registration},
	volume = {136},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1115/1.4028250}}

@article{Said:2014aa,
	abstract = {OBJECTIVES: The purpose of this study was to investigate whether diffusion tensor imaging (DTI) of the corticospinal tract (CST) is a reliable surrogate for intraoperative macrostimulation through the deep brain stimulation (DBS) leads. The authors hypothesized that the distance on MRI from the DBS lead to the CST as determined by DTI would correlate with intraoperative motor thresholds from macrostimulations through the same DBS lead.
METHODS: The authors retrospectively reviewed pre- and postoperative MRI studies and intraoperative macrostimulation recordings in 17 patients with Parkinson disease (PD) treated by DBS stimulation. Preoperative DTI tractography of the CST was coregistered with postoperative MRI studies showing the position of the DBS leads. The shortest distance and the angle from each contact of each DBS lead to the CST was automatically calculated using software-based analysis. The distance measurements calculated for each contact were evaluated with respect to the intraoperative voltage thresholds that elicited a motor response at each contact.
RESULTS: There was a nonsignificant trend for voltage thresholds to increase when the distances between the DBS leads and the CST increased. There was a significant correlation between the angle and the voltage, but the correlation was weak (coefficient of correlation [R] = 0.36).
CONCLUSIONS: Caution needs to be exercised when using DTI tractography information to guide DBS lead placement in patients with PD. Further studies are needed to compare DTI tractography measurements with other approaches such as microelectrode recordings and conventional intraoperative MRI-guided placement of DBS leads.},
	author = {Said, Nicholas and Elias, W Jeff and Raghavan, Prashant and Cupino, Alan and Tustison, Nicholas and Frysinger, Robert and Patrie, James and Xin, Wenjun and Wintermark, Max},
	date-added = {2014-12-07 05:41:46 +0000},
	date-modified = {2014-12-07 05:46:56 +0000},
	doi = {10.3171/2014.6.JNS131673},
	journal = {J Neurosurg},
	journal-full = {Journal of neurosurgery},
	keywords = {CST = corticospinal tract; DBS = deep brain stimulation; DTI = diffusion tensor imaging; GEE = generalized estimating equation; GPi = globus pallidus internus; MPRAGE = magnetization-prepared rapid acquisition gradient echo; NEX = number of excitations; PD = Parkinson disease; STN = subthalamic nucleus; Vim = ventral intermediate nucleus; deep brain stimulation; diffusion tensor tractography; functional neurosurgery},
	month = {Oct},
	number = {4},
	pages = {929-35},
	pmid = {25061862},
	pst = {ppublish},
	title = {Correlation of diffusion tensor tractography and intraoperative macrostimulation during deep brain stimulation for {P}arkinson disease},
	volume = {121},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.3171/2014.6.JNS131673}}

@article{Wintermark:2014ab,
	abstract = {Precise focusing is essential for transcranial MRI-guided focused ultrasound (TcMRgFUS) to minimize collateral damage to non-diseased tissues and to achieve temperatures capable of inducing coagulative necrosis at acceptable power deposition levels. CT is usually used for this refocusing but requires a separate study (CT) ahead of the TcMRgFUS procedure. The goal of this study was to determine whether MRI using an appropriate sequence would be a viable alternative to CT for planning ultrasound refocusing in TcMRgFUS. We tested three MRI pulse sequences (3D T1 weighted 3D volume interpolated breath hold examination (VIBE), proton density weighted 3D sampling perfection with applications optimized contrasts using different flip angle evolution and 3D true fast imaging with steady state precision T2-weighted imaging) on patients who have already had a CT scan performed. We made detailed measurements of the calvarial structure based on the MRI data and compared those so-called 'virtual CT' to detailed measurements of the calvarial structure based on the CT data, used as a reference standard. We then loaded both standard and virtual CT in a TcMRgFUS device and compared the calculated phase correction values, as well as the temperature elevation in a phantom. A series of Bland-Altman measurement agreement analyses showed T1 3D VIBE as the optimal MRI sequence, with respect to minimizing the measurement discrepancy between the MRI derived total skull thickness measurement and the CT derived total skull thickness measurement (mean measurement discrepancy: 0.025; 95% CL (-0.22-0.27); p = 0.825). The T1-weighted sequence was also optimal in estimating skull CT density and skull layer thickness. The mean difference between the phase shifts calculated with the standard CT and the virtual CT reconstructed from the T1 dataset was 0.08 $\pm$ 1.2 rad on patients and 0.1 $\pm$ 0.9 rad on phantom. Compared to the real CT, the MR-based correction showed a 1 $\,^{\circ}$C drop on the maximum temperature elevation in the phantom (7% relative drop). Without any correction, the maximum temperature was down 6 $\,^{\circ}$C (43% relative drop). We have developed an approach that allows for a reconstruction of a virtual CT dataset from MRI to perform phase correction in TcMRgFUS.},
	author = {Wintermark, Max and Tustison, Nicholas J and Elias, William J and Patrie, James T and Xin, Wenjun and Demartini, Nicholas and Eames, Matt and Sumer, Suna and Lau, Benison and Cupino, Alan and Snell, John and Hananel, Arik and Kassell, Neal and Aubry, Jean-Francois},
	date-added = {2014-12-07 05:41:43 +0000},
	date-modified = {2014-12-07 05:45:02 +0000},
	doi = {10.1088/0031-9155/59/13/3599},
	journal = {Phys Med Biol},
	journal-full = {Physics in medicine and biology},
	month = {Jul},
	number = {13},
	pages = {3599-614},
	pmid = {24909357},
	pst = {ppublish},
	title = {T1-weighted {MRI} as a substitute to {CT} for refocusing planning in {MR}-guided focused ultrasound},
	volume = {59},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0031-9155/59/13/3599}}

@article{Tustison:2014ab,
	abstract = {Many studies of the human brain have explored the relationship between cortical thickness and cognition, phenotype, or disease. Due to the subjectivity and time requirements in manual measurement of cortical thickness, scientists have relied on robust software tools for automation which facilitate the testing and refinement of neuroscientific hypotheses. The most widely used tool for cortical thickness studies is the publicly available, surface-based FreeSurfer package. Critical to the adoption of such tools is a demonstration of their reproducibility, validity, and the documentation of specific implementations that are robust across large, diverse imaging datasets. To this end, we have developed the automated, volume-based Advanced Normalization Tools (ANTs) cortical thickness pipeline comprising well-vetted components such as SyGN (multivariate template construction), SyN (image registration), N4 (bias correction), Atropos (n-tissue segmentation), and DiReCT (cortical thickness estimation). In this work, we have conducted the largest evaluation of automated cortical thickness measures in publicly available data, comparing FreeSurfer and ANTs measures computed on 1205 images from four open data sets (IXI, MMRR, NKI, and OASIS), with parcellation based on the recently proposed Desikan-Killiany-Tourville (DKT) cortical labeling protocol. We found good scan-rescan repeatability with both FreeSurfer and ANTs measures. Given that such assessments of precision do not necessarily reflect accuracy or an ability to make statistical inferences, we further tested the neurobiological validity of these approaches by evaluating thickness-based prediction of age and gender. ANTs is shown to have a higher predictive performance than FreeSurfer for both of these measures. In promotion of open science, we make all of our scripts, data, and results publicly available which complements the use of open image data sets and the open source availability of the proposed ANTs cortical thickness pipeline.},
	author = {Tustison, Nicholas J and Cook, Philip A and Klein, Arno and Song, Gang and Das, Sandhitsu R and Duda, Jeffrey T and Kandel, Benjamin M and van Strien, Niels and Stone, James R and Gee, James C and Avants, Brian B},
	date-added = {2014-12-07 05:41:41 +0000},
	date-modified = {2014-12-07 05:44:30 +0000},
	doi = {10.1016/j.neuroimage.2014.05.044},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	keywords = {Advanced Normalization Tools; Age prediction; Gender prediction; MRI; Open science; Scientific reproducibility},
	month = {Oct},
	pages = {166-79},
	pmid = {24879923},
	pst = {ppublish},
	title = {Large-scale evaluation of {ANTs} and {FreeSurfer} cortical thickness measurements},
	volume = {99},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2014.05.044}}

@article{Teague:2014aa,
	abstract = {Non-uniform distribution of inspired gas within the lung, termed ventilation heterogeneity, is present in patients with even mild asthma. Current evidence strongly supports ventilation heterogeneity as a fundamental derangement of lung function in asthma that contributes per se to hypoxemia and airway hyper-responsiveness. An extreme example of ventilation heterogeneity is the identification by hyperpolarized gas MRI of lung regions with no ventilation, termed filling defects. Lung filling defects in patients with asthma can persist over time, increase in size with methacholine-induced bronchospasm and more likely are caused by obstruction of the peripheral and not the proximal airways. Ventilation heterogeneity can be quantified in the conducting and acinar lung zones with the multiple gas washout method, and in the acinar zone does not fully resolve following bronchodilator treatment in patients with asthma. In prospective studies, the degree of ventilation heterogeneity at baseline predicts airway hyper-responsiveness and response to corticosteroid dose titration. An important unanswered question is the relationship of airways inflammation to ventilation heterogeneity. In consideration of the importance of ventilation heterogeneity in its pathobiology, asthma is more a focal disorder with regional pathology akin to regional ileitis and not the generalized disorder of the airways as it has been viewed in the past.},
	author = {Teague, W Gerald and Tustison, Nicholas J and Altes, Talissa A},
	date-added = {2014-12-07 05:41:40 +0000},
	date-modified = {2014-12-07 05:41:40 +0000},
	doi = {10.3109/02770903.2014.914535},
	journal = {J Asthma},
	journal-full = {The Journal of asthma : official journal of the Association for the Care of Asthma},
	keywords = {Mechanisms; pathophysiology; reviews},
	mesh = {Asthma; Diagnostic Imaging; Humans; Lung; Pulmonary Ventilation},
	month = {Sep},
	number = {7},
	pages = {677-84},
	pmid = {24823323},
	pst = {ppublish},
	title = {Ventilation heterogeneity in asthma},
	volume = {51},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.3109/02770903.2014.914535}}

@article{Avants:2014aa,
	abstract = {Publicly available scientific resources help establish evaluation standards, provide a platform for teaching and improve reproducibility. Version 4 of the Insight ToolKit (ITK(4)) seeks to establish new standards in publicly available image registration methodology. ITK(4) makes several advances in comparison to previous versions of ITK. ITK(4) supports both multivariate images and objective functions; it also unifies high-dimensional (deformation field) and low-dimensional (affine) transformations with metrics that are reusable across transform types and with composite transforms that allow arbitrary series of geometric mappings to be chained together seamlessly. Metrics and optimizers take advantage of multi-core resources, when available. Furthermore, ITK(4) reduces the parameter optimization burden via principled heuristics that automatically set scaling across disparate parameter types (rotations vs. translations). A related approach also constrains steps sizes for gradient-based optimizers. The result is that tuning for different metrics and/or image pairs is rarely necessary allowing the researcher to more easily focus on design/comparison of registration strategies. In total, the ITK(4) contribution is intended as a structure to support reproducible research practices, will provide a more extensive foundation against which to evaluate new work in image registration and also enable application level programmers a broad suite of tools on which to build. Finally, we contextualize this work with a reference registration evaluation study with application to pediatric brain labeling.},
	author = {Avants, Brian B and Tustison, Nicholas J and Stauffer, Michael and Song, Gang and Wu, Baohua and Gee, James C},
	date-added = {2014-12-07 05:41:39 +0000},
	date-modified = {2014-12-07 05:52:12 +0000},
	doi = {10.3389/fninf.2014.00044},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	keywords = {MRI; brain; death; open-source; registration},
	pages = {44},
	pmc = {PMC4009425},
	pmid = {24817849},
	pst = {epublish},
	title = {The {Insight} {ToolKit} image registration framework},
	volume = {8},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fninf.2014.00044}}

@article{Wintermark:2014aa,
	abstract = {PURPOSE: To use diffusion-tensor (DT) magnetic resonance (MR) imaging in patients with essential tremor who were treated with transcranial MR imaging-guided focused ultrasound lesion inducement to identify the structural connectivity of the ventralis intermedius nucleus of the thalamus and determine how DT imaging changes correlated with tremor changes after lesion inducement.
MATERIALS AND METHODS: With institutional review board approval, and with prospective informed consent, 15 patients with medication-refractory essential tremor were enrolled in a HIPAA-compliant pilot study and were treated with transcranial MR imaging-guided focused ultrasound surgery targeting the ventralis intermedius nucleus of the thalamus contralateral to their dominant hand. Fourteen patients were ultimately included. DT MR imaging studies at 3.0 T were performed preoperatively and 24 hours, 1 week, 1 month, and 3 months after the procedure. Fractional anisotropy (FA) maps were calculated from the DT imaging data sets for all time points in all patients. Voxels where FA consistently decreased over time were identified, and FA change in these voxels was correlated with clinical changes in tremor over the same period by using Pearson correlation.
RESULTS: Ipsilateral brain structures that showed prespecified negative correlation values of FA over time of -0.5 or less included the pre- and postcentral subcortical white matter in the hand knob area; the region of the corticospinal tract in the centrum semiovale, in the posterior limb of the internal capsule, and in the cerebral peduncle; the thalamus; the region of the red nucleus; the location of the central tegmental tract; and the region of the inferior olive. The contralateral middle cerebellar peduncle and bilateral portions of the superior vermis also showed persistent decrease in FA over time. There was strong correlation between decrease in FA and clinical improvement in hand tremor 3 months after lesion inducement (P < .001).
CONCLUSION: DT MR imaging after MR imaging-guided focused ultrasound thalamotomy depicts changes in specific brain structures. The magnitude of the DT imaging changes after thalamic lesion inducement correlates with the degree of clinical improvement in essential tremor.},
	author = {Wintermark, Max and Huss, Diane S and Shah, Binit B and Tustison, Nicholas and Druzgal, T Jason and Kassell, Neal and Elias, W Jeff},
	date-added = {2014-12-07 05:41:36 +0000},
	date-modified = {2014-12-07 05:45:21 +0000},
	doi = {10.1148/radiol.14132112},
	journal = {Radiology},
	journal-full = {Radiology},
	mesh = {Aged; Brain Mapping; Diffusion Magnetic Resonance Imaging; Essential Tremor; Female; Humans; Magnetic Resonance Imaging, Interventional; Male; Nerve Fibers, Myelinated; Pilot Projects; Prospective Studies; Thalamus; Treatment Outcome; Ultrasonic Surgical Procedures},
	month = {Jul},
	number = {1},
	pages = {202-9},
	pmid = {24620914},
	pst = {ppublish},
	title = {Thalamic connectivity in patients with essential tremor treated with {MR} imaging-guided focused ultrasound: in vivo fiber tracking by using diffusion-tensor {MR} imaging},
	volume = {272},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1148/radiol.14132112}}

@article{Tustison:2013ac,
	abstract = {Diffeomorphic mappings are central to image registration due largely to their topological properties and success in providing biologically plausible solutions to deformation and morphological estimation problems. Popular diffeomorphic image registration algorithms include those characterized by time-varying and constant velocity fields, and symmetrical considerations. Prior information in the form of regularization is used to enforce transform plausibility taking the form of physics-based constraints or through some approximation thereof, e.g., Gaussian smoothing of the vector fields [a la Thirion's Demons (Thirion, 1998)]. In the context of the original Demons' framework, the so-called directly manipulated free-form deformation (DMFFD) (Tustison et al., 2009) can be viewed as a smoothing alternative in which explicit regularization is achieved through fast B-spline approximation. This characterization can be used to provide B-spline "flavored" diffeomorphic image registration solutions with several advantages. Implementation is open source and available through the Insight Toolkit and our Advanced Normalization Tools (ANTs) repository. A thorough comparative evaluation with the well-known SyN algorithm (Avants et al., 2008), implemented within the same framework, and its B-spline analog is performed using open labeled brain data and open source evaluation tools.},
	author = {Tustison, Nicholas J and Avants, Brian B},
	date-added = {2014-12-07 05:41:31 +0000},
	date-modified = {2015-01-07 16:14:35 +0000},
	doi = {10.3389/fninf.2013.00039},
	journal = {Front Neuroinform},
	journal-full = {Frontiers in neuroinformatics},
	keywords = {Advanced normalization tools; Insight Toolkit; diffeomorphisms; directly manipulated free-form deformation; spatial normalization},
	pages = {39},
	pmc = {PMC3870320},
	pmid = {24409140},
	pst = {epublish},
	title = {Explicit {B}-spline regularization in diffeomorphic image registration},
	volume = {7},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fninf.2013.00039}}

@article{Tustison:2014aa,
	abstract = {Segmenting and quantifying gliomas from MRI is an important task for diagnosis, planning intervention, and for tracking tumor changes over time. However, this task is complicated by the lack of prior knowledge concerning tumor location, spatial extent, shape, possible displacement of normal tissue, and intensity signature. To accommodate such complications, we introduce a framework for supervised segmentation based on multiple modality intensity, geometry, and asymmetry feature sets. These features drive a supervised whole-brain and tumor segmentation approach based on random forest-derived probabilities. The asymmetry-related features (based on optimal symmetric multimodal templates) demonstrate excellent discriminative properties within this framework. We also gain performance by generating probability maps from random forest models and using these maps for a refining Markov random field regularized probabilistic segmentation. This strategy allows us to interface the supervised learning capabilities of the random forest model with regularized probabilistic segmentation using the recently developed ANTsR package-a comprehensive statistical and visualization interface between the popular Advanced Normalization Tools (ANTs) and the R statistical project. The reported algorithmic framework was the top-performing entry in the MICCAI 2013 Multimodal Brain Tumor Segmentation challenge. The challenge data were widely varying consisting of both high-grade and low-grade glioma tumor four-modality MRI from five different institutions. Average Dice overlap measures for the final algorithmic assessment were 0.87, 0.78, and 0.74 for "complete", "core", and "enhanced" tumor components, respectively.},
	author = {Tustison, Nicholas J and Shrinidhi, K L and Wintermark, Max and Durst, Christopher R and Kandel, Benjamin M and Gee, James C and Grossman, Murray C and Avants, Brian B},
	date-added = {2014-12-07 05:41:26 +0000},
	date-modified = {2014-12-07 05:43:58 +0000},
	doi = {10.1007/s12021-014-9245-2},
	journal = {Neuroinformatics},
	journal-full = {Neuroinformatics},
	month = {Nov},
	pmid = {25433513},
	pst = {aheadofprint},
	title = {Optimal Symmetric Multimodal Templates and Concatenated Random Forests for Supervised Brain Tumor Segmentation (Simplified) with {$ANTsR$}},
	year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s12021-014-9245-2}}

@article{siqueira2008,
	author = {Marcelo Siqueira and Longin J. Latecki and Nicholas J. Tustison and J. Gallier and James C. Gee},
	date-added = {2013-11-07 01:27:28 +0000},
	date-modified = {2014-01-16 23:27:44 +0000},
	journal = {Journal of Mathematical Imaging and Vision},
	month = {March},
	number = {3},
	pages = {249--274},
	title = {Topological Repairing of {3D} Digital Images},
	volume = {30},
	year = {2008}}

@article{Hagspiel:2000aa,
	abstract = {Hyperpolarized gas MR virtual colonography was performed in plastic phantoms and in the dog colon. (3)He was laser polarized in a prototype commercial system. 2D and 3D gradient echo sequences were used to image the noble gas-filled structures. The hyperpolarized (3)He within the plastic tube and colon lumen produced high signal, providing excellent contrast from the surrounding structures. The virtual colonoscopic analysis of the canine dataset allowed visualization of the colonic features and the colonic wall from inside the colon. (3)He colonoscopy is a novel technique to visualize the colon with MRI with the application of an inert gaseous endoluminal contrast agent.},
	author = {Hagspiel, K D and Altes, T A and Mugler, 3rd, J P and Spellman, Jr, M J and Mata, J F and Tustison, N J and Brookeman, J R},
	date-added = {2013-11-07 00:38:50 +0000},
	date-modified = {2013-11-07 00:47:29 +0000},
	journal = {Magn Reson Med},
	journal-full = {Magnetic resonance in medicine : official journal of the Society of Magnetic Resonance in Medicine / Society of Magnetic Resonance in Medicine},
	mesh = {Animals; Colon; Colonic Neoplasms; Contrast Media; Dogs; Feasibility Studies; Helium; Magnetic Resonance Imaging},
	month = {Nov},
	number = {5},
	pages = {813-6},
	pmid = {11064418},
	pst = {ppublish},
	title = {{MR} virtual colonography using hyperpolarized {(3)He} as an endoluminal contrast agent: demonstration of feasibility},
	volume = {44},
	year = {2000}}

@article{Tustison:2003aa,
	abstract = {Current research investigating the modeling of left ventricular dynamics for accurate clinical assessment of cardiac function is extensive. Magnetic resonance (MR) tagging is a functional imaging method which allows for encoding of a grid of signal voids on cardiac MR images, providing a mechanism for noninvasive measurement of intramural tissue deformations, in vivo. We present a novel technique of employing a four-dimensional (4-D) B-spline model which permits concurrent determination of myocardial beads and myocardial strains. The method entails fitting the knot planes of the 4-D B-spline model for fixed times to a sequence of triplets of orthogonal sets of tag surfaces for all imaged volumetric frames within the constraints of the model's spatio-temporal internal energy. From a three-dimensional (3-D) displacement field, the corresponding long and short-axis Lagrangian normal, shear, and principal strain maps are produced. As an important byproduct, the points defined by the 3-D intersections of the triplets of orthogonal tag planes, which we refer to as myocardial beads, can easily be determined by our model. Displaying the beads as a movie loop allows for the visualization of the nonrigid movement of the left ventricle in 3-D.},
	author = {Tustison, Nicholas J and D{\'a}vila-Rom{\'a}n, Victor G and Amini, Amir A},
	date-added = {2013-11-07 00:38:48 +0000},
	date-modified = {2013-11-07 00:50:10 +0000},
	doi = {10.1109/TBME.2003.814530},
	journal = {IEEE Trans Biomed Eng},
	journal-full = {IEEE transactions on bio-medical engineering},
	mesh = {Algorithms; Heart Ventricles; Humans; Image Enhancement; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Motion; Movement; Pattern Recognition, Automated; Stress, Mechanical; Ventricular Function; Ventricular Function, Left},
	month = {Aug},
	number = {8},
	pages = {1038-40},
	pmid = {12892332},
	pst = {ppublish},
	title = {Myocardial kinematics from tagged {MRI} based on a {4-D} {B}-spline model},
	volume = {50},
	year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TBME.2003.814530}}

@article{Tustison:2006aa,
	abstract = {We present research in which both left and right ventricular deformation is estimated from tagged cardiac magnetic resonance imaging using volumetric deformable models constructed from nonuniform rational B-splines (NURBS). The four model types considered and compared for the left ventricle include two Cartesian NURBS models--one with a cylindrical parameter assignment and one with a prolate spheroidal parameter assignment. The remaining two are non-Cartesian, i.e., prolate spheroidal and cylindrical each with their respective prolate spheroidal and cylindrical parameter assignment regimes. These choices were made based on the typical shape of the left ventricle. For each frame starting with end-diastole, a NURBS model is constructed by fitting two surfaces with the same parameterization to the corresponding set of epicardial and endocardial contours from which a volumetric model is created. Using normal displacements of the three sets of orthogonal tag planes as well as displacements of contour/tag line intersection points and tag plane intersection points, one can solve for the optimal homogeneous coordinates, in a weighted least squares sense, of the control points of the deformed NURBS model at end-diastole using quadratic programming. This allows for subsequent nonrigid registration of the biventricular model at end-diastole to all later time frames. After registration of the model to all later time points, the registered NURBS models are temporally lofted in order to create a comprehensive four-dimensional NURBS model. From the lofted model, we can extract three-dimensional myocardial deformation fields and corresponding Lagrangian and Eulerian strain maps which are local measures of nonrigid deformation. The results show that, in the case of simulated data, the quadratic Cartesian NURBS models with the cylindrical and prolate spheroidal parameter assignments outperform their counterparts in predicting normal strain. The decreased complexity associated with the Cartesian model with the cylindrical parameter assignment prompted its use for subsequent calculations. Lagrangian strains in three canine data, a normal human, and a patient with history of myocardial infarction are presented. Eulerian strains for the normal human data are also included.},
	author = {Tustison, Nicholas J and Amini, Amir A},
	date-added = {2013-11-07 00:38:46 +0000},
	date-modified = {2013-11-07 00:50:24 +0000},
	doi = {10.1109/TMI.2005.861015},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Animals; Artificial Intelligence; Computer Simulation; Dogs; Elasticity; Heart Ventricles; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Magnetic Resonance Imaging; Models, Cardiovascular; Myocardial Contraction; Reproducibility of Results; Sensitivity and Specificity; Stress, Mechanical; Subtraction Technique; Ventricular Function},
	month = {Jan},
	number = {1},
	pages = {94-112},
	pmid = {16398418},
	pst = {ppublish},
	title = {Biventricular myocardial strains via nonrigid registration of anatomical {NURBS} model [corrected]},
	volume = {25},
	year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2005.861015}}

@article{Tustison:2009ab,
	abstract = {Previous contributions to both the research and open source software communities detailed a generalization of a fast scalar field fitting technique for cubic B-splines based on the work originally proposed by Lee . One advantage of our proposed generalized B-spline fitting approach is its immediate application to a class of nonrigid registration techniques frequently employed in medical image analysis. Specifically, these registration techniques fall under the rubric of free-form deformation (FFD) approaches in which the object to be registered is embedded within a B-spline object. The deformation of the B-spline object describes the transformation of the image registration solution. Representative of this class of techniques, and often cited within the relevant community, is the formulation of Rueckert who employed cubic splines with normalized mutual information to study breast deformation. Similar techniques from various groups provided incremental novelty in the form of disparate explicit regularization terms, as well as the employment of various image metrics and tailored optimization methods. For several algorithms, the underlying gradient-based optimization retained the essential characteristics of Rueckert's original contribution. The contribution which we provide in this paper is two-fold: 1) the observation that the generic FFD framework is intrinsically susceptible to problematic energy topographies and 2) that the standard gradient used in FFD image registration can be modified to a well-understood preconditioned form which substantially improves performance. This is demonstrated with theoretical discussion and comparative evaluation experimentation.},
	author = {Tustison, Nicholas J and Avants, Brian B and Gee, James C},
	date-added = {2013-11-07 00:38:33 +0000},
	date-modified = {2013-11-07 00:38:33 +0000},
	doi = {10.1109/TIP.2008.2010072},
	journal = {IEEE Trans Image Process},
	journal-full = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	mesh = {Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique},
	month = {Mar},
	number = {3},
	pages = {624-35},
	pmid = {19171516},
	pst = {ppublish},
	title = {Directly manipulated free-form deformation image registration},
	volume = {18},
	year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIP.2008.2010072}}

@article{Tustison:2010ac,
	abstract = {A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as "N4ITK," available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized (3)He lung image data, and 9.4T postmortem hippocampus data.},
	author = {Tustison, Nicholas J and Avants, Brian B and Cook, Philip A and Zheng, Yuanjie and Egan, Alexander and Yushkevich, Paul A and Gee, James C},
	date-added = {2013-11-07 00:38:29 +0000},
	date-modified = {2013-11-07 00:51:04 +0000},
	doi = {10.1109/TMI.2010.2046908},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Artifacts; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Reproducibility of Results; Sensitivity and Specificity},
	month = {Jun},
	number = {6},
	pages = {1310-20},
	pmc = {PMC3071855},
	pmid = {20378467},
	pst = {ppublish},
	title = {{N4ITK}: improved {N3} bias correction},
	volume = {29},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2010.2046908}}

@article{Tustison:2010ab,
	abstract = {A computational framework is described that was developed for quantitative analysis of hyperpolarized helium-3 MR lung ventilation image data. This computational framework was applied to a study consisting of 55 subjects (47 asthmatic and eight normal). Each subject was imaged before and after respiratory challenge and also underwent spirometry. Approximately 1600 image features were calculated from the lungs in each image. Both the image and 27 spirometric features were ranked based on their ability to characterize clinical diagnosis using a mutual information-based feature subset selection algorithm. It was found that the top image features perform much better compared with the current clinical gold-standard spirometric values when considered individually. Interestingly, it was also found that spirometric values are relatively orthogonal to these image feature values in terms of informational content.},
	author = {Tustison, Nicholas J and Altes, Talissa A and Song, Gang and de Lange, Eduard E and Mugler, 3rd, John P and Gee, James C},
	date-added = {2013-11-07 00:38:28 +0000},
	date-modified = {2013-11-07 00:50:55 +0000},
	doi = {10.1002/mrm.22390},
	journal = {Magn Reson Med},
	journal-full = {Magnetic resonance in medicine : official journal of the Society of Magnetic Resonance in Medicine / Society of Magnetic Resonance in Medicine},
	mesh = {Asthma; Computer Simulation; Helium; Humans; Lung; Magnetic Resonance Imaging; Reference Standards; Respiratory Function Tests; Spirometry},
	month = {Jun},
	number = {6},
	pages = {1448-55},
	pmid = {20512846},
	pst = {ppublish},
	title = {Feature analysis of hyperpolarized helium-3 pulmonary {MRI}: a study of asthmatics versus nonasthmatics},
	volume = {63},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/mrm.22390}}

@article{Tustison:2010aa,
	abstract = {PURPOSE: To propose and test the feasibility of a novel method for quantifying 3D regional pulmonary kinematics from hyperpolarized helium-3 tagged MRI in human subjects using a tailored image processing pipeline and a recently developed nonrigid registration framework.
MATERIALS AND METHODS: Following image acquisition, inspiratory and expiratory tagged (3)He magnetic resonance (MR) images were preprocessed using various image filtering techniques to enhance the tag surfaces. Segmentation of the three orthogonal sets of tag planes in each lung produced distinct point-set representations of the tag surfaces. Using these labeled point-sets, deformation fields and corresponding strain maps were obtained via nonrigid point-set registration. Kinematic analysis was performed on three volunteers.
RESULTS: Tag lines in inspiratory and expiratory images were coregistered producing a continuous 3D correspondence mapping. Average displacement and directional strains were calculated in three subjects in the inferior, mid, and superior portions of the right and left lungs. As expected, the predominant direction of displacements with expiration is from inferior to superior.
CONCLUSION: Kinematic quantitation of pulmonary motion using tagged (3)He MRI is feasible using the applied image preprocessing filtering techniques and nonrigid point-set registration. Potential benefits from regional pulmonary kinematic quantitation include the facilitation of diagnosis and local assessment of disease progression.},
	author = {Tustison, Nicholas J and Awate, Suyash P and Cai, Jing and Altes, Talissa A and Miller, G Wilson and de Lange, Eduard E and Mugler, 3rd, John P and Gee, James C},
	date-added = {2013-11-07 00:38:26 +0000},
	date-modified = {2013-11-07 00:50:46 +0000},
	doi = {10.1002/jmri.22137},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Administration, Inhalation; Contrast Media; Female; Helium; Humans; Isotopes; Lung; Magnetic Resonance Imaging; Male; Reproducibility of Results; Respiratory Function Tests; Sensitivity and Specificity},
	month = {May},
	number = {5},
	pages = {1236-41},
	pmc = {PMC2997045},
	pmid = {20432362},
	pst = {ppublish},
	title = {Pulmonary kinematics from tagged hyperpolarized helium-3 {MRI}},
	volume = {31},
	year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.22137}}

@article{Avants:2011ab,
	abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard=0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard=0.669$\pm$0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling.},
	author = {Avants, Brian B and Tustison, Nicholas J and Song, Gang and Cook, Philip A and Klein, Arno and Gee, James C},
	date-added = {2013-11-07 00:38:23 +0000},
	date-modified = {2013-11-07 00:46:57 +0000},
	doi = {10.1016/j.neuroimage.2010.09.025},
	journal = {Neuroimage},
	journal-full = {NeuroImage},
	mesh = {Algorithms; Brain; Databases, Factual; Diagnostic Imaging; Head; Humans; Image Processing, Computer-Assisted; Linear Models; Models, Anatomic; Models, Neurological; Population; Reproducibility of Results; Software},
	month = {Feb},
	number = {3},
	pages = {2033-44},
	pmc = {PMC3065962},
	pmid = {20851191},
	pst = {ppublish},
	title = {A reproducible evaluation of {ANTs} similarity metric performance in brain image registration},
	volume = {54},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neuroimage.2010.09.025}}

@article{Tustison:2011ad,
	abstract = {We introduce a labeled point set registration algorithm based on a family of novel information-theoretic measures derived as a generalization of the well-known Shannon entropy. This generalization, known as the Havrda-Charvat-Tsallis entropy, permits a fine-tuning between solution types of varying degrees of robustness of the divergence measure between multiple point sets. A variant of the traditional free-form deformation approach, known as directly manipulated free-form deformation, is used to model the transformation of the registration solution. We provide an overview of its open source implementation based on the Insight Toolkit of the National Institutes of Health. Characterization of the proposed framework includes comparison with other state of the art kernel-based methods and demonstration of its utility for lung registration via labeled point set representation of lung anatomy.},
	author = {Tustison, Nicholas J and Awate, Suyash P and Song, Gang and Cook, Tessa S and Gee, James C},
	date-added = {2013-11-07 00:38:18 +0000},
	date-modified = {2013-11-07 01:00:27 +0000},
	doi = {10.1109/TMI.2010.2086065},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Entropy; Humans; Image Processing, Computer-Assisted; Lung; Magnetic Resonance Imaging; Normal Distribution},
	month = {Feb},
	number = {2},
	pages = {451-60},
	pmid = {20937578},
	pst = {ppublish},
	title = {Point set registration using {H}avrda-{C}harvat-{T}sallis entropy measures},
	volume = {30},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2010.2086065}}

@article{Tustison:2011ac,
	abstract = {We propose a new approach to front propagation algorithms based on a topological variant of well-composedness which contrasts with previous methods based on simple point detection. This provides for a theoretical justification, based on the digital Jordan separation theorem, for digitally "gluing" evolved well-composed objects separated by well-composed curves or surfaces. Additionally, our framework can be extended to more relaxed topologically constrained algorithms based on multisimple points. For both methods this framework has the additional benefit of obviating the requirement for both a user-specified connectivity and a topologically-consistent marching cubes/squares algorithm in meshing the resulting segmentation.},
	author = {Tustison, Nicholas J and Avants, Brian B and Siqueira, Marcelo and Gee, James C},
	date-added = {2013-11-07 00:38:17 +0000},
	date-modified = {2013-11-07 00:38:17 +0000},
	doi = {10.1109/TIP.2010.2095021},
	journal = {IEEE Trans Image Process},
	journal-full = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	mesh = {Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique},
	month = {Jun},
	number = {6},
	pages = {1756-61},
	pmid = {21118779},
	pst = {ppublish},
	title = {Topological well-composedness and glamorous glue: a digital gluing algorithm for topologically constrained front propagation},
	volume = {20},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIP.2010.2095021}}

@article{Avants:2011aa,
	abstract = {We introduce Atropos, an ITK-based multivariate n-class open source segmentation algorithm distributed with ANTs ( http://www.picsl.upenn.edu/ANTs). The Bayesian formulation of the segmentation problem is solved using the Expectation Maximization (EM) algorithm with the modeling of the class intensities based on either parametric or non-parametric finite mixtures. Atropos is capable of incorporating spatial prior probability maps (sparse), prior label maps and/or Markov Random Field (MRF) modeling. Atropos has also been efficiently implemented to handle large quantities of possible labelings (in the experimental section, we use up to 69 classes) with a minimal memory footprint. This work describes the technical and implementation aspects of Atropos and evaluates its performance on two different ground-truth datasets. First, we use the BrainWeb dataset from Montreal Neurological Institute to evaluate three-tissue segmentation performance via (1) K-means segmentation without use of template data; (2) MRF segmentation with initialization by prior probability maps derived from a group template; (3) Prior-based segmentation with use of spatial prior probability maps derived from a group template. We also evaluate Atropos performance by using spatial priors to drive a 69-class EM segmentation problem derived from the Hammers atlas from University College London. These evaluation studies, combined with illustrative examples that exercise Atropos options, demonstrate both performance and wide applicability of this new platform-independent open source segmentation tool.},
	author = {Avants, Brian B and Tustison, Nicholas J and Wu, Jue and Cook, Philip A and Gee, James C},
	date-added = {2013-11-07 00:38:16 +0000},
	date-modified = {2013-11-07 00:46:47 +0000},
	doi = {10.1007/s12021-011-9109-y},
	journal = {Neuroinformatics},
	journal-full = {Neuroinformatics},
	mesh = {Access to Information; Algorithms; Bayes Theorem; Databases, Factual; Humans; Image Processing, Computer-Assisted; Internet; Magnetic Resonance Imaging; Models, Statistical; Pattern Recognition, Automated; Software},
	month = {Dec},
	number = {4},
	pages = {381-400},
	pmc = {PMC3297199},
	pmid = {21373993},
	pst = {ppublish},
	title = {An open source multivariate framework for $n$-tissue segmentation with evaluation on public data},
	volume = {9},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s12021-011-9109-y}}

@article{Murphy:2011aa,
	abstract = {EMPIRE10 (Evaluation of Methods for Pulmonary Image REgistration 2010) is a public platform for fair and meaningful comparison of registration algorithms which are applied to a database of intrapatient thoracic CT image pairs. Evaluation of nonrigid registration techniques is a nontrivial task. This is compounded by the fact that researchers typically test only on their own data, which varies widely. For this reason, reliable assessment and comparison of different registration algorithms has been virtually impossible in the past. In this work we present the results of the launch phase of EMPIRE10, which comprised the comprehensive evaluation and comparison of 20 individual algorithms from leading academic and industrial research groups. All algorithms are applied to the same set of 30 thoracic CT pairs. Algorithm settings and parameters are chosen by researchers expert in the configuration of their own method and the evaluation is independent, using the same criteria for all participants. All results are published on the EMPIRE10 website (http://empire10.isi.uu.nl). The challenge remains ongoing and open to new participants. Full results from 24 algorithms have been published at the time of writing. This paper details the organization of the challenge, the data and evaluation methods and the outcome of the initial launch with 20 algorithms. The gain in knowledge and future work are discussed.},
	author = {Murphy, Keelin and van Ginneken, Bram and Reinhardt, Joseph M and Kabus, Sven and Ding, Kai and Deng, Xiang and Cao, Kunlin and Du, Kaifang and Christensen, Gary E and Garcia, Vincent and Vercauteren, Tom and Ayache, Nicholas and Commowick, Olivier and Malandain, Gr{\'e}goire and Glocker, Ben and Paragios, Nikos and Navab, Nassir and Gorbunova, Vladlena and Sporring, Jon and de Bruijne, Marleen and Han, Xiao and Heinrich, Mattias P and Schnabel, Julia A and Jenkinson, Mark and Lorenz, Cristian and Modat, Marc and McClelland, Jamie R and Ourselin, S{\'e}bastien and Muenzing, Sascha E A and Viergever, Max A and De Nigris, Dante and Collins, D Louis and Arbel, Tal and Peroni, Marta and Li, Rui and Sharp, Gregory C and Schmidt-Richberg, Alexander and Ehrhardt, Jan and Werner, Ren{\'e} and Smeets, Dirk and Loeckx, Dirk and Song, Gang and Tustison, Nicholas and Avants, Brian and Gee, James C and Staring, Marius and Klein, Stefan and Stoel, Berend C and Urschler, Martin and Werlberger, Manuel and Vandemeulebroucke, Jef and Rit, Simon and Sarrut, David and Pluim, Josien P W},
	date-added = {2013-11-07 00:38:15 +0000},
	date-modified = {2013-11-07 00:49:14 +0000},
	doi = {10.1109/TMI.2011.2158349},
	journal = {IEEE Trans Med Imaging},
	journal-full = {IEEE transactions on medical imaging},
	mesh = {Algorithms; Animals; Databases, Factual; Lung; Observer Variation; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reference Standards; Reproducibility of Results; Sensitivity and Specificity; Sheep; Software Validation; Thorax; Tomography, X-Ray Computed},
	month = {Nov},
	number = {11},
	pages = {1901-20},
	pmid = {21632295},
	pst = {ppublish},
	title = {Evaluation of registration methods on thoracic {CT}: the {EMPIRE10} challenge},
	volume = {30},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TMI.2011.2158349}}

@article{Tustison:2011ab,
	abstract = {The effects of certain lung pathologies include alterations in lung physiology negatively affecting pulmonary compliance. Current approaches to diagnosis and treatment assessment of lung disease commonly rely on pulmonary function testing. Such testing is limited to global measures of lung function, neglecting regional measurements, which are critical for early diagnosis and localization of disease. Increased accessibility to medical image acquisition strategies with high spatiotemporal resolution coupled with the development of sophisticated intensity-based and geometric registration techniques has resulted in the recent exploration of modeling pulmonary motion for calculating local measures of deformation. In this review, the authors provide a broad overview of such research efforts for the estimation of pulmonary deformation. This includes discussion of various techniques, current trends in validation approaches, and the public availability of software and data resources.},
	author = {Tustison, Nicholas J and Cook, Tessa S and Song, Gang and Gee, James C},
	date-added = {2013-11-07 00:38:12 +0000},
	date-modified = {2013-11-07 00:38:12 +0000},
	doi = {10.1016/j.acra.2010.10.019},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Algorithms; Diagnostic Imaging; Humans; Image Interpretation, Computer-Assisted; Respiratory Function Tests},
	month = {Apr},
	number = {4},
	pages = {402-17},
	pmid = {21377592},
	pst = {ppublish},
	title = {Pulmonary kinematics from image data: a review},
	volume = {18},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2010.10.019}}

@article{Yilmaz:2011aa,
	abstract = {In adult canines following major lung resection, the remaining lobes expand asymmetrically, associated with alveolar tissue regrowth, remodeling, and progressive functional compensation over many months. To permit noninvasive longitudinal assessment of regional growth and function, we performed serial high-resolution computed tomography (HRCT) on six male dogs (∼9 mo old, 25.0 $\pm$ 4.5 kg, $\pm$SD) at 15 and 30 cmH(2)O transpulmonary pressure (Ptp) before resection (PRE) and 3 and 15 mo postresection (POST3 and POST15, respectively) of 65-70% of lung units. At POST3, lobar air volume increased 83-148% and tissue (including microvascular blood) volume 120-234% above PRE values without further changes at POST15. Lobar-specific compliance (Cs) increased 52-137% from PRE to POST3 and 28-79% from POST3 to POST15. Inflation-related parenchyma strain and shear were estimated by detailed registration of corresponding anatomical features at each Ptp. Within each lobe, regional displacement was most pronounced at the caudal region, whereas strain was pronounced in the periphery. Regional three-dimensional strain magnitudes increased heterogeneously from PRE to POST3, with further medial-lateral increases from POST3 to POST15. Lobar principal strains (PSs) were unchanged or modestly elevated postresection; changes in lobar maximum PS correlated inversely with changes in lobar air and tissue volumes. Lobar shear distortion increased in coronal and transverse planes at POST3 without further changes thereafter. These results establish a novel use of functional HRCT to map heterogeneous regional deformation during compensatory lung growth and illustrate a stimulus-response feedback loop whereby postresection mechanical stress initiates differential lobar regrowth and sustained remodeling, which in turn, relieves parenchyma stress and strain, resulting in progressive increases in lobar Cs and a delayed increase in whole lung Cs.},
	author = {Yilmaz, Cuneyt and Tustison, Nicholas J and Dane, D Merrill and Ravikumar, Priya and Takahashi, Masaya and Gee, James C and Hsia, Connie C W},
	date-added = {2013-11-07 00:38:11 +0000},
	date-modified = {2013-11-07 00:38:11 +0000},
	doi = {10.1152/japplphysiol.00527.2011},
	journal = {J Appl Physiol (1985)},
	journal-full = {Journal of applied physiology (Bethesda, Md. : 1985)},
	mesh = {Adaptation, Physiological; Airway Remodeling; Animals; Dogs; Longitudinal Studies; Lung; Male; Pneumonectomy; Pressure; Stress, Mechanical; Tomography, X-Ray Computed},
	month = {Oct},
	number = {4},
	pages = {1150-8},
	pmc = {PMC3191793},
	pmid = {21799134},
	pst = {ppublish},
	title = {Progressive adaptation in regional parenchyma mechanics following extensive lung resection assessed by functional computed tomography},
	volume = {111},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1152/japplphysiol.00527.2011}}

@article{Tustison:2011aa,
	abstract = {PURPOSE: To develop an automated segmentation method to differentiate the ventilated lung volume on (3) He magnetic resonance imaging (MRI).
MATERIALS AND METHODS: Computational processing (CP) for each subject consisted of the following three essential steps: 1) inhomogeneity bias correction, 2) whole lung segmentation, and 3) subdivision of the lung segmentation into regions of similar ventilation. Evaluation consisted of two comparative analyses: i) comparison of the number of defects scored by two human readers in 43 subjects, and ii) simultaneous truth and performance level estimation (STAPLE) in 18 subjects in which the ventilation defects were manually segmented by four human readers.
RESULTS: There was excellent correlation between the number of ventilation defects tabulated by CP and reader #1 (intraclass correlation coefficient [ICC] = 0.86), CP and reader #2 (ICC = 0.85), and between the two readers (ICC = 0.97). The STAPLE results from the second analysis yielded the following sensitivity/specificity numbers: CP (0.898/0.905), radiologist #1 (0.743/0.897), radiologist #2 (0.501/0.985), radiologist #3 (0.898/0.848), and the first author (0.600/0.984).
CONCLUSION: We developed and evaluated an automated method for quantifying the ventilated lung volume on (3) He MRI. The findings strongly indicate that our proposed algorithmic processing may be a reliable, automatic method for quantitating ventilation defects.},
	author = {Tustison, Nicholas J and Avants, Brian B and Flors, Lucia and Altes, Talissa A and de Lange, Eduard E and Mugler, 3rd, John P and Gee, James C},
	date-added = {2013-11-07 00:38:10 +0000},
	date-modified = {2013-11-07 01:00:00 +0000},
	doi = {10.1002/jmri.22738},
	journal = {J Magn Reson Imaging},
	journal-full = {Journal of magnetic resonance imaging : JMRI},
	mesh = {Administration, Inhalation; Asthma; Automation; Case-Control Studies; Cystic Fibrosis; Female; Helium; Humans; Image Processing, Computer-Assisted; Lung; Magnetic Resonance Imaging; Male; Pulmonary Gas Exchange; Pulmonary Ventilation; Sensitivity and Specificity},
	month = {Oct},
	number = {4},
	pages = {831-41},
	pmid = {21837781},
	pst = {ppublish},
	title = {Ventilation-based segmentation of the lungs using hyperpolarized (3)He {MRI}},
	volume = {34},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/jmri.22738}}

@article{Barbosa:2011aa,
	abstract = {RATIONALE AND OBJECTIVES: Obstructive pulmonary disease phenotypes are related to variable combinations of emphysema and small-airway disease, the latter manifested as air trapping (AT) on imaging. The investigators propose a method to extract AT information quantitatively from thoracic multi-detector row high-resolution computed tomography (HRCT), validated by pulmonary function testing (PFT) correlation.
MATERIALS AND METHODS: Seventeen patients with obstructive pulmonary disease who underwent HRCT and PFT within a 3-day interval were retrospectively identified. Thin-section volumetric HRCT in inspiration and expiration was registered and analyzed using custom-made software. Nonaerated regions of lung were segmented through exclusion of voxels > -50 Hounsfield units (HU); emphysematous areas were segmented as voxels < -950 HU on inspiratory images. Small-airway AT volume (ATV) was segmented as regions of lung voxels whose attenuation values increased by less than a specified change threshold (set from 5 to 300 HU in 25-HU increments) between inspiration and expiration. Inspiratory and expiratory total segmented lung volumes, emphysema volume (EV), and ATV for each threshold were subsequently calculated and correlated with PFT parameters.
RESULTS: A strong positive correlation was obtained between total segmented lung volume in inspiration and total lung capacity (r = 0.83). A strong negative correlation (r = -0.80) was obtained between EV and the ratio between forced expiratory volume in 1 second and forced vital capacity. Stronger negative correlation with forced expiratory volume in 1 second/forced vital capacity (r = -0.85) was demonstrated when ATV (threshold, 50 HU) was added to EV, indicating improved quantification of total AT to predict obstructive disease severity. A moderately strong positive correlation between ATV and residual volume was observed, with a maximum r value of 0.72 (threshold, 25 HU), greater than that between EV and residual volume (r = 0.58). The benefit of ATV quantification was greater in a subgroup of patients with negligible emphysema compared to patients with moderate to severe emphysema.
CONCLUSIONS: Small-airway AT segmentation in conjunction with emphysema segmentation through computer-assisted methodologies may provide better correlations with key PFT parameters, suggesting that the quantification of emphysema-related and small airway-related components of AT from thoracic HRCT has great potential to elucidate phenotypic differences in patients with chronic obstructive pulmonary disease.},
	author = {Barbosa, Jr, Eduardo Mortani and Song, Gang and Tustison, Nicholas and Kreider, Maryl and Gee, James C and Gefter, Warren B and Torigian, Drew A},
	date-added = {2013-11-07 00:38:09 +0000},
	date-modified = {2013-11-07 00:47:09 +0000},
	doi = {10.1016/j.acra.2011.06.004},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Adult; Aged; Aged, 80 and over; Algorithms; Female; Humans; Male; Middle Aged; Phenotype; Pulmonary Disease, Chronic Obstructive; Pulmonary Emphysema; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Respiratory Function Tests; Retrospective Studies; Software; Subtraction Technique; Tomography, X-Ray Computed},
	month = {Oct},
	number = {10},
	pages = {1258-69},
	pmid = {21893294},
	pst = {ppublish},
	title = {Computational analysis of thoracic multidetector row {HRCT} for segmentation and quantification of small airway air trapping and emphysema in obstructive pulmonary disease},
	volume = {18},
	year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2011.06.004}}

@article{Song:2012aa,
	abstract = {RATIONALE AND OBJECTIVES: The aim of this study was to compare the performance of various image-based metrics computed from thoracic high-resolution computed tomography (HRCT) with data from pulmonary function testing (PFT) in characterizing interstitial lung disease (ILD) and chronic obstructive pulmonary disease (COPD).
MATERIALS AND METHODS: Fourteen patients with ILD and 11 with COPD had undergone both PFT and HRCT within 3 days. For each patient, 93 image-based metrics were computed, and their relationships with the 21 clinically used PFT parameters were analyzed using a minimal-redundancy-maximal-relevance statistical framework. The first 20 features were selected among the total of 114 mixed image metrics and PFT values in the characterization of ILD and COPD.
RESULTS: Among the best-performing 20 features, 14 were image metrics, derived from attenuation histograms and texture descriptions. The highest relevance value computed from PFT parameters was 0.47, and the highest from image metrics was 0.52, given the theoretical bound as [0, 0.69]. The ILD or COPD classifier using the first four features achieved a 1.92% error rate.
CONCLUSIONS: Some image metrics are not only as good discriminators as PFT for the characterization of ILD and COPD but are also not redundant when PFT values are provided. Image metrics of attenuation histogram statistics and texture descriptions may be valuable for further investigation in computer-assisted diagnosis.},
	author = {Song, Gang and Mortani Barbosa, Jr, Eduardo and Tustison, Nicholas and Gefter, Warren B and Kreider, Maryl and Gee, James C and Torigian, Drew A},
	date-added = {2013-11-07 00:38:04 +0000},
	date-modified = {2013-11-07 00:49:41 +0000},
	doi = {10.1016/j.acra.2012.03.007},
	journal = {Acad Radiol},
	journal-full = {Academic radiology},
	mesh = {Female; Humans; Image Processing, Computer-Assisted; Lung Diseases, Interstitial; Male; Middle Aged; Pulmonary Disease, Chronic Obstructive; Respiratory Function Tests; Tomography, X-Ray Computed},
	month = {Jul},
	number = {7},
	pages = {857-64},
	pmid = {22516670},
	pst = {ppublish},
	title = {A comparative study of {HRCT} image metrics and {PFT} values for characterization of {ILD} and {COPD}},
	volume = {19},
	year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acra.2012.03.007}}

@article{Tustison:2013aa,
	author = {Tustison, Nicholas J and Johnson, Hans J and Rohlfing, Torsten and Klein, Arno and Ghosh, Satrajit S and Ibanez, Luis and Avants, Brian B},
	date-added = {2013-11-07 00:38:01 +0000},
	date-modified = {2013-11-07 00:38:01 +0000},
	doi = {10.3389/fnins.2013.00162},
	journal = {Front Neurosci},
	journal-full = {Frontiers in neuroscience},
	keywords = {best practices; comparative evaluations; confirmation bias; open science; reproducibility},
	pages = {162},
	pmc = {PMC3766821},
	pmid = {24058331},
	pst = {epublish},
	title = {Instrumentation bias in the use and evaluation of scientific software: recommendations for reproducible practices in the computational sciences},
	volume = {7},
	year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.3389/fnins.2013.00162}}
